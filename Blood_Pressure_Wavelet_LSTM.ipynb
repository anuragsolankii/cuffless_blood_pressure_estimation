{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzCMlT_uHCBZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.ndimage import uniform_filter1d\n",
        "from scipy.signal import detrend\n",
        "\n",
        "\n",
        "def find_peaks_original(x, scale=None, debug=False):\n",
        "    \"\"\"Find peaks in quasi-periodic noisy signals using AMPD algorithm.\n",
        "    Automatic Multi-Scale Peak Detection originally proposed in\n",
        "    \"An Efficient Algorithm for Automatic Peak Detection in\n",
        "    Noisy Periodic and Quasi-Periodic Signals\", Algorithms 2012, 5, 588-603\n",
        "    https://doi.org/10.1109/ICRERA.2016.7884365\n",
        "    Optimized implementation by Igor Gotlibovych, 2018\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : ndarray\n",
        "        1-D array on which to find peaks\n",
        "    scale : int, optional\n",
        "        specify maximum scale window size of (2 * scale + 1)\n",
        "    debug : bool, optional\n",
        "        if set to True, return the Local Scalogram Matrix, `LSM`,\n",
        "        and scale with most local maxima, `l`,\n",
        "        together with peak locations\n",
        "    Returns\n",
        "    -------\n",
        "    pks: ndarray\n",
        "        The ordered array of peak indices found in `x`\n",
        "    \"\"\"\n",
        "    x = detrend(x)\n",
        "    N = len(x)\n",
        "    L = N // 2\n",
        "    if scale:\n",
        "        L = min(scale, L)\n",
        "\n",
        "    # create LSM matix\n",
        "    LSM = np.zeros((L, N), dtype=bool)\n",
        "    for k in np.arange(1, L):\n",
        "        LSM[k - 1, k:N - k] = (\n",
        "            (x[0:N - 2 * k] < x[k:N - k]) & (x[k:N - k] > x[2 * k:N])\n",
        "        )\n",
        "\n",
        "    # Find scale with most maxima\n",
        "    G = LSM.sum(axis=1)\n",
        "    l_scale = np.argmax(G)\n",
        "\n",
        "    # find peaks that persist on all scales up to l\n",
        "    pks_logical = np.min(LSM[0:l_scale, :], axis=0)\n",
        "    pks = np.flatnonzero(pks_logical)\n",
        "    if debug:\n",
        "        return pks, LSM, l_scale\n",
        "    return pks\n",
        "\n",
        "\n",
        "def find_peaks(x, scale=None, debug=False):\n",
        "    \"\"\"Find peaks in quasi-periodic noisy signals using AMPD algorithm.\n",
        "    Extended implementation handles peaks near start/end of the signal.\n",
        "    Optimized implementation by Igor Gotlibovych, 2018\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : ndarray\n",
        "        1-D array on which to find peaks\n",
        "    scale : int, optional\n",
        "        specify maximum scale window size of (2 * scale + 1)\n",
        "    debug : bool, optional\n",
        "        if set to True, return the Local Scalogram Matrix, `LSM`,\n",
        "        weigted number of maxima, 'G',\n",
        "        and scale at which G is maximized, `l`,\n",
        "        together with peak locations\n",
        "    Returns\n",
        "    -------\n",
        "    pks: ndarray\n",
        "        The ordered array of peak indices found in `x`\n",
        "    \"\"\"\n",
        "    x = detrend(x)\n",
        "    N = len(x)\n",
        "    L = N // 2\n",
        "    if scale:\n",
        "        L = min(scale, L)\n",
        "\n",
        "    # create LSM matix\n",
        "    LSM = np.ones((L, N), dtype=bool)\n",
        "    for k in np.arange(1, L + 1):\n",
        "        LSM[k - 1, 0:N - k] &= (x[0:N - k] > x[k:N]\n",
        "                                )  # compare to right neighbours\n",
        "        LSM[k - 1, k:N] &= (x[k:N] > x[0:N - k])  # compare to left neighbours\n",
        "\n",
        "    # Find scale with most maxima\n",
        "    G = LSM.sum(axis=1)\n",
        "    G = G * np.arange(\n",
        "        N // 2, N // 2 - L, -1\n",
        "    )  # normalize to adjust for new edge regions\n",
        "    l_scale = np.argmax(G)\n",
        "\n",
        "    # find peaks that persist on all scales up to l\n",
        "    pks_logical = np.min(LSM[0:l_scale, :], axis=0)\n",
        "    pks = np.flatnonzero(pks_logical)\n",
        "    if debug:\n",
        "        return pks, LSM, G, l_scale\n",
        "    return pks\n",
        "\n",
        "\n",
        "def find_peaks_adaptive(x, window=None, debug=False):\n",
        "    \"\"\"Find peaks in quasi-periodic noisy signals using ASS-AMPD algorithm.\n",
        "    Adaptive Scale Selection Automatic Multi-Scale Peak Detection,\n",
        "    an extension of AMPD -\n",
        "    \"An Efficient Algorithm for Automatic Peak Detection in\n",
        "    Noisy Periodic and Quasi-Periodic Signals\", Algorithms 2012, 5, 588-603\n",
        "    https://doi.org/10.1109/ICRERA.2016.7884365\n",
        "    Optimized implementation by Igor Gotlibovych, 2018\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : ndarray\n",
        "        1-D array on which to find peaks\n",
        "    window : int, optional\n",
        "        sliding window size for adaptive scale selection\n",
        "    debug : bool, optional\n",
        "        if set to True, return the Local Scalogram Matrix, `LSM`,\n",
        "        and `adaptive_scale`,\n",
        "        together with peak locations\n",
        "    Returns\n",
        "    -------\n",
        "    pks: ndarray\n",
        "        The ordered array of peak indices found in `x`\n",
        "    \"\"\"\n",
        "    x = detrend(x)\n",
        "    N = len(x)\n",
        "    if not window:\n",
        "        window = N\n",
        "    if window > N:\n",
        "        window = N\n",
        "    L = window // 2\n",
        "\n",
        "    # create LSM matix\n",
        "    LSM = np.ones((L, N), dtype=bool)\n",
        "    for k in np.arange(1, L + 1):\n",
        "        LSM[k - 1, 0:N - k] &= (x[0:N - k] > x[k:N]\n",
        "                                )  # compare to right neighbours\n",
        "        LSM[k - 1, k:N] &= (x[k:N] > x[0:N - k])  # compare to left neighbours\n",
        "\n",
        "    # Create continuos adaptive LSM\n",
        "    ass_LSM = uniform_filter1d(LSM * window, window, axis=1, mode='nearest')\n",
        "    normalization = np.arange(L, 0, -1)  # scale normalization weight\n",
        "    ass_LSM = ass_LSM * normalization.reshape(-1, 1)\n",
        "\n",
        "    # Find adaptive scale at each point\n",
        "    adaptive_scale = ass_LSM.argmax(axis=0)\n",
        "\n",
        "    # construct reduced LSM\n",
        "    LSM_reduced = LSM[:adaptive_scale.max(), :]\n",
        "    mask = (np.indices(LSM_reduced.shape)[0] > adaptive_scale\n",
        "            )  # these elements are outside scale of interest\n",
        "    LSM_reduced[mask] = 1\n",
        "\n",
        "    # find peaks that persist on all scales up to l\n",
        "    pks_logical = np.min(LSM_reduced, axis=0)\n",
        "    pks = np.flatnonzero(pks_logical)\n",
        "    if debug:\n",
        "        return pks, ass_LSM, adaptive_scale\n",
        "    return pks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enDq36J5HIBg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ee1c113-482f-494a-b020-1896026da570"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.utils.data as Data\n",
        "import torchvision.datasets\n",
        "import torchvision.transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import glob\n",
        "# from pyampd.ampd import find_peaks\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bp2nOsNHIET",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2000bc90-017b-4fbd-ec90-df3cbe3c7bb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TETlJZeCHIHV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70725c14-4c2e-4042-ada6-945f5ab79b1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  gdrive/My Drive/archive.zip\n",
            "replace Samples/rec_1.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip gdrive/My\\ Drive/archive.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hC3iruvHIKK"
      },
      "outputs": [],
      "source": [
        "PPG_datas = []\n",
        "ABP_datas = []\n",
        "ECG_datas = []\n",
        "i = 0\n",
        "for name in glob.glob('./Samples/*.csv'):\n",
        "    raw_training_data = pd.read_csv(name, header=None)\n",
        "    raw_training_data = np.array(raw_training_data)\n",
        "    #print(raw_training_data.shape)\n",
        "    PPG_data = raw_training_data[0]\n",
        "    ABP_data = raw_training_data[1]\n",
        "    ECG_data = raw_training_data[2]\n",
        "\n",
        "    PPG_datas.append(PPG_data)\n",
        "    ABP_datas.append(ABP_data)\n",
        "    ECG_datas.append(ECG_data)\n",
        "    # i = i + 1\n",
        " #   print(i)\n",
        "#     if (i == 100):\n",
        "#         break\n",
        "\n",
        "PPG_datas = np.array(PPG_datas)\n",
        "ABP_datas = np.array(ABP_datas)\n",
        "ECG_datas = np.array(ECG_datas)\n",
        "# raw_training_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ko8szvVyHINT"
      },
      "outputs": [],
      "source": [
        "import pywt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def generate_wavelet_vector(X, wavelet_family, decomposition_level):\n",
        "    coefficients = pywt.wavedec(X, wavelet_family, level=decomposition_level)\n",
        "    vector = np.array([])\n",
        "    for coeffs in coefficients:\n",
        "        vector = np.concatenate((vector, coeffs))\n",
        "    return vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LAzWKO_HIQY"
      },
      "outputs": [],
      "source": [
        "SAMPLE_FREQ = 125"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-fpu9tRHITR"
      },
      "outputs": [],
      "source": [
        "wavelet_family = 'db4'\n",
        "decomposition_level = 5\n",
        "\n",
        "wavelet_vectors = []\n",
        "SBP_data = []\n",
        "DBP_data = []\n",
        "MAP_data = []\n",
        "\n",
        "for j in range(len(PPG_datas)):\n",
        "    sec_15 = 15 * SAMPLE_FREQ\n",
        "    PPG_data = PPG_datas[j]\n",
        "    ABP_data = ABP_datas[j]\n",
        "    PPG_peaks = find_peaks(PPG_data, scale=SAMPLE_FREQ)\n",
        "    for i in range(2, PPG_peaks.shape[0]):\n",
        "        X = PPG_data[PPG_peaks[i - 1]:PPG_peaks[i]]\n",
        "        if len(X) < SAMPLE_FREQ:\n",
        "            wavelet_vector = generate_wavelet_vector(X, wavelet_family, decomposition_level)\n",
        "\n",
        "            SBP = np.max(ABP_data[PPG_peaks[i - 1]:PPG_peaks[i - 1] + sec_15])\n",
        "            DBP = np.min(ABP_data[PPG_peaks[i - 1]:PPG_peaks[i - 1] + sec_15])\n",
        "            MAP = SBP / 3 + 2 * DBP / 3\n",
        "\n",
        "            wavelet_vectors.append(wavelet_vector)\n",
        "            SBP_data.append(SBP)\n",
        "            DBP_data.append(DBP)\n",
        "            MAP_data.append(MAP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YYleBSrHIWL"
      },
      "outputs": [],
      "source": [
        "wavelet_vectors = np.array(wavelet_vectors)\n",
        "SBP_data = np.array(SBP_data)\n",
        "DBP_data = np.array(DBP_data)\n",
        "MAP_data = np.array(MAP_data)\n",
        "\n",
        "print(wavelet_vectors.shape)\n",
        "print(SBP_data.shape)\n",
        "\n",
        "# Find the maximum length of the wavelet vectors\n",
        "max_length = max([len(vector) for vector in wavelet_vectors])\n",
        "\n",
        "# Pad the wavelet vectors with zeros to make them uniform length\n",
        "padded_wavelet_vectors = []\n",
        "for vector in wavelet_vectors:\n",
        "    padded_vector = np.pad(vector, (0, max_length - len(vector)), mode='constant')\n",
        "    padded_wavelet_vectors.append(padded_vector)\n",
        "\n",
        "padded_wavelet_vectors = np.array(padded_wavelet_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpdtzAIBHIZJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=43)\n",
        "pca_wavelet_vectors = pca.fit_transform(padded_wavelet_vectors)\n",
        "\n",
        "print(pca_wavelet_vectors.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqFY87YWHIcL"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, SBP_train, SBP_test, DBP_train, DBP_test, MAP_train, MAP_test = train_test_split(\n",
        "    pca_wavelet_vectors, SBP_data, DBP_data, MAP_data, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVd-rEABHIfS"
      },
      "outputs": [],
      "source": [
        "def AAMI_standard(predict, test):\n",
        "    total = len(predict)\n",
        "    ME = np.mean(predict - test)\n",
        "    MAE = np.mean(np.abs(predict-test))\n",
        "    SD = np.std(predict-test)\n",
        "\n",
        "    return total, ME, MAE, SD\n",
        "\n",
        "def BHS_standard(predict, test):\n",
        "    total = len(predict)\n",
        "    mm5 = np.sum(np.abs(predict-test)<=5)\n",
        "    mm10 = np.sum(np.abs(predict-test)<=10)\n",
        "    mm15 = np.sum(np.abs(predict-test)<=15)\n",
        "    return total, mm5, mm10, mm15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15iZcSYPHIh9"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 2\n",
        "LR = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjtujRd_HIlI"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_dim = 256\n",
        "        self.input = nn.LSTM(input_size=input_size, hidden_size=self.hidden_dim, num_layers=1, batch_first=True)\n",
        "        self.hidden1 = nn.Linear(in_features=self.hidden_dim, out_features=512)\n",
        "        self.hidden2 = nn.Linear(in_features=512, out_features=256)\n",
        "        self.hidden3 = nn.Linear(in_features=256, out_features=32)\n",
        "        self.output = nn.Linear(in_features=32, out_features=1)\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        self.activation = lambda X: F.relu(X)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, (h_n, c_n) = self.input(x)\n",
        "        out = self.hidden1(self.activation(h_n[-1]))\n",
        "        out = self.dropout(out)\n",
        "        out = self.hidden2(self.activation(out))\n",
        "        out = self.hidden3(self.activation(out))\n",
        "        out = self.output(self.activation(out))\n",
        "        return out\n",
        "\n",
        "    def get_weight(self):\n",
        "        weight = []\n",
        "        weight.append(self.input.weight_ih_l0)\n",
        "        weight.append(self.input.weight_hh_l0)\n",
        "        weight.append(self.input.bias_ih_l0)\n",
        "        weight.append(self.input.bias_hh_l0)\n",
        "        weight.append(self.hidden1.weight)\n",
        "        weight.append(self.hidden2.weight)\n",
        "        weight.append(self.hidden3.weight)\n",
        "        weight.append(self.output.weight)\n",
        "        return weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-bmutlxHInu"
      },
      "outputs": [],
      "source": [
        "# torch.set_default_tensor_type(torch.FloatTensor)\n",
        "\n",
        "class LSTMModel():\n",
        "    def __init__(self, net, train_loader, test_loader, EPOCH=20, LR=0.0001):\n",
        "        self.net = net\n",
        "        self.optimizer = torch.optim.Adam(net.parameters(), lr = LR)\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.EPOCHS = EPOCH\n",
        "        self.LR = LR\n",
        "\n",
        "        self.net = self.net.to(device)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def start(self):\n",
        "\n",
        "        self.history_train_loss = []\n",
        "        self.history_test_loss = []\n",
        "#         history_train_accuracy = []\n",
        "#         history_test_accuracy = []\n",
        "        for epoch in range(self.EPOCHS):\n",
        "            print('Epoch:', epoch)\n",
        "            train_loss = self.train()\n",
        "            test_loss = self.test()\n",
        "\n",
        "            self.history_train_loss.append(train_loss)\n",
        "            self.history_test_loss.append(test_loss)\n",
        "#             history_train_accuracy.append(train_accuracy)\n",
        "#             history_test_accuracy.append(test_accuracy)\n",
        "\n",
        "    def train(self):\n",
        "        self.net.train()\n",
        "        train_loss = 0\n",
        "        for step, (batch_X, batch_y) in enumerate(self.train_loader):\n",
        "            batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.net(batch_X).double()\n",
        "\n",
        "            loss = self.criterion(outputs.reshape(-1), batch_y)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            #_, predicted = outputs.max(1)\n",
        "            #total += batch_y.size(0)\n",
        "            #correct += predicted.eq(batch_y).sum().item()\n",
        "\n",
        "        print('【Training】Loss: %.3f ' % ( train_loss))\n",
        "        return train_loss\n",
        "\n",
        "    def test(self):\n",
        "        self.net.eval()\n",
        "\n",
        "        test_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for step, (batch_X, batch_y) in enumerate(self.test_loader):\n",
        "                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
        "                outputs = self.net(batch_X).double()\n",
        "                loss = self.criterion(outputs.reshape(-1), batch_y)\n",
        "\n",
        "                test_loss += loss.item()\n",
        "#                 _, predicted = outputs.max(1)\n",
        "#                 #print(predicted, batch_y)\n",
        "#                 total += batch_y.size(0)\n",
        "#                 correct += predicted.eq(batch_y).sum().item()\n",
        "\n",
        "        print('【Testing】Loss: %.3f )' % ( test_loss))\n",
        "        return test_loss\n",
        "\n",
        "    def predict(self, test_loader):\n",
        "        outputs = []\n",
        "        self.net.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for step, (batch_X, batch_y) in enumerate(test_loader):\n",
        "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "                output = self.net(batch_X).double().cpu().numpy().reshape(-1)\n",
        "                for i in range(len(output)):\n",
        "                    outputs.append(output[i])\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def get_weight(self):\n",
        "        return self.net.get_weight()\n",
        "\n",
        "    def get_model(self):\n",
        "        return self.net.state_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTZbM0juHIrR"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "L = 43\n",
        "\n",
        "x_train, x_test, SBP_train, SBP_test, DBP_train, DBP_test, MAP_train, MAP_test = train_test_split(\n",
        "                                            pca_wavelet_vectors, SBP_data, DBP_data, MAP_data, test_size=0.1, random_state=42)\n",
        "\n",
        "\n",
        "x_train = torch.from_numpy(x_train).type(torch.FloatTensor)\n",
        "x_test = torch.from_numpy(x_test).type(torch.FloatTensor)\n",
        "SBP_train = torch.from_numpy(SBP_train)\n",
        "SBP_test = torch.from_numpy(SBP_test)\n",
        "DBP_train = torch.from_numpy(DBP_train)\n",
        "DBP_test = torch.from_numpy(DBP_test)\n",
        "MAP_train = torch.from_numpy(MAP_train)\n",
        "MAP_test = torch.from_numpy(MAP_test)\n",
        "\n",
        "x_train = x_train.view(-1, 1, L)\n",
        "x_test = x_test.view(-1, 1, L)\n",
        "\n",
        "print(x_train.shape, x_test.shape, SBP_train.shape, SBP_test.shape)\n",
        "\n",
        "SBP_train_dataset = Data.TensorDataset(x_train, SBP_train)\n",
        "SBP_test_dataset = Data.TensorDataset(x_test, SBP_test)\n",
        "\n",
        "\n",
        "SBP_train_loader = Data.DataLoader(\n",
        "    dataset = SBP_train_dataset,\n",
        "    batch_size = 128,\n",
        ")\n",
        "\n",
        "SBP_test_loader = Data.DataLoader(\n",
        "    dataset = SBP_test_dataset,\n",
        "    batch_size = 10,\n",
        ")\n",
        "\n",
        "DBP_train_dataset = Data.TensorDataset(x_train, DBP_train)\n",
        "DBP_test_dataset = Data.TensorDataset(x_test, DBP_test)\n",
        "\n",
        "\n",
        "DBP_train_loader = Data.DataLoader(\n",
        "    dataset = DBP_train_dataset,\n",
        "    batch_size = 128,\n",
        ")\n",
        "\n",
        "DBP_test_loader = Data.DataLoader(\n",
        "    dataset = DBP_test_dataset,\n",
        "    batch_size = 10,\n",
        ")\n",
        "\n",
        "MAP_train_dataset = Data.TensorDataset(x_train, MAP_train)\n",
        "MAP_test_dataset = Data.TensorDataset(x_test, MAP_test)\n",
        "\n",
        "\n",
        "MAP_train_loader = Data.DataLoader(\n",
        "    dataset = MAP_train_dataset,\n",
        "    batch_size = 128,\n",
        ")\n",
        "\n",
        "MAP_test_loader = Data.DataLoader(\n",
        "    dataset = MAP_test_dataset,\n",
        "    batch_size = 10,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOlFUcvQH98e"
      },
      "outputs": [],
      "source": [
        "SBP_lstm_module = LSTMModel(LSTM(input_size=L), SBP_train_loader, SBP_test_loader, EPOCH=200, LR=LR)\n",
        "SBP_lstm_module.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Pzi31JtH-Cr"
      },
      "outputs": [],
      "source": [
        "DBP_lstm_module = LSTMModel(LSTM(input_size=L), DBP_train_loader, DBP_test_loader, EPOCH=200, LR=LR)\n",
        "DBP_lstm_module.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1C3QscSH-Kc"
      },
      "outputs": [],
      "source": [
        "MAP_lstm_module = LSTMModel(LSTM(input_size=L), MAP_train_loader, MAP_test_loader, EPOCH=200, LR=LR)\n",
        "MAP_lstm_module.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdb66I63H-UW"
      },
      "outputs": [],
      "source": [
        "# temp_predict = []\n",
        "# for predicts in SBP_predict:\n",
        "#     for i in range(len(predicts))\n",
        "#         temp_predict.append(predicts[i])\n",
        "# print(SBP_predict[0:20])\n",
        "\n",
        "# print(SBP_test[0:20])\n",
        "import time\n",
        "\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "SBP_predict = np.array(SBP_lstm_module.predict(SBP_test_loader)).reshape(-1)\n",
        "SBP_test = np.array(SBP_test)\n",
        "DBP_predict = np.array(DBP_lstm_module.predict(DBP_test_loader)).reshape(-1)\n",
        "DBP_test = np.array(DBP_test)\n",
        "MAP_predict = np.array(MAP_lstm_module.predict(MAP_test_loader)).reshape(-1)\n",
        "MAP_test = np.array(MAP_test)\n",
        "\n",
        "print('LSTM')\n",
        "\n",
        "total, ME, MAE, SD = AAMI_standard(SBP_predict, SBP_test)\n",
        "print()\n",
        "print()\n",
        "print('----------------SBP-----------------')\n",
        "print()\n",
        "print('-------------AAMI standard----------')\n",
        "print('ME             MAE             SD           total')\n",
        "print('%.3f         %.3f         %.3f          %d' % (ME,MAE,SD,total))\n",
        "#print('ME: ', ME, '   MAE: ', MAE, '   SD: ', SD, '   total: ', total)\n",
        "total, mm5, mm10, mm15 = BHS_standard(SBP_predict, SBP_test)\n",
        "print()\n",
        "print('-------------BHS standard------------')\n",
        "print('<5mmHg        <10mmHg        <15mmHg        total')\n",
        "print('%d            %d           %d          %d' % (mm5,mm10,mm15,total))\n",
        "print('%.3f%s        %.3f%s       %.3f%s         %d' % (mm5/total*100, '%',mm10/total*100, '%',mm15/total*100, '%', total))\n",
        "\n",
        "total, ME, MAE, SD = AAMI_standard(DBP_predict, DBP_test)\n",
        "print()\n",
        "print()\n",
        "print('----------------DBP-----------------')\n",
        "print()\n",
        "print('-------------AAMI standard----------')\n",
        "print('ME             MAE             SD           total')\n",
        "print('%.3f         %.3f         %.3f          %d' % (ME,MAE,SD,total))\n",
        "#print('ME: ', ME, '   MAE: ', MAE, '   SD: ', SD, '   total: ', total)\n",
        "total, mm5, mm10, mm15 = BHS_standard(DBP_predict, DBP_test)\n",
        "print()\n",
        "print('-------------BHS standard------------')\n",
        "print('<5mmHg        <10mmHg        <15mmHg        total')\n",
        "print('%d            %d           %d          %d' % (mm5,mm10,mm15,total))\n",
        "print('%.3f%s        %.3f%s       %.3f%s         %d' % (mm5/total*100, '%',mm10/total*100, '%',mm15/total*100, '%', total))\n",
        "\n",
        "total, ME, MAE, SD = AAMI_standard(MAP_predict, MAP_test)\n",
        "print()\n",
        "print()\n",
        "print('----------------MAP-----------------')\n",
        "print()\n",
        "print('-------------AAMI standard----------')\n",
        "print('ME             MAE             SD           total')\n",
        "print('%.3f         %.3f         %.3f          %d' % (ME,MAE,SD,total))\n",
        "#print('ME: ', ME, '   MAE: ', MAE, '   SD: ', SD, '   total: ', total)\n",
        "total, mm5, mm10, mm15 = BHS_standard(MAP_predict, MAP_test)\n",
        "print()\n",
        "print('-------------BHS standard------------')\n",
        "print('<5mmHg        <10mmHg        <15mmHg        total')\n",
        "print('%d            %d           %d          %d' % (mm5,mm10,mm15,total))\n",
        "print('%.3f%s        %.3f%s       %.3f%s         %d' % (mm5/total*100, '%',mm10/total*100, '%',mm15/total*100, '%', total))\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Print the elapsed time\n",
        "print(\"Time taken:\", elapsed_time, \"seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMGl2W4DH-b3"
      },
      "outputs": [],
      "source": [
        "print('LSTM')\n",
        "\n",
        "SBP_cc= np.corrcoef(SBP_predict, SBP_test)\n",
        "DBP_cc= np.corrcoef(DBP_predict, DBP_test)\n",
        "MAP_cc= np.corrcoef(MAP_predict, MAP_test)\n",
        "print()\n",
        "print()\n",
        "print('------------Correlation Coefficient-------------')\n",
        "print()\n",
        "print('SBP: %.3f' % (SBP_cc[0, 1]))\n",
        "print()\n",
        "print('DBP: %.3f' % (DBP_cc[0, 1]))\n",
        "print()\n",
        "print('MAP: %.3f' % (MAP_cc[0, 1]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}