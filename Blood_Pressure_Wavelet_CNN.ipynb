{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ITXDbMYCwIP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.ndimage import uniform_filter1d\n",
        "from scipy.signal import detrend\n",
        "\n",
        "\n",
        "def find_peaks_original(x, scale=None, debug=False):\n",
        "    \"\"\"Find peaks in quasi-periodic noisy signals using AMPD algorithm.\n",
        "    Automatic Multi-Scale Peak Detection originally proposed in\n",
        "    \"An Efficient Algorithm for Automatic Peak Detection in\n",
        "    Noisy Periodic and Quasi-Periodic Signals\", Algorithms 2012, 5, 588-603\n",
        "    https://doi.org/10.1109/ICRERA.2016.7884365\n",
        "    Optimized implementation by Igor Gotlibovych, 2018\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : ndarray\n",
        "        1-D array on which to find peaks\n",
        "    scale : int, optional\n",
        "        specify maximum scale window size of (2 * scale + 1)\n",
        "    debug : bool, optional\n",
        "        if set to True, return the Local Scalogram Matrix, `LSM`,\n",
        "        and scale with most local maxima, `l`,\n",
        "        together with peak locations\n",
        "    Returns\n",
        "    -------\n",
        "    pks: ndarray\n",
        "        The ordered array of peak indices found in `x`\n",
        "    \"\"\"\n",
        "    x = detrend(x)\n",
        "    N = len(x)\n",
        "    L = N // 2\n",
        "    if scale:\n",
        "        L = min(scale, L)\n",
        "\n",
        "    # create LSM matix\n",
        "    LSM = np.zeros((L, N), dtype=bool)\n",
        "    for k in np.arange(1, L):\n",
        "        LSM[k - 1, k:N - k] = (\n",
        "            (x[0:N - 2 * k] < x[k:N - k]) & (x[k:N - k] > x[2 * k:N])\n",
        "        )\n",
        "\n",
        "    # Find scale with most maxima\n",
        "    G = LSM.sum(axis=1)\n",
        "    l_scale = np.argmax(G)\n",
        "\n",
        "    # find peaks that persist on all scales up to l\n",
        "    pks_logical = np.min(LSM[0:l_scale, :], axis=0)\n",
        "    pks = np.flatnonzero(pks_logical)\n",
        "    if debug:\n",
        "        return pks, LSM, l_scale\n",
        "    return pks\n",
        "\n",
        "\n",
        "def find_peaks(x, scale=None, debug=False):\n",
        "    \"\"\"Find peaks in quasi-periodic noisy signals using AMPD algorithm.\n",
        "    Extended implementation handles peaks near start/end of the signal.\n",
        "    Optimized implementation by Igor Gotlibovych, 2018\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : ndarray\n",
        "        1-D array on which to find peaks\n",
        "    scale : int, optional\n",
        "        specify maximum scale window size of (2 * scale + 1)\n",
        "    debug : bool, optional\n",
        "        if set to True, return the Local Scalogram Matrix, `LSM`,\n",
        "        weigted number of maxima, 'G',\n",
        "        and scale at which G is maximized, `l`,\n",
        "        together with peak locations\n",
        "    Returns\n",
        "    -------\n",
        "    pks: ndarray\n",
        "        The ordered array of peak indices found in `x`\n",
        "    \"\"\"\n",
        "    x = detrend(x)\n",
        "    N = len(x)\n",
        "    L = N // 2\n",
        "    if scale:\n",
        "        L = min(scale, L)\n",
        "\n",
        "    # create LSM matix\n",
        "    LSM = np.ones((L, N), dtype=bool)\n",
        "    for k in np.arange(1, L + 1):\n",
        "        LSM[k - 1, 0:N - k] &= (x[0:N - k] > x[k:N]\n",
        "                                )  # compare to right neighbours\n",
        "        LSM[k - 1, k:N] &= (x[k:N] > x[0:N - k])  # compare to left neighbours\n",
        "\n",
        "    # Find scale with most maxima\n",
        "    G = LSM.sum(axis=1)\n",
        "    G = G * np.arange(\n",
        "        N // 2, N // 2 - L, -1\n",
        "    )  # normalize to adjust for new edge regions\n",
        "    l_scale = np.argmax(G)\n",
        "\n",
        "    # find peaks that persist on all scales up to l\n",
        "    pks_logical = np.min(LSM[0:l_scale, :], axis=0)\n",
        "    pks = np.flatnonzero(pks_logical)\n",
        "    if debug:\n",
        "        return pks, LSM, G, l_scale\n",
        "    return pks\n",
        "\n",
        "\n",
        "def find_peaks_adaptive(x, window=None, debug=False):\n",
        "    \"\"\"Find peaks in quasi-periodic noisy signals using ASS-AMPD algorithm.\n",
        "    Adaptive Scale Selection Automatic Multi-Scale Peak Detection,\n",
        "    an extension of AMPD -\n",
        "    \"An Efficient Algorithm for Automatic Peak Detection in\n",
        "    Noisy Periodic and Quasi-Periodic Signals\", Algorithms 2012, 5, 588-603\n",
        "    https://doi.org/10.1109/ICRERA.2016.7884365\n",
        "    Optimized implementation by Igor Gotlibovych, 2018\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : ndarray\n",
        "        1-D array on which to find peaks\n",
        "    window : int, optional\n",
        "        sliding window size for adaptive scale selection\n",
        "    debug : bool, optional\n",
        "        if set to True, return the Local Scalogram Matrix, `LSM`,\n",
        "        and `adaptive_scale`,\n",
        "        together with peak locations\n",
        "    Returns\n",
        "    -------\n",
        "    pks: ndarray\n",
        "        The ordered array of peak indices found in `x`\n",
        "    \"\"\"\n",
        "    x = detrend(x)\n",
        "    N = len(x)\n",
        "    if not window:\n",
        "        window = N\n",
        "    if window > N:\n",
        "        window = N\n",
        "    L = window // 2\n",
        "\n",
        "    # create LSM matix\n",
        "    LSM = np.ones((L, N), dtype=bool)\n",
        "    for k in np.arange(1, L + 1):\n",
        "        LSM[k - 1, 0:N - k] &= (x[0:N - k] > x[k:N]\n",
        "                                )  # compare to right neighbours\n",
        "        LSM[k - 1, k:N] &= (x[k:N] > x[0:N - k])  # compare to left neighbours\n",
        "\n",
        "    # Create continuos adaptive LSM\n",
        "    ass_LSM = uniform_filter1d(LSM * window, window, axis=1, mode='nearest')\n",
        "    normalization = np.arange(L, 0, -1)  # scale normalization weight\n",
        "    ass_LSM = ass_LSM * normalization.reshape(-1, 1)\n",
        "\n",
        "    # Find adaptive scale at each point\n",
        "    adaptive_scale = ass_LSM.argmax(axis=0)\n",
        "\n",
        "    # construct reduced LSM\n",
        "    LSM_reduced = LSM[:adaptive_scale.max(), :]\n",
        "    mask = (np.indices(LSM_reduced.shape)[0] > adaptive_scale\n",
        "            )  # these elements are outside scale of interest\n",
        "    LSM_reduced[mask] = 1\n",
        "\n",
        "    # find peaks that persist on all scales up to l\n",
        "    pks_logical = np.min(LSM_reduced, axis=0)\n",
        "    pks = np.flatnonzero(pks_logical)\n",
        "    if debug:\n",
        "        return pks, ass_LSM, adaptive_scale\n",
        "    return pks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.utils.data as Data\n",
        "import torchvision.datasets\n",
        "import torchvision.transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import glob\n",
        "# from pyampd.ampd import find_peaks\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import joblib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDx6YO47C5mQ",
        "outputId": "b91d4acb-1ac9-4a77-8a66-32106eb0b5e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MP3ASpqC5o5",
        "outputId": "66110c72-30df-4fa9-a0bf-989d179ce4ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip gdrive/My\\ Drive/archive.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USfTn_H5C5rm",
        "outputId": "345e7954-7397-4286-c1dd-bfb0a41a656a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  gdrive/My Drive/archive.zip\n",
            "  inflating: Samples/rec_1.csv       \n",
            "  inflating: Samples/rec_10.csv      \n",
            "  inflating: Samples/rec_100.csv     \n",
            "  inflating: Samples/rec_101.csv     \n",
            "  inflating: Samples/rec_102.csv     \n",
            "  inflating: Samples/rec_103.csv     \n",
            "  inflating: Samples/rec_104.csv     \n",
            "  inflating: Samples/rec_105.csv     \n",
            "  inflating: Samples/rec_106.csv     \n",
            "  inflating: Samples/rec_107.csv     \n",
            "  inflating: Samples/rec_108.csv     \n",
            "  inflating: Samples/rec_109.csv     \n",
            "  inflating: Samples/rec_11.csv      \n",
            "  inflating: Samples/rec_110.csv     \n",
            "  inflating: Samples/rec_111.csv     \n",
            "  inflating: Samples/rec_112.csv     \n",
            "  inflating: Samples/rec_113.csv     \n",
            "  inflating: Samples/rec_114.csv     \n",
            "  inflating: Samples/rec_115.csv     \n",
            "  inflating: Samples/rec_116.csv     \n",
            "  inflating: Samples/rec_117.csv     \n",
            "  inflating: Samples/rec_118.csv     \n",
            "  inflating: Samples/rec_119.csv     \n",
            "  inflating: Samples/rec_12.csv      \n",
            "  inflating: Samples/rec_120.csv     \n",
            "  inflating: Samples/rec_121.csv     \n",
            "  inflating: Samples/rec_122.csv     \n",
            "  inflating: Samples/rec_123.csv     \n",
            "  inflating: Samples/rec_124.csv     \n",
            "  inflating: Samples/rec_125.csv     \n",
            "  inflating: Samples/rec_126.csv     \n",
            "  inflating: Samples/rec_127.csv     \n",
            "  inflating: Samples/rec_128.csv     \n",
            "  inflating: Samples/rec_129.csv     \n",
            "  inflating: Samples/rec_13.csv      \n",
            "  inflating: Samples/rec_130.csv     \n",
            "  inflating: Samples/rec_131.csv     \n",
            "  inflating: Samples/rec_132.csv     \n",
            "  inflating: Samples/rec_133.csv     \n",
            "  inflating: Samples/rec_134.csv     \n",
            "  inflating: Samples/rec_135.csv     \n",
            "  inflating: Samples/rec_136.csv     \n",
            "  inflating: Samples/rec_137.csv     \n",
            "  inflating: Samples/rec_138.csv     \n",
            "  inflating: Samples/rec_139.csv     \n",
            "  inflating: Samples/rec_14.csv      \n",
            "  inflating: Samples/rec_140.csv     \n",
            "  inflating: Samples/rec_141.csv     \n",
            "  inflating: Samples/rec_142.csv     \n",
            "  inflating: Samples/rec_143.csv     \n",
            "  inflating: Samples/rec_144.csv     \n",
            "  inflating: Samples/rec_145.csv     \n",
            "  inflating: Samples/rec_146.csv     \n",
            "  inflating: Samples/rec_147.csv     \n",
            "  inflating: Samples/rec_148.csv     \n",
            "  inflating: Samples/rec_149.csv     \n",
            "  inflating: Samples/rec_15.csv      \n",
            "  inflating: Samples/rec_150.csv     \n",
            "  inflating: Samples/rec_151.csv     \n",
            "  inflating: Samples/rec_152.csv     \n",
            "  inflating: Samples/rec_153.csv     \n",
            "  inflating: Samples/rec_154.csv     \n",
            "  inflating: Samples/rec_155.csv     \n",
            "  inflating: Samples/rec_156.csv     \n",
            "  inflating: Samples/rec_157.csv     \n",
            "  inflating: Samples/rec_158.csv     \n",
            "  inflating: Samples/rec_159.csv     \n",
            "  inflating: Samples/rec_16.csv      \n",
            "  inflating: Samples/rec_160.csv     \n",
            "  inflating: Samples/rec_161.csv     \n",
            "  inflating: Samples/rec_162.csv     \n",
            "  inflating: Samples/rec_163.csv     \n",
            "  inflating: Samples/rec_164.csv     \n",
            "  inflating: Samples/rec_165.csv     \n",
            "  inflating: Samples/rec_166.csv     \n",
            "  inflating: Samples/rec_167.csv     \n",
            "  inflating: Samples/rec_168.csv     \n",
            "  inflating: Samples/rec_169.csv     \n",
            "  inflating: Samples/rec_17.csv      \n",
            "  inflating: Samples/rec_170.csv     \n",
            "  inflating: Samples/rec_171.csv     \n",
            "  inflating: Samples/rec_172.csv     \n",
            "  inflating: Samples/rec_173.csv     \n",
            "  inflating: Samples/rec_174.csv     \n",
            "  inflating: Samples/rec_175.csv     \n",
            "  inflating: Samples/rec_176.csv     \n",
            "  inflating: Samples/rec_177.csv     \n",
            "  inflating: Samples/rec_178.csv     \n",
            "  inflating: Samples/rec_179.csv     \n",
            "  inflating: Samples/rec_18.csv      \n",
            "  inflating: Samples/rec_180.csv     \n",
            "  inflating: Samples/rec_181.csv     \n",
            "  inflating: Samples/rec_182.csv     \n",
            "  inflating: Samples/rec_183.csv     \n",
            "  inflating: Samples/rec_184.csv     \n",
            "  inflating: Samples/rec_185.csv     \n",
            "  inflating: Samples/rec_186.csv     \n",
            "  inflating: Samples/rec_187.csv     \n",
            "  inflating: Samples/rec_188.csv     \n",
            "  inflating: Samples/rec_189.csv     \n",
            "  inflating: Samples/rec_19.csv      \n",
            "  inflating: Samples/rec_190.csv     \n",
            "  inflating: Samples/rec_191.csv     \n",
            "  inflating: Samples/rec_192.csv     \n",
            "  inflating: Samples/rec_193.csv     \n",
            "  inflating: Samples/rec_194.csv     \n",
            "  inflating: Samples/rec_195.csv     \n",
            "  inflating: Samples/rec_196.csv     \n",
            "  inflating: Samples/rec_197.csv     \n",
            "  inflating: Samples/rec_198.csv     \n",
            "  inflating: Samples/rec_199.csv     \n",
            "  inflating: Samples/rec_2.csv       \n",
            "  inflating: Samples/rec_20.csv      \n",
            "  inflating: Samples/rec_200.csv     \n",
            "  inflating: Samples/rec_201.csv     \n",
            "  inflating: Samples/rec_202.csv     \n",
            "  inflating: Samples/rec_203.csv     \n",
            "  inflating: Samples/rec_204.csv     \n",
            "  inflating: Samples/rec_205.csv     \n",
            "  inflating: Samples/rec_206.csv     \n",
            "  inflating: Samples/rec_207.csv     \n",
            "  inflating: Samples/rec_208.csv     \n",
            "  inflating: Samples/rec_209.csv     \n",
            "  inflating: Samples/rec_21.csv      \n",
            "  inflating: Samples/rec_210.csv     \n",
            "  inflating: Samples/rec_211.csv     \n",
            "  inflating: Samples/rec_212.csv     \n",
            "  inflating: Samples/rec_213.csv     \n",
            "  inflating: Samples/rec_214.csv     \n",
            "  inflating: Samples/rec_215.csv     \n",
            "  inflating: Samples/rec_216.csv     \n",
            "  inflating: Samples/rec_217.csv     \n",
            "  inflating: Samples/rec_218.csv     \n",
            "  inflating: Samples/rec_219.csv     \n",
            "  inflating: Samples/rec_22.csv      \n",
            "  inflating: Samples/rec_220.csv     \n",
            "  inflating: Samples/rec_221.csv     \n",
            "  inflating: Samples/rec_222.csv     \n",
            "  inflating: Samples/rec_223.csv     \n",
            "  inflating: Samples/rec_224.csv     \n",
            "  inflating: Samples/rec_225.csv     \n",
            "  inflating: Samples/rec_226.csv     \n",
            "  inflating: Samples/rec_227.csv     \n",
            "  inflating: Samples/rec_228.csv     \n",
            "  inflating: Samples/rec_229.csv     \n",
            "  inflating: Samples/rec_23.csv      \n",
            "  inflating: Samples/rec_230.csv     \n",
            "  inflating: Samples/rec_231.csv     \n",
            "  inflating: Samples/rec_232.csv     \n",
            "  inflating: Samples/rec_233.csv     \n",
            "  inflating: Samples/rec_234.csv     \n",
            "  inflating: Samples/rec_235.csv     \n",
            "  inflating: Samples/rec_236.csv     \n",
            "  inflating: Samples/rec_237.csv     \n",
            "  inflating: Samples/rec_238.csv     \n",
            "  inflating: Samples/rec_239.csv     \n",
            "  inflating: Samples/rec_24.csv      \n",
            "  inflating: Samples/rec_240.csv     \n",
            "  inflating: Samples/rec_241.csv     \n",
            "  inflating: Samples/rec_242.csv     \n",
            "  inflating: Samples/rec_243.csv     \n",
            "  inflating: Samples/rec_244.csv     \n",
            "  inflating: Samples/rec_245.csv     \n",
            "  inflating: Samples/rec_246.csv     \n",
            "  inflating: Samples/rec_247.csv     \n",
            "  inflating: Samples/rec_248.csv     \n",
            "  inflating: Samples/rec_249.csv     \n",
            "  inflating: Samples/rec_25.csv      \n",
            "  inflating: Samples/rec_250.csv     \n",
            "  inflating: Samples/rec_251.csv     \n",
            "  inflating: Samples/rec_252.csv     \n",
            "  inflating: Samples/rec_253.csv     \n",
            "  inflating: Samples/rec_254.csv     \n",
            "  inflating: Samples/rec_255.csv     \n",
            "  inflating: Samples/rec_256.csv     \n",
            "  inflating: Samples/rec_257.csv     \n",
            "  inflating: Samples/rec_258.csv     \n",
            "  inflating: Samples/rec_259.csv     \n",
            "  inflating: Samples/rec_26.csv      \n",
            "  inflating: Samples/rec_260.csv     \n",
            "  inflating: Samples/rec_261.csv     \n",
            "  inflating: Samples/rec_262.csv     \n",
            "  inflating: Samples/rec_263.csv     \n",
            "  inflating: Samples/rec_264.csv     \n",
            "  inflating: Samples/rec_265.csv     \n",
            "  inflating: Samples/rec_266.csv     \n",
            "  inflating: Samples/rec_267.csv     \n",
            "  inflating: Samples/rec_268.csv     \n",
            "  inflating: Samples/rec_269.csv     \n",
            "  inflating: Samples/rec_27.csv      \n",
            "  inflating: Samples/rec_270.csv     \n",
            "  inflating: Samples/rec_271.csv     \n",
            "  inflating: Samples/rec_272.csv     \n",
            "  inflating: Samples/rec_273.csv     \n",
            "  inflating: Samples/rec_274.csv     \n",
            "  inflating: Samples/rec_275.csv     \n",
            "  inflating: Samples/rec_276.csv     \n",
            "  inflating: Samples/rec_277.csv     \n",
            "  inflating: Samples/rec_278.csv     \n",
            "  inflating: Samples/rec_279.csv     \n",
            "  inflating: Samples/rec_28.csv      \n",
            "  inflating: Samples/rec_280.csv     \n",
            "  inflating: Samples/rec_281.csv     \n",
            "  inflating: Samples/rec_282.csv     \n",
            "  inflating: Samples/rec_283.csv     \n",
            "  inflating: Samples/rec_284.csv     \n",
            "  inflating: Samples/rec_285.csv     \n",
            "  inflating: Samples/rec_286.csv     \n",
            "  inflating: Samples/rec_287.csv     \n",
            "  inflating: Samples/rec_288.csv     \n",
            "  inflating: Samples/rec_289.csv     \n",
            "  inflating: Samples/rec_29.csv      \n",
            "  inflating: Samples/rec_290.csv     \n",
            "  inflating: Samples/rec_291.csv     \n",
            "  inflating: Samples/rec_292.csv     \n",
            "  inflating: Samples/rec_293.csv     \n",
            "  inflating: Samples/rec_294.csv     \n",
            "  inflating: Samples/rec_295.csv     \n",
            "  inflating: Samples/rec_296.csv     \n",
            "  inflating: Samples/rec_297.csv     \n",
            "  inflating: Samples/rec_298.csv     \n",
            "  inflating: Samples/rec_299.csv     \n",
            "  inflating: Samples/rec_3.csv       \n",
            "  inflating: Samples/rec_30.csv      \n",
            "  inflating: Samples/rec_300.csv     \n",
            "  inflating: Samples/rec_301.csv     \n",
            "  inflating: Samples/rec_302.csv     \n",
            "  inflating: Samples/rec_303.csv     \n",
            "  inflating: Samples/rec_304.csv     \n",
            "  inflating: Samples/rec_305.csv     \n",
            "  inflating: Samples/rec_306.csv     \n",
            "  inflating: Samples/rec_307.csv     \n",
            "  inflating: Samples/rec_308.csv     \n",
            "  inflating: Samples/rec_309.csv     \n",
            "  inflating: Samples/rec_31.csv      \n",
            "  inflating: Samples/rec_310.csv     \n",
            "  inflating: Samples/rec_311.csv     \n",
            "  inflating: Samples/rec_312.csv     \n",
            "  inflating: Samples/rec_313.csv     \n",
            "  inflating: Samples/rec_314.csv     \n",
            "  inflating: Samples/rec_315.csv     \n",
            "  inflating: Samples/rec_316.csv     \n",
            "  inflating: Samples/rec_317.csv     \n",
            "  inflating: Samples/rec_318.csv     \n",
            "  inflating: Samples/rec_319.csv     \n",
            "  inflating: Samples/rec_32.csv      \n",
            "  inflating: Samples/rec_320.csv     \n",
            "  inflating: Samples/rec_321.csv     \n",
            "  inflating: Samples/rec_322.csv     \n",
            "  inflating: Samples/rec_323.csv     \n",
            "  inflating: Samples/rec_324.csv     \n",
            "  inflating: Samples/rec_325.csv     \n",
            "  inflating: Samples/rec_326.csv     \n",
            "  inflating: Samples/rec_327.csv     \n",
            "  inflating: Samples/rec_328.csv     \n",
            "  inflating: Samples/rec_329.csv     \n",
            "  inflating: Samples/rec_33.csv      \n",
            "  inflating: Samples/rec_330.csv     \n",
            "  inflating: Samples/rec_331.csv     \n",
            "  inflating: Samples/rec_332.csv     \n",
            "  inflating: Samples/rec_333.csv     \n",
            "  inflating: Samples/rec_334.csv     \n",
            "  inflating: Samples/rec_335.csv     \n",
            "  inflating: Samples/rec_336.csv     \n",
            "  inflating: Samples/rec_337.csv     \n",
            "  inflating: Samples/rec_338.csv     \n",
            "  inflating: Samples/rec_339.csv     \n",
            "  inflating: Samples/rec_34.csv      \n",
            "  inflating: Samples/rec_340.csv     \n",
            "  inflating: Samples/rec_341.csv     \n",
            "  inflating: Samples/rec_342.csv     \n",
            "  inflating: Samples/rec_343.csv     \n",
            "  inflating: Samples/rec_344.csv     \n",
            "  inflating: Samples/rec_345.csv     \n",
            "  inflating: Samples/rec_346.csv     \n",
            "  inflating: Samples/rec_347.csv     \n",
            "  inflating: Samples/rec_348.csv     \n",
            "  inflating: Samples/rec_349.csv     \n",
            "  inflating: Samples/rec_35.csv      \n",
            "  inflating: Samples/rec_350.csv     \n",
            "  inflating: Samples/rec_351.csv     \n",
            "  inflating: Samples/rec_352.csv     \n",
            "  inflating: Samples/rec_353.csv     \n",
            "  inflating: Samples/rec_354.csv     \n",
            "  inflating: Samples/rec_355.csv     \n",
            "  inflating: Samples/rec_356.csv     \n",
            "  inflating: Samples/rec_357.csv     \n",
            "  inflating: Samples/rec_358.csv     \n",
            "  inflating: Samples/rec_359.csv     \n",
            "  inflating: Samples/rec_36.csv      \n",
            "  inflating: Samples/rec_360.csv     \n",
            "  inflating: Samples/rec_361.csv     \n",
            "  inflating: Samples/rec_362.csv     \n",
            "  inflating: Samples/rec_363.csv     \n",
            "  inflating: Samples/rec_364.csv     \n",
            "  inflating: Samples/rec_365.csv     \n",
            "  inflating: Samples/rec_366.csv     \n",
            "  inflating: Samples/rec_367.csv     \n",
            "  inflating: Samples/rec_368.csv     \n",
            "  inflating: Samples/rec_369.csv     \n",
            "  inflating: Samples/rec_37.csv      \n",
            "  inflating: Samples/rec_370.csv     \n",
            "  inflating: Samples/rec_371.csv     \n",
            "  inflating: Samples/rec_372.csv     \n",
            "  inflating: Samples/rec_373.csv     \n",
            "  inflating: Samples/rec_374.csv     \n",
            "  inflating: Samples/rec_375.csv     \n",
            "  inflating: Samples/rec_376.csv     \n",
            "  inflating: Samples/rec_377.csv     \n",
            "  inflating: Samples/rec_378.csv     \n",
            "  inflating: Samples/rec_379.csv     \n",
            "  inflating: Samples/rec_38.csv      \n",
            "  inflating: Samples/rec_380.csv     \n",
            "  inflating: Samples/rec_381.csv     \n",
            "  inflating: Samples/rec_382.csv     \n",
            "  inflating: Samples/rec_383.csv     \n",
            "  inflating: Samples/rec_384.csv     \n",
            "  inflating: Samples/rec_385.csv     \n",
            "  inflating: Samples/rec_386.csv     \n",
            "  inflating: Samples/rec_387.csv     \n",
            "  inflating: Samples/rec_388.csv     \n",
            "  inflating: Samples/rec_389.csv     \n",
            "  inflating: Samples/rec_39.csv      \n",
            "  inflating: Samples/rec_390.csv     \n",
            "  inflating: Samples/rec_391.csv     \n",
            "  inflating: Samples/rec_392.csv     \n",
            "  inflating: Samples/rec_393.csv     \n",
            "  inflating: Samples/rec_394.csv     \n",
            "  inflating: Samples/rec_395.csv     \n",
            "  inflating: Samples/rec_396.csv     \n",
            "  inflating: Samples/rec_397.csv     \n",
            "  inflating: Samples/rec_398.csv     \n",
            "  inflating: Samples/rec_399.csv     \n",
            "  inflating: Samples/rec_4.csv       \n",
            "  inflating: Samples/rec_40.csv      \n",
            "  inflating: Samples/rec_400.csv     \n",
            "  inflating: Samples/rec_401.csv     \n",
            "  inflating: Samples/rec_402.csv     \n",
            "  inflating: Samples/rec_403.csv     \n",
            "  inflating: Samples/rec_404.csv     \n",
            "  inflating: Samples/rec_405.csv     \n",
            "  inflating: Samples/rec_406.csv     \n",
            "  inflating: Samples/rec_407.csv     \n",
            "  inflating: Samples/rec_408.csv     \n",
            "  inflating: Samples/rec_409.csv     \n",
            "  inflating: Samples/rec_41.csv      \n",
            "  inflating: Samples/rec_410.csv     \n",
            "  inflating: Samples/rec_411.csv     \n",
            "  inflating: Samples/rec_412.csv     \n",
            "  inflating: Samples/rec_413.csv     \n",
            "  inflating: Samples/rec_414.csv     \n",
            "  inflating: Samples/rec_415.csv     \n",
            "  inflating: Samples/rec_416.csv     \n",
            "  inflating: Samples/rec_417.csv     \n",
            "  inflating: Samples/rec_418.csv     \n",
            "  inflating: Samples/rec_419.csv     \n",
            "  inflating: Samples/rec_42.csv      \n",
            "  inflating: Samples/rec_420.csv     \n",
            "  inflating: Samples/rec_421.csv     \n",
            "  inflating: Samples/rec_422.csv     \n",
            "  inflating: Samples/rec_423.csv     \n",
            "  inflating: Samples/rec_424.csv     \n",
            "  inflating: Samples/rec_425.csv     \n",
            "  inflating: Samples/rec_426.csv     \n",
            "  inflating: Samples/rec_427.csv     \n",
            "  inflating: Samples/rec_428.csv     \n",
            "  inflating: Samples/rec_429.csv     \n",
            "  inflating: Samples/rec_43.csv      \n",
            "  inflating: Samples/rec_430.csv     \n",
            "  inflating: Samples/rec_431.csv     \n",
            "  inflating: Samples/rec_432.csv     \n",
            "  inflating: Samples/rec_433.csv     \n",
            "  inflating: Samples/rec_434.csv     \n",
            "  inflating: Samples/rec_435.csv     \n",
            "  inflating: Samples/rec_436.csv     \n",
            "  inflating: Samples/rec_437.csv     \n",
            "  inflating: Samples/rec_438.csv     \n",
            "  inflating: Samples/rec_439.csv     \n",
            "  inflating: Samples/rec_44.csv      \n",
            "  inflating: Samples/rec_440.csv     \n",
            "  inflating: Samples/rec_441.csv     \n",
            "  inflating: Samples/rec_442.csv     \n",
            "  inflating: Samples/rec_443.csv     \n",
            "  inflating: Samples/rec_444.csv     \n",
            "  inflating: Samples/rec_445.csv     \n",
            "  inflating: Samples/rec_446.csv     \n",
            "  inflating: Samples/rec_447.csv     \n",
            "  inflating: Samples/rec_448.csv     \n",
            "  inflating: Samples/rec_449.csv     \n",
            "  inflating: Samples/rec_45.csv      \n",
            "  inflating: Samples/rec_450.csv     \n",
            "  inflating: Samples/rec_451.csv     \n",
            "  inflating: Samples/rec_452.csv     \n",
            "  inflating: Samples/rec_453.csv     \n",
            "  inflating: Samples/rec_454.csv     \n",
            "  inflating: Samples/rec_455.csv     \n",
            "  inflating: Samples/rec_456.csv     \n",
            "  inflating: Samples/rec_457.csv     \n",
            "  inflating: Samples/rec_458.csv     \n",
            "  inflating: Samples/rec_459.csv     \n",
            "  inflating: Samples/rec_46.csv      \n",
            "  inflating: Samples/rec_460.csv     \n",
            "  inflating: Samples/rec_461.csv     \n",
            "  inflating: Samples/rec_462.csv     \n",
            "  inflating: Samples/rec_463.csv     \n",
            "  inflating: Samples/rec_464.csv     \n",
            "  inflating: Samples/rec_465.csv     \n",
            "  inflating: Samples/rec_466.csv     \n",
            "  inflating: Samples/rec_467.csv     \n",
            "  inflating: Samples/rec_468.csv     \n",
            "  inflating: Samples/rec_469.csv     \n",
            "  inflating: Samples/rec_47.csv      \n",
            "  inflating: Samples/rec_470.csv     \n",
            "  inflating: Samples/rec_471.csv     \n",
            "  inflating: Samples/rec_472.csv     \n",
            "  inflating: Samples/rec_473.csv     \n",
            "  inflating: Samples/rec_474.csv     \n",
            "  inflating: Samples/rec_475.csv     \n",
            "  inflating: Samples/rec_476.csv     \n",
            "  inflating: Samples/rec_477.csv     \n",
            "  inflating: Samples/rec_478.csv     \n",
            "  inflating: Samples/rec_479.csv     \n",
            "  inflating: Samples/rec_48.csv      \n",
            "  inflating: Samples/rec_480.csv     \n",
            "  inflating: Samples/rec_481.csv     \n",
            "  inflating: Samples/rec_482.csv     \n",
            "  inflating: Samples/rec_483.csv     \n",
            "  inflating: Samples/rec_484.csv     \n",
            "  inflating: Samples/rec_485.csv     \n",
            "  inflating: Samples/rec_486.csv     \n",
            "  inflating: Samples/rec_487.csv     \n",
            "  inflating: Samples/rec_488.csv     \n",
            "  inflating: Samples/rec_489.csv     \n",
            "  inflating: Samples/rec_49.csv      \n",
            "  inflating: Samples/rec_490.csv     \n",
            "  inflating: Samples/rec_491.csv     \n",
            "  inflating: Samples/rec_492.csv     \n",
            "  inflating: Samples/rec_493.csv     \n",
            "  inflating: Samples/rec_494.csv     \n",
            "  inflating: Samples/rec_495.csv     \n",
            "  inflating: Samples/rec_496.csv     \n",
            "  inflating: Samples/rec_497.csv     \n",
            "  inflating: Samples/rec_498.csv     \n",
            "  inflating: Samples/rec_499.csv     \n",
            "  inflating: Samples/rec_5.csv       \n",
            "  inflating: Samples/rec_50.csv      \n",
            "  inflating: Samples/rec_500.csv     \n",
            "  inflating: Samples/rec_51.csv      \n",
            "  inflating: Samples/rec_52.csv      \n",
            "  inflating: Samples/rec_53.csv      \n",
            "  inflating: Samples/rec_54.csv      \n",
            "  inflating: Samples/rec_55.csv      \n",
            "  inflating: Samples/rec_56.csv      \n",
            "  inflating: Samples/rec_57.csv      \n",
            "  inflating: Samples/rec_58.csv      \n",
            "  inflating: Samples/rec_59.csv      \n",
            "  inflating: Samples/rec_6.csv       \n",
            "  inflating: Samples/rec_60.csv      \n",
            "  inflating: Samples/rec_61.csv      \n",
            "  inflating: Samples/rec_62.csv      \n",
            "  inflating: Samples/rec_63.csv      \n",
            "  inflating: Samples/rec_64.csv      \n",
            "  inflating: Samples/rec_65.csv      \n",
            "  inflating: Samples/rec_66.csv      \n",
            "  inflating: Samples/rec_67.csv      \n",
            "  inflating: Samples/rec_68.csv      \n",
            "  inflating: Samples/rec_69.csv      \n",
            "  inflating: Samples/rec_7.csv       \n",
            "  inflating: Samples/rec_70.csv      \n",
            "  inflating: Samples/rec_71.csv      \n",
            "  inflating: Samples/rec_72.csv      \n",
            "  inflating: Samples/rec_73.csv      \n",
            "  inflating: Samples/rec_74.csv      \n",
            "  inflating: Samples/rec_75.csv      \n",
            "  inflating: Samples/rec_76.csv      \n",
            "  inflating: Samples/rec_77.csv      \n",
            "  inflating: Samples/rec_78.csv      \n",
            "  inflating: Samples/rec_79.csv      \n",
            "  inflating: Samples/rec_8.csv       \n",
            "  inflating: Samples/rec_80.csv      \n",
            "  inflating: Samples/rec_81.csv      \n",
            "  inflating: Samples/rec_82.csv      \n",
            "  inflating: Samples/rec_83.csv      \n",
            "  inflating: Samples/rec_84.csv      \n",
            "  inflating: Samples/rec_85.csv      \n",
            "  inflating: Samples/rec_86.csv      \n",
            "  inflating: Samples/rec_87.csv      \n",
            "  inflating: Samples/rec_88.csv      \n",
            "  inflating: Samples/rec_89.csv      \n",
            "  inflating: Samples/rec_9.csv       \n",
            "  inflating: Samples/rec_90.csv      \n",
            "  inflating: Samples/rec_91.csv      \n",
            "  inflating: Samples/rec_92.csv      \n",
            "  inflating: Samples/rec_93.csv      \n",
            "  inflating: Samples/rec_94.csv      \n",
            "  inflating: Samples/rec_95.csv      \n",
            "  inflating: Samples/rec_96.csv      \n",
            "  inflating: Samples/rec_97.csv      \n",
            "  inflating: Samples/rec_98.csv      \n",
            "  inflating: Samples/rec_99.csv      \n",
            "  inflating: part_1.mat              \n",
            "  inflating: part_10.mat             \n",
            "  inflating: part_11.mat             \n",
            "  inflating: part_12.mat             \n",
            "  inflating: part_2.mat              \n",
            "  inflating: part_3.mat              \n",
            "  inflating: part_4.mat              \n",
            "  inflating: part_5.mat              \n",
            "  inflating: part_6.mat              \n",
            "  inflating: part_7.mat              \n",
            "  inflating: part_8.mat              \n",
            "  inflating: part_9.mat              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PPG_datas = []\n",
        "ABP_datas = []\n",
        "ECG_datas = []\n",
        "i = 0\n",
        "for name in glob.glob('./Samples/*.csv'):\n",
        "    raw_training_data = pd.read_csv(name, header=None)\n",
        "    raw_training_data = np.array(raw_training_data)\n",
        "    #print(raw_training_data.shape)\n",
        "    PPG_data = raw_training_data[0]\n",
        "    ABP_data = raw_training_data[1]\n",
        "    ECG_data = raw_training_data[2]\n",
        "\n",
        "    PPG_datas.append(PPG_data)\n",
        "    ABP_datas.append(ABP_data)\n",
        "    ECG_datas.append(ECG_data)\n",
        "    # i = i + 1\n",
        " #   print(i)\n",
        "#     if (i == 100):\n",
        "#         break\n",
        "\n",
        "PPG_datas = np.array(PPG_datas)\n",
        "ABP_datas = np.array(ABP_datas)\n",
        "ECG_datas = np.array(ECG_datas)\n",
        "# raw_training_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxlNVPrXC5uc",
        "outputId": "b45c9fb6-e441-4141-eff1-e288dd655622"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-cafb0a523fd7>:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  PPG_datas = np.array(PPG_datas)\n",
            "<ipython-input-5-cafb0a523fd7>:22: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  ABP_datas = np.array(ABP_datas)\n",
            "<ipython-input-5-cafb0a523fd7>:23: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  ECG_datas = np.array(ECG_datas)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pywt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def generate_wavelet_vector(X, wavelet_family, decomposition_level):\n",
        "    coefficients = pywt.wavedec(X, wavelet_family, level=decomposition_level)\n",
        "    vector = np.array([])\n",
        "    for coeffs in coefficients:\n",
        "        vector = np.concatenate((vector, coeffs))\n",
        "    return vector"
      ],
      "metadata": {
        "id": "5jJDoNoVC5xK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLE_FREQ = 125"
      ],
      "metadata": {
        "id": "QHNyTafbC50Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wavelet_family = 'db4'\n",
        "decomposition_level = 5\n",
        "\n",
        "wavelet_vectors = []\n",
        "SBP_data = []\n",
        "DBP_data = []\n",
        "MAP_data = []\n",
        "\n",
        "for j in range(len(PPG_datas)):\n",
        "    sec_15 = 15 * SAMPLE_FREQ\n",
        "    PPG_data = PPG_datas[j]\n",
        "    ABP_data = ABP_datas[j]\n",
        "    PPG_peaks = find_peaks(PPG_data, scale=SAMPLE_FREQ)\n",
        "    for i in range(2, PPG_peaks.shape[0]):\n",
        "        X = PPG_data[PPG_peaks[i - 1]:PPG_peaks[i]]\n",
        "        if len(X) < SAMPLE_FREQ:\n",
        "            wavelet_vector = generate_wavelet_vector(X, wavelet_family, decomposition_level)\n",
        "\n",
        "            SBP = np.max(ABP_data[PPG_peaks[i - 1]:PPG_peaks[i - 1] + sec_15])\n",
        "            DBP = np.min(ABP_data[PPG_peaks[i - 1]:PPG_peaks[i - 1] + sec_15])\n",
        "            MAP = SBP / 3 + 2 * DBP / 3\n",
        "\n",
        "            wavelet_vectors.append(wavelet_vector)\n",
        "            SBP_data.append(SBP)\n",
        "            DBP_data.append(DBP)\n",
        "            MAP_data.append(MAP)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVEmBw5IC53K",
        "outputId": "04f4d101-b570-4a05-c1b6-033ed30fd852"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pywt/_multilevel.py:43: UserWarning: Level value of 5 is too high: all coefficients will experience boundary effects.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wavelet_vectors = np.array(wavelet_vectors)\n",
        "SBP_data = np.array(SBP_data)\n",
        "DBP_data = np.array(DBP_data)\n",
        "MAP_data = np.array(MAP_data)\n",
        "\n",
        "print(wavelet_vectors.shape)\n",
        "print(SBP_data.shape)\n",
        "\n",
        "# Find the maximum length of the wavelet vectors\n",
        "max_length = max([len(vector) for vector in wavelet_vectors])\n",
        "\n",
        "# Pad the wavelet vectors with zeros to make them uniform length\n",
        "padded_wavelet_vectors = []\n",
        "for vector in wavelet_vectors:\n",
        "    padded_vector = np.pad(vector, (0, max_length - len(vector)), mode='constant')\n",
        "    padded_wavelet_vectors.append(padded_vector)\n",
        "\n",
        "padded_wavelet_vectors = np.array(padded_wavelet_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBtX4J50C56H",
        "outputId": "c4598474-06de-4c03-b0bd-c440a5f65848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-6f3e639507bb>:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  wavelet_vectors = np.array(wavelet_vectors)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(196625,)\n",
            "(196625,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=43)\n",
        "pca_wavelet_vectors = pca.fit_transform(padded_wavelet_vectors)\n",
        "\n",
        "print(pca_wavelet_vectors.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWu8VHzWC58_",
        "outputId": "85f4b6ca-f269-4d21-ecdf-6451cabe4d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(196625, 43)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, SBP_train, SBP_test, DBP_train, DBP_test, MAP_train, MAP_test = train_test_split(\n",
        "    pca_wavelet_vectors, SBP_data, DBP_data, MAP_data, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "wUZzqXSqC5_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def AAMI_standard(predict, test):\n",
        "    total = len(predict)\n",
        "    ME = np.mean(predict - test)\n",
        "    MAE = np.mean(np.abs(predict-test))\n",
        "    SD = np.std(predict-test)\n",
        "\n",
        "    return total, ME, MAE, SD\n",
        "\n",
        "def BHS_standard(predict, test):\n",
        "    total = len(predict)\n",
        "    mm5 = np.sum(np.abs(predict-test)<=5)\n",
        "    mm10 = np.sum(np.abs(predict-test)<=10)\n",
        "    mm15 = np.sum(np.abs(predict-test)<=15)\n",
        "    return total, mm5, mm10, mm15"
      ],
      "metadata": {
        "id": "SUVa-4lXC6C5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 2\n",
        "LR = 0.001"
      ],
      "metadata": {
        "id": "B5Kt2xTgC6F2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class NN(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(NN, self).__init__()\n",
        "        self.input = nn.Linear(in_features=input_size, out_features=256)\n",
        "        self.hidden1 = nn.Linear(in_features=256, out_features=512)\n",
        "        self.hidden2 = nn.Linear(in_features=512, out_features=256)\n",
        "        self.hidden3 = nn.Linear(in_features=256, out_features=32)\n",
        "        self.output = nn.Linear(in_features=32, out_features=1)\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        self.activation = lambda X: F.relu(X)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.input(x)\n",
        "        out = self.hidden1(self.activation(out))\n",
        "        out = self.dropout(out)\n",
        "        out = self.hidden2(self.activation(out))\n",
        "        out = self.hidden3(self.activation(out))\n",
        "#         out = self.dropout(out)\n",
        "        out = self.output(self.activation(out))\n",
        "        return out\n",
        "\n",
        "    def get_weight(self):\n",
        "        weight = []\n",
        "        weight.append(self.input.weight)\n",
        "        weight.append(self.hidden1.weight)\n",
        "        weight.append(self.hidden2.weight)\n",
        "        weight.append(self.hidden3.weight)\n",
        "        weight.append(self.output.weight)\n",
        "        return weight"
      ],
      "metadata": {
        "id": "jKFy_8iKC6JR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_default_tensor_type(torch.FloatTensor)\n",
        "\n",
        "class module():\n",
        "    def __init__(self, net, train_loader, test_loader, EPOCH=20, LR=0.0001):\n",
        "        self.net = net\n",
        "        self.optimizer = torch.optim.Adam(net.parameters(), lr = LR)\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.EPOCHS = EPOCH\n",
        "        self.LR = LR\n",
        "\n",
        "        self.net = self.net.to(device)\n",
        "        if device == 'cuda':\n",
        "            # self.net = torch.nn.DataParallel(self.net)\n",
        "            torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    def start(self):\n",
        "\n",
        "        self.history_train_loss = []\n",
        "        self.history_test_loss = []\n",
        "#         history_train_accuracy = []\n",
        "#         history_test_accuracy = []\n",
        "        for epoch in range(self.EPOCHS):\n",
        "            print('Epoch:', epoch)\n",
        "            train_loss = self.train()\n",
        "            test_loss = self.test()\n",
        "\n",
        "            self.history_train_loss.append(train_loss)\n",
        "            self.history_test_loss.append(test_loss)\n",
        "#             history_train_accuracy.append(train_accuracy)\n",
        "#             history_test_accuracy.append(test_accuracy)\n",
        "\n",
        "    def train(self):\n",
        "        self.net.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for step, (batch_X, batch_y) in enumerate(self.train_loader):\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.net(batch_X).double()\n",
        "\n",
        "            loss = self.criterion(outputs.reshape(-1), batch_y)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            #_, predicted = outputs.max(1)\n",
        "            #total += batch_y.size(0)\n",
        "            #correct += predicted.eq(batch_y).sum().item()\n",
        "\n",
        "        print('TrainingLoss: %.3f ' % ( train_loss))\n",
        "        return train_loss\n",
        "\n",
        "    def test(self):\n",
        "        self.net.eval()\n",
        "\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for step, (batch_X, batch_y) in enumerate(self.test_loader):\n",
        "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "                outputs = self.net(batch_X).double()\n",
        "                loss = self.criterion(outputs.reshape(-1), batch_y)\n",
        "\n",
        "                test_loss += loss.item()\n",
        "#                 _, predicted = outputs.max(1)\n",
        "#                 #print(predicted, batch_y)\n",
        "#                 total += batch_y.size(0)\n",
        "#                 correct += predicted.eq(batch_y).sum().item()\n",
        "\n",
        "        print('TestingLoss: %.3f )' % ( test_loss))\n",
        "        return test_loss\n",
        "\n",
        "    def predict(self, test_loader):\n",
        "        outputs = []\n",
        "        self.net.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for step, (batch_X, batch_y) in enumerate(test_loader):\n",
        "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "                output = self.net(batch_X).double().cpu().numpy().reshape(-1)\n",
        "                for i in range(len(output)):\n",
        "                    outputs.append(output[i])\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def get_weight(self):\n",
        "        return self.net.get_weight()\n",
        "\n",
        "    def get_model(self):\n",
        "        return self.net.state_dict()"
      ],
      "metadata": {
        "id": "VzLvkNjzC6MF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "L = 43\n",
        "\n",
        "x_train, x_test, SBP_train, SBP_test, DBP_train, DBP_test, MAP_train, MAP_test = train_test_split(\n",
        "                                            pca_wavelet_vectors, SBP_data, DBP_data, MAP_data, test_size=0.1, random_state=42)\n",
        "\n",
        "\n",
        "x_train = torch.from_numpy(x_train).type(torch.FloatTensor)\n",
        "x_test = torch.from_numpy(x_test).type(torch.FloatTensor)\n",
        "SBP_train = torch.from_numpy(SBP_train)\n",
        "SBP_test = torch.from_numpy(SBP_test)\n",
        "DBP_train = torch.from_numpy(DBP_train)\n",
        "DBP_test = torch.from_numpy(DBP_test)\n",
        "MAP_train = torch.from_numpy(MAP_train)\n",
        "MAP_test = torch.from_numpy(MAP_test)\n",
        "\n",
        "x_train = x_train.view(-1, 1, L)\n",
        "x_test = x_test.view(-1, 1, L)\n",
        "\n",
        "print(x_train.shape, x_test.shape, SBP_train.shape, SBP_test.shape)\n",
        "\n",
        "SBP_train_dataset = Data.TensorDataset(x_train, SBP_train)\n",
        "SBP_test_dataset = Data.TensorDataset(x_test, SBP_test)\n",
        "\n",
        "\n",
        "SBP_train_loader = Data.DataLoader(\n",
        "    dataset = SBP_train_dataset,\n",
        "    batch_size = 128,\n",
        ")\n",
        "\n",
        "SBP_test_loader = Data.DataLoader(\n",
        "    dataset = SBP_test_dataset,\n",
        "    batch_size = 10,\n",
        ")\n",
        "\n",
        "DBP_train_dataset = Data.TensorDataset(x_train, DBP_train)\n",
        "DBP_test_dataset = Data.TensorDataset(x_test, DBP_test)\n",
        "\n",
        "\n",
        "DBP_train_loader = Data.DataLoader(\n",
        "    dataset = DBP_train_dataset,\n",
        "    batch_size = 128,\n",
        ")\n",
        "\n",
        "DBP_test_loader = Data.DataLoader(\n",
        "    dataset = DBP_test_dataset,\n",
        "    batch_size = 10,\n",
        ")\n",
        "\n",
        "MAP_train_dataset = Data.TensorDataset(x_train, MAP_train)\n",
        "MAP_test_dataset = Data.TensorDataset(x_test, MAP_test)\n",
        "\n",
        "\n",
        "MAP_train_loader = Data.DataLoader(\n",
        "    dataset = MAP_train_dataset,\n",
        "    batch_size = 128,\n",
        ")\n",
        "\n",
        "MAP_test_loader = Data.DataLoader(\n",
        "    dataset = MAP_test_dataset,\n",
        "    batch_size = 10,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8K1dzBlhC6O-",
        "outputId": "9b14f043-f297-4e8d-e039-a7042cf63ab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([176962, 1, 43]) torch.Size([19663, 1, 43]) torch.Size([176962]) torch.Size([19663])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SBP_nn_module = module(NN(input_size=L), SBP_train_loader, SBP_test_loader, EPOCH=200, LR=LR)\n",
        "SBP_nn_module.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txfFl94dC6Sd",
        "outputId": "e3a7bf18-e61f-43a4-e2b1-8bfd43bd6b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "TrainingLoss: 872680.150 \n",
            "TestingLoss: 644464.685 )\n",
            "Epoch: 1\n",
            "TrainingLoss: 428723.692 \n",
            "TestingLoss: 555209.251 )\n",
            "Epoch: 2\n",
            "TrainingLoss: 393668.606 \n",
            "TestingLoss: 530252.214 )\n",
            "Epoch: 3\n",
            "TrainingLoss: 368274.669 \n",
            "TestingLoss: 491782.138 )\n",
            "Epoch: 4\n",
            "TrainingLoss: 350748.185 \n",
            "TestingLoss: 485031.810 )\n",
            "Epoch: 5\n",
            "TrainingLoss: 336544.386 \n",
            "TestingLoss: 457054.129 )\n",
            "Epoch: 6\n",
            "TrainingLoss: 326448.952 \n",
            "TestingLoss: 443302.838 )\n",
            "Epoch: 7\n",
            "TrainingLoss: 316288.635 \n",
            "TestingLoss: 438519.210 )\n",
            "Epoch: 8\n",
            "TrainingLoss: 307381.548 \n",
            "TestingLoss: 443090.357 )\n",
            "Epoch: 9\n",
            "TrainingLoss: 300188.553 \n",
            "TestingLoss: 411607.666 )\n",
            "Epoch: 10\n",
            "TrainingLoss: 294988.128 \n",
            "TestingLoss: 422250.992 )\n",
            "Epoch: 11\n",
            "TrainingLoss: 289077.418 \n",
            "TestingLoss: 408962.827 )\n",
            "Epoch: 12\n",
            "TrainingLoss: 284416.254 \n",
            "TestingLoss: 395065.073 )\n",
            "Epoch: 13\n",
            "TrainingLoss: 280672.074 \n",
            "TestingLoss: 392292.473 )\n",
            "Epoch: 14\n",
            "TrainingLoss: 276872.565 \n",
            "TestingLoss: 385784.699 )\n",
            "Epoch: 15\n",
            "TrainingLoss: 273058.388 \n",
            "TestingLoss: 379449.809 )\n",
            "Epoch: 16\n",
            "TrainingLoss: 269097.291 \n",
            "TestingLoss: 366093.685 )\n",
            "Epoch: 17\n",
            "TrainingLoss: 266554.554 \n",
            "TestingLoss: 368804.206 )\n",
            "Epoch: 18\n",
            "TrainingLoss: 263992.988 \n",
            "TestingLoss: 359080.424 )\n",
            "Epoch: 19\n",
            "TrainingLoss: 261377.747 \n",
            "TestingLoss: 356385.247 )\n",
            "Epoch: 20\n",
            "TrainingLoss: 258986.428 \n",
            "TestingLoss: 357783.731 )\n",
            "Epoch: 21\n",
            "TrainingLoss: 256134.592 \n",
            "TestingLoss: 354754.139 )\n",
            "Epoch: 22\n",
            "TrainingLoss: 253008.794 \n",
            "TestingLoss: 347908.981 )\n",
            "Epoch: 23\n",
            "TrainingLoss: 250876.306 \n",
            "TestingLoss: 357809.890 )\n",
            "Epoch: 24\n",
            "TrainingLoss: 248463.587 \n",
            "TestingLoss: 348506.927 )\n",
            "Epoch: 25\n",
            "TrainingLoss: 246135.359 \n",
            "TestingLoss: 337683.518 )\n",
            "Epoch: 26\n",
            "TrainingLoss: 244020.277 \n",
            "TestingLoss: 337142.470 )\n",
            "Epoch: 27\n",
            "TrainingLoss: 242617.183 \n",
            "TestingLoss: 338043.454 )\n",
            "Epoch: 28\n",
            "TrainingLoss: 239425.578 \n",
            "TestingLoss: 344677.598 )\n",
            "Epoch: 29\n",
            "TrainingLoss: 238384.875 \n",
            "TestingLoss: 329702.073 )\n",
            "Epoch: 30\n",
            "TrainingLoss: 237224.740 \n",
            "TestingLoss: 337368.515 )\n",
            "Epoch: 31\n",
            "TrainingLoss: 233938.997 \n",
            "TestingLoss: 330141.119 )\n",
            "Epoch: 32\n",
            "TrainingLoss: 232321.818 \n",
            "TestingLoss: 332914.558 )\n",
            "Epoch: 33\n",
            "TrainingLoss: 231873.269 \n",
            "TestingLoss: 325878.791 )\n",
            "Epoch: 34\n",
            "TrainingLoss: 228540.628 \n",
            "TestingLoss: 334130.985 )\n",
            "Epoch: 35\n",
            "TrainingLoss: 227733.424 \n",
            "TestingLoss: 331744.073 )\n",
            "Epoch: 36\n",
            "TrainingLoss: 226311.024 \n",
            "TestingLoss: 331914.380 )\n",
            "Epoch: 37\n",
            "TrainingLoss: 224633.137 \n",
            "TestingLoss: 322008.323 )\n",
            "Epoch: 38\n",
            "TrainingLoss: 223077.199 \n",
            "TestingLoss: 327228.260 )\n",
            "Epoch: 39\n",
            "TrainingLoss: 221188.456 \n",
            "TestingLoss: 335100.922 )\n",
            "Epoch: 40\n",
            "TrainingLoss: 219161.800 \n",
            "TestingLoss: 327478.851 )\n",
            "Epoch: 41\n",
            "TrainingLoss: 217658.330 \n",
            "TestingLoss: 322338.143 )\n",
            "Epoch: 42\n",
            "TrainingLoss: 216042.675 \n",
            "TestingLoss: 315637.736 )\n",
            "Epoch: 43\n",
            "TrainingLoss: 214830.581 \n",
            "TestingLoss: 324691.735 )\n",
            "Epoch: 44\n",
            "TrainingLoss: 213771.818 \n",
            "TestingLoss: 320111.955 )\n",
            "Epoch: 45\n",
            "TrainingLoss: 212437.124 \n",
            "TestingLoss: 328552.178 )\n",
            "Epoch: 46\n",
            "TrainingLoss: 210433.152 \n",
            "TestingLoss: 316798.393 )\n",
            "Epoch: 47\n",
            "TrainingLoss: 208873.862 \n",
            "TestingLoss: 324723.707 )\n",
            "Epoch: 48\n",
            "TrainingLoss: 207838.273 \n",
            "TestingLoss: 316617.461 )\n",
            "Epoch: 49\n",
            "TrainingLoss: 206362.492 \n",
            "TestingLoss: 311985.248 )\n",
            "Epoch: 50\n",
            "TrainingLoss: 204781.803 \n",
            "TestingLoss: 321735.900 )\n",
            "Epoch: 51\n",
            "TrainingLoss: 203585.497 \n",
            "TestingLoss: 305148.354 )\n",
            "Epoch: 52\n",
            "TrainingLoss: 201381.835 \n",
            "TestingLoss: 322305.773 )\n",
            "Epoch: 53\n",
            "TrainingLoss: 201413.829 \n",
            "TestingLoss: 314288.058 )\n",
            "Epoch: 54\n",
            "TrainingLoss: 200196.244 \n",
            "TestingLoss: 317657.268 )\n",
            "Epoch: 55\n",
            "TrainingLoss: 197467.900 \n",
            "TestingLoss: 322344.508 )\n",
            "Epoch: 56\n",
            "TrainingLoss: 196473.545 \n",
            "TestingLoss: 315943.406 )\n",
            "Epoch: 57\n",
            "TrainingLoss: 195559.769 \n",
            "TestingLoss: 309584.474 )\n",
            "Epoch: 58\n",
            "TrainingLoss: 194218.570 \n",
            "TestingLoss: 303842.820 )\n",
            "Epoch: 59\n",
            "TrainingLoss: 192457.242 \n",
            "TestingLoss: 312736.554 )\n",
            "Epoch: 60\n",
            "TrainingLoss: 190993.478 \n",
            "TestingLoss: 312994.775 )\n",
            "Epoch: 61\n",
            "TrainingLoss: 189541.000 \n",
            "TestingLoss: 307961.225 )\n",
            "Epoch: 62\n",
            "TrainingLoss: 188509.199 \n",
            "TestingLoss: 304954.517 )\n",
            "Epoch: 63\n",
            "TrainingLoss: 185443.513 \n",
            "TestingLoss: 308993.460 )\n",
            "Epoch: 64\n",
            "TrainingLoss: 184284.178 \n",
            "TestingLoss: 311483.063 )\n",
            "Epoch: 65\n",
            "TrainingLoss: 183519.176 \n",
            "TestingLoss: 299805.519 )\n",
            "Epoch: 66\n",
            "TrainingLoss: 182456.142 \n",
            "TestingLoss: 298043.685 )\n",
            "Epoch: 67\n",
            "TrainingLoss: 180498.868 \n",
            "TestingLoss: 294642.217 )\n",
            "Epoch: 68\n",
            "TrainingLoss: 179497.166 \n",
            "TestingLoss: 298066.472 )\n",
            "Epoch: 69\n",
            "TrainingLoss: 178200.798 \n",
            "TestingLoss: 298499.448 )\n",
            "Epoch: 70\n",
            "TrainingLoss: 176744.882 \n",
            "TestingLoss: 292747.768 )\n",
            "Epoch: 71\n",
            "TrainingLoss: 175406.744 \n",
            "TestingLoss: 292269.920 )\n",
            "Epoch: 72\n",
            "TrainingLoss: 173466.975 \n",
            "TestingLoss: 292467.335 )\n",
            "Epoch: 73\n",
            "TrainingLoss: 172411.124 \n",
            "TestingLoss: 294225.667 )\n",
            "Epoch: 74\n",
            "TrainingLoss: 170016.690 \n",
            "TestingLoss: 283950.291 )\n",
            "Epoch: 75\n",
            "TrainingLoss: 167565.351 \n",
            "TestingLoss: 285629.656 )\n",
            "Epoch: 76\n",
            "TrainingLoss: 165348.738 \n",
            "TestingLoss: 287390.975 )\n",
            "Epoch: 77\n",
            "TrainingLoss: 163990.092 \n",
            "TestingLoss: 283621.884 )\n",
            "Epoch: 78\n",
            "TrainingLoss: 162147.356 \n",
            "TestingLoss: 283474.787 )\n",
            "Epoch: 79\n",
            "TrainingLoss: 160170.675 \n",
            "TestingLoss: 278240.597 )\n",
            "Epoch: 80\n",
            "TrainingLoss: 158622.245 \n",
            "TestingLoss: 281592.252 )\n",
            "Epoch: 81\n",
            "TrainingLoss: 155785.587 \n",
            "TestingLoss: 273278.644 )\n",
            "Epoch: 82\n",
            "TrainingLoss: 154033.589 \n",
            "TestingLoss: 272334.306 )\n",
            "Epoch: 83\n",
            "TrainingLoss: 152919.388 \n",
            "TestingLoss: 271015.698 )\n",
            "Epoch: 84\n",
            "TrainingLoss: 150399.684 \n",
            "TestingLoss: 268148.545 )\n",
            "Epoch: 85\n",
            "TrainingLoss: 148890.309 \n",
            "TestingLoss: 272186.353 )\n",
            "Epoch: 86\n",
            "TrainingLoss: 147672.890 \n",
            "TestingLoss: 266543.049 )\n",
            "Epoch: 87\n",
            "TrainingLoss: 144763.895 \n",
            "TestingLoss: 268406.803 )\n",
            "Epoch: 88\n",
            "TrainingLoss: 143447.231 \n",
            "TestingLoss: 270023.296 )\n",
            "Epoch: 89\n",
            "TrainingLoss: 142265.423 \n",
            "TestingLoss: 265994.470 )\n",
            "Epoch: 90\n",
            "TrainingLoss: 139752.619 \n",
            "TestingLoss: 273640.214 )\n",
            "Epoch: 91\n",
            "TrainingLoss: 138093.444 \n",
            "TestingLoss: 271309.610 )\n",
            "Epoch: 92\n",
            "TrainingLoss: 136832.296 \n",
            "TestingLoss: 265411.400 )\n",
            "Epoch: 93\n",
            "TrainingLoss: 134619.277 \n",
            "TestingLoss: 268186.158 )\n",
            "Epoch: 94\n",
            "TrainingLoss: 133717.140 \n",
            "TestingLoss: 268356.545 )\n",
            "Epoch: 95\n",
            "TrainingLoss: 132520.257 \n",
            "TestingLoss: 264354.982 )\n",
            "Epoch: 96\n",
            "TrainingLoss: 130657.008 \n",
            "TestingLoss: 265111.368 )\n",
            "Epoch: 97\n",
            "TrainingLoss: 129988.208 \n",
            "TestingLoss: 267575.755 )\n",
            "Epoch: 98\n",
            "TrainingLoss: 128149.181 \n",
            "TestingLoss: 266352.685 )\n",
            "Epoch: 99\n",
            "TrainingLoss: 127048.673 \n",
            "TestingLoss: 260481.796 )\n",
            "Epoch: 100\n",
            "TrainingLoss: 126379.641 \n",
            "TestingLoss: 264038.124 )\n",
            "Epoch: 101\n",
            "TrainingLoss: 124805.601 \n",
            "TestingLoss: 261464.811 )\n",
            "Epoch: 102\n",
            "TrainingLoss: 123161.128 \n",
            "TestingLoss: 261866.111 )\n",
            "Epoch: 103\n",
            "TrainingLoss: 121561.266 \n",
            "TestingLoss: 264113.019 )\n",
            "Epoch: 104\n",
            "TrainingLoss: 120710.211 \n",
            "TestingLoss: 257129.633 )\n",
            "Epoch: 105\n",
            "TrainingLoss: 119901.298 \n",
            "TestingLoss: 266201.380 )\n",
            "Epoch: 106\n",
            "TrainingLoss: 118520.889 \n",
            "TestingLoss: 258500.424 )\n",
            "Epoch: 107\n",
            "TrainingLoss: 117808.003 \n",
            "TestingLoss: 257801.338 )\n",
            "Epoch: 108\n",
            "TrainingLoss: 115842.690 \n",
            "TestingLoss: 255024.688 )\n",
            "Epoch: 109\n",
            "TrainingLoss: 115283.356 \n",
            "TestingLoss: 262434.705 )\n",
            "Epoch: 110\n",
            "TrainingLoss: 113565.248 \n",
            "TestingLoss: 256831.400 )\n",
            "Epoch: 111\n",
            "TrainingLoss: 112505.555 \n",
            "TestingLoss: 258715.487 )\n",
            "Epoch: 112\n",
            "TrainingLoss: 112028.474 \n",
            "TestingLoss: 259834.622 )\n",
            "Epoch: 113\n",
            "TrainingLoss: 111028.925 \n",
            "TestingLoss: 257506.720 )\n",
            "Epoch: 114\n",
            "TrainingLoss: 110725.068 \n",
            "TestingLoss: 256242.733 )\n",
            "Epoch: 115\n",
            "TrainingLoss: 109495.654 \n",
            "TestingLoss: 257105.212 )\n",
            "Epoch: 116\n",
            "TrainingLoss: 107987.664 \n",
            "TestingLoss: 256041.779 )\n",
            "Epoch: 117\n",
            "TrainingLoss: 107630.278 \n",
            "TestingLoss: 258957.995 )\n",
            "Epoch: 118\n",
            "TrainingLoss: 105970.617 \n",
            "TestingLoss: 257456.807 )\n",
            "Epoch: 119\n",
            "TrainingLoss: 105910.173 \n",
            "TestingLoss: 254800.640 )\n",
            "Epoch: 120\n",
            "TrainingLoss: 105258.839 \n",
            "TestingLoss: 255746.507 )\n",
            "Epoch: 121\n",
            "TrainingLoss: 103406.054 \n",
            "TestingLoss: 250724.035 )\n",
            "Epoch: 122\n",
            "TrainingLoss: 103716.479 \n",
            "TestingLoss: 252253.930 )\n",
            "Epoch: 123\n",
            "TrainingLoss: 102480.541 \n",
            "TestingLoss: 252347.001 )\n",
            "Epoch: 124\n",
            "TrainingLoss: 101868.803 \n",
            "TestingLoss: 252719.313 )\n",
            "Epoch: 125\n",
            "TrainingLoss: 100406.202 \n",
            "TestingLoss: 254525.061 )\n",
            "Epoch: 126\n",
            "TrainingLoss: 100181.587 \n",
            "TestingLoss: 255383.307 )\n",
            "Epoch: 127\n",
            "TrainingLoss: 99499.974 \n",
            "TestingLoss: 255230.170 )\n",
            "Epoch: 128\n",
            "TrainingLoss: 98202.702 \n",
            "TestingLoss: 253552.469 )\n",
            "Epoch: 129\n",
            "TrainingLoss: 97963.233 \n",
            "TestingLoss: 253429.202 )\n",
            "Epoch: 130\n",
            "TrainingLoss: 96858.574 \n",
            "TestingLoss: 257713.321 )\n",
            "Epoch: 131\n",
            "TrainingLoss: 97206.183 \n",
            "TestingLoss: 255255.696 )\n",
            "Epoch: 132\n",
            "TrainingLoss: 95543.284 \n",
            "TestingLoss: 250203.911 )\n",
            "Epoch: 133\n",
            "TrainingLoss: 95376.653 \n",
            "TestingLoss: 257629.774 )\n",
            "Epoch: 134\n",
            "TrainingLoss: 94452.530 \n",
            "TestingLoss: 253215.069 )\n",
            "Epoch: 135\n",
            "TrainingLoss: 93689.090 \n",
            "TestingLoss: 249725.188 )\n",
            "Epoch: 136\n",
            "TrainingLoss: 93380.512 \n",
            "TestingLoss: 256456.857 )\n",
            "Epoch: 137\n",
            "TrainingLoss: 92143.258 \n",
            "TestingLoss: 256879.650 )\n",
            "Epoch: 138\n",
            "TrainingLoss: 92495.818 \n",
            "TestingLoss: 255826.931 )\n",
            "Epoch: 139\n",
            "TrainingLoss: 91692.392 \n",
            "TestingLoss: 248420.117 )\n",
            "Epoch: 140\n",
            "TrainingLoss: 91369.486 \n",
            "TestingLoss: 252185.194 )\n",
            "Epoch: 141\n",
            "TrainingLoss: 90673.752 \n",
            "TestingLoss: 251273.970 )\n",
            "Epoch: 142\n",
            "TrainingLoss: 89801.714 \n",
            "TestingLoss: 250391.150 )\n",
            "Epoch: 143\n",
            "TrainingLoss: 89408.143 \n",
            "TestingLoss: 249464.574 )\n",
            "Epoch: 144\n",
            "TrainingLoss: 88044.959 \n",
            "TestingLoss: 249989.429 )\n",
            "Epoch: 145\n",
            "TrainingLoss: 88204.735 \n",
            "TestingLoss: 249040.458 )\n",
            "Epoch: 146\n",
            "TrainingLoss: 88117.219 \n",
            "TestingLoss: 253302.668 )\n",
            "Epoch: 147\n",
            "TrainingLoss: 86622.392 \n",
            "TestingLoss: 255415.437 )\n",
            "Epoch: 148\n",
            "TrainingLoss: 86991.864 \n",
            "TestingLoss: 254666.784 )\n",
            "Epoch: 149\n",
            "TrainingLoss: 85739.075 \n",
            "TestingLoss: 252694.532 )\n",
            "Epoch: 150\n",
            "TrainingLoss: 85581.081 \n",
            "TestingLoss: 256136.672 )\n",
            "Epoch: 151\n",
            "TrainingLoss: 85362.288 \n",
            "TestingLoss: 250250.533 )\n",
            "Epoch: 152\n",
            "TrainingLoss: 84240.398 \n",
            "TestingLoss: 257701.830 )\n",
            "Epoch: 153\n",
            "TrainingLoss: 84136.750 \n",
            "TestingLoss: 254107.935 )\n",
            "Epoch: 154\n",
            "TrainingLoss: 84152.090 \n",
            "TestingLoss: 253719.137 )\n",
            "Epoch: 155\n",
            "TrainingLoss: 84522.972 \n",
            "TestingLoss: 249303.963 )\n",
            "Epoch: 156\n",
            "TrainingLoss: 82442.136 \n",
            "TestingLoss: 258401.011 )\n",
            "Epoch: 157\n",
            "TrainingLoss: 82874.428 \n",
            "TestingLoss: 253526.919 )\n",
            "Epoch: 158\n",
            "TrainingLoss: 81878.209 \n",
            "TestingLoss: 253274.620 )\n",
            "Epoch: 159\n",
            "TrainingLoss: 82272.003 \n",
            "TestingLoss: 255436.372 )\n",
            "Epoch: 160\n",
            "TrainingLoss: 81413.318 \n",
            "TestingLoss: 250102.549 )\n",
            "Epoch: 161\n",
            "TrainingLoss: 80163.457 \n",
            "TestingLoss: 253402.411 )\n",
            "Epoch: 162\n",
            "TrainingLoss: 81079.530 \n",
            "TestingLoss: 250616.538 )\n",
            "Epoch: 163\n",
            "TrainingLoss: 80188.761 \n",
            "TestingLoss: 252600.315 )\n",
            "Epoch: 164\n",
            "TrainingLoss: 79816.421 \n",
            "TestingLoss: 253995.289 )\n",
            "Epoch: 165\n",
            "TrainingLoss: 79326.704 \n",
            "TestingLoss: 254788.893 )\n",
            "Epoch: 166\n",
            "TrainingLoss: 78690.077 \n",
            "TestingLoss: 255736.636 )\n",
            "Epoch: 167\n",
            "TrainingLoss: 78739.339 \n",
            "TestingLoss: 256926.870 )\n",
            "Epoch: 168\n",
            "TrainingLoss: 79029.240 \n",
            "TestingLoss: 250758.942 )\n",
            "Epoch: 169\n",
            "TrainingLoss: 78205.307 \n",
            "TestingLoss: 252686.023 )\n",
            "Epoch: 170\n",
            "TrainingLoss: 77981.847 \n",
            "TestingLoss: 251940.437 )\n",
            "Epoch: 171\n",
            "TrainingLoss: 77408.630 \n",
            "TestingLoss: 251118.592 )\n",
            "Epoch: 172\n",
            "TrainingLoss: 76569.398 \n",
            "TestingLoss: 249991.154 )\n",
            "Epoch: 173\n",
            "TrainingLoss: 77124.159 \n",
            "TestingLoss: 256003.239 )\n",
            "Epoch: 174\n",
            "TrainingLoss: 76677.221 \n",
            "TestingLoss: 252456.448 )\n",
            "Epoch: 175\n",
            "TrainingLoss: 77071.134 \n",
            "TestingLoss: 255004.569 )\n",
            "Epoch: 176\n",
            "TrainingLoss: 75432.893 \n",
            "TestingLoss: 256446.881 )\n",
            "Epoch: 177\n",
            "TrainingLoss: 76140.055 \n",
            "TestingLoss: 252006.158 )\n",
            "Epoch: 178\n",
            "TrainingLoss: 75287.653 \n",
            "TestingLoss: 255237.271 )\n",
            "Epoch: 179\n",
            "TrainingLoss: 74913.719 \n",
            "TestingLoss: 254220.727 )\n",
            "Epoch: 180\n",
            "TrainingLoss: 74991.664 \n",
            "TestingLoss: 255009.696 )\n",
            "Epoch: 181\n",
            "TrainingLoss: 74138.139 \n",
            "TestingLoss: 248013.850 )\n",
            "Epoch: 182\n",
            "TrainingLoss: 74477.655 \n",
            "TestingLoss: 254084.294 )\n",
            "Epoch: 183\n",
            "TrainingLoss: 73948.199 \n",
            "TestingLoss: 251487.142 )\n",
            "Epoch: 184\n",
            "TrainingLoss: 72836.016 \n",
            "TestingLoss: 249263.077 )\n",
            "Epoch: 185\n",
            "TrainingLoss: 73005.755 \n",
            "TestingLoss: 252889.239 )\n",
            "Epoch: 186\n",
            "TrainingLoss: 72437.284 \n",
            "TestingLoss: 249877.744 )\n",
            "Epoch: 187\n",
            "TrainingLoss: 73378.489 \n",
            "TestingLoss: 252237.612 )\n",
            "Epoch: 188\n",
            "TrainingLoss: 71525.797 \n",
            "TestingLoss: 251816.299 )\n",
            "Epoch: 189\n",
            "TrainingLoss: 72752.292 \n",
            "TestingLoss: 248552.389 )\n",
            "Epoch: 190\n",
            "TrainingLoss: 72129.008 \n",
            "TestingLoss: 251307.138 )\n",
            "Epoch: 191\n",
            "TrainingLoss: 72253.763 \n",
            "TestingLoss: 247112.502 )\n",
            "Epoch: 192\n",
            "TrainingLoss: 71469.572 \n",
            "TestingLoss: 251078.594 )\n",
            "Epoch: 193\n",
            "TrainingLoss: 71688.599 \n",
            "TestingLoss: 248218.060 )\n",
            "Epoch: 194\n",
            "TrainingLoss: 71479.560 \n",
            "TestingLoss: 251786.052 )\n",
            "Epoch: 195\n",
            "TrainingLoss: 72024.116 \n",
            "TestingLoss: 246570.175 )\n",
            "Epoch: 196\n",
            "TrainingLoss: 70235.895 \n",
            "TestingLoss: 255063.226 )\n",
            "Epoch: 197\n",
            "TrainingLoss: 70569.774 \n",
            "TestingLoss: 250532.308 )\n",
            "Epoch: 198\n",
            "TrainingLoss: 69901.427 \n",
            "TestingLoss: 252925.419 )\n",
            "Epoch: 199\n",
            "TrainingLoss: 69923.532 \n",
            "TestingLoss: 259646.720 )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DBP_nn_module = module(NN(input_size=L), DBP_train_loader, DBP_test_loader, EPOCH=200, LR=LR)\n",
        "DBP_nn_module.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkDxTAVJEAgS",
        "outputId": "5cd413d9-ab9a-4979-9201-ab8de45f3210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "TrainingLoss: 188380.005 \n",
            "TestingLoss: 159561.574 )\n",
            "Epoch: 1\n",
            "TrainingLoss: 115167.022 \n",
            "TestingLoss: 149896.822 )\n",
            "Epoch: 2\n",
            "TrainingLoss: 107696.319 \n",
            "TestingLoss: 140490.206 )\n",
            "Epoch: 3\n",
            "TrainingLoss: 99666.070 \n",
            "TestingLoss: 132844.098 )\n",
            "Epoch: 4\n",
            "TrainingLoss: 93826.629 \n",
            "TestingLoss: 123279.358 )\n",
            "Epoch: 5\n",
            "TrainingLoss: 89099.582 \n",
            "TestingLoss: 118052.156 )\n",
            "Epoch: 6\n",
            "TrainingLoss: 85060.312 \n",
            "TestingLoss: 113453.068 )\n",
            "Epoch: 7\n",
            "TrainingLoss: 81924.009 \n",
            "TestingLoss: 107472.836 )\n",
            "Epoch: 8\n",
            "TrainingLoss: 79711.689 \n",
            "TestingLoss: 104574.651 )\n",
            "Epoch: 9\n",
            "TrainingLoss: 77220.317 \n",
            "TestingLoss: 103656.164 )\n",
            "Epoch: 10\n",
            "TrainingLoss: 75410.693 \n",
            "TestingLoss: 99416.833 )\n",
            "Epoch: 11\n",
            "TrainingLoss: 73318.238 \n",
            "TestingLoss: 101043.997 )\n",
            "Epoch: 12\n",
            "TrainingLoss: 71926.395 \n",
            "TestingLoss: 95210.949 )\n",
            "Epoch: 13\n",
            "TrainingLoss: 70669.023 \n",
            "TestingLoss: 92980.933 )\n",
            "Epoch: 14\n",
            "TrainingLoss: 69065.212 \n",
            "TestingLoss: 90732.727 )\n",
            "Epoch: 15\n",
            "TrainingLoss: 68010.383 \n",
            "TestingLoss: 95860.271 )\n",
            "Epoch: 16\n",
            "TrainingLoss: 66591.879 \n",
            "TestingLoss: 90346.931 )\n",
            "Epoch: 17\n",
            "TrainingLoss: 65817.915 \n",
            "TestingLoss: 88874.211 )\n",
            "Epoch: 18\n",
            "TrainingLoss: 64797.748 \n",
            "TestingLoss: 88129.363 )\n",
            "Epoch: 19\n",
            "TrainingLoss: 63836.192 \n",
            "TestingLoss: 85742.807 )\n",
            "Epoch: 20\n",
            "TrainingLoss: 63035.955 \n",
            "TestingLoss: 84148.438 )\n",
            "Epoch: 21\n",
            "TrainingLoss: 62199.731 \n",
            "TestingLoss: 85033.947 )\n",
            "Epoch: 22\n",
            "TrainingLoss: 61709.918 \n",
            "TestingLoss: 84489.855 )\n",
            "Epoch: 23\n",
            "TrainingLoss: 60803.977 \n",
            "TestingLoss: 84029.469 )\n",
            "Epoch: 24\n",
            "TrainingLoss: 60032.350 \n",
            "TestingLoss: 83580.049 )\n",
            "Epoch: 25\n",
            "TrainingLoss: 59607.349 \n",
            "TestingLoss: 83113.449 )\n",
            "Epoch: 26\n",
            "TrainingLoss: 58965.988 \n",
            "TestingLoss: 80862.498 )\n",
            "Epoch: 27\n",
            "TrainingLoss: 57928.332 \n",
            "TestingLoss: 81163.580 )\n",
            "Epoch: 28\n",
            "TrainingLoss: 57898.976 \n",
            "TestingLoss: 82122.581 )\n",
            "Epoch: 29\n",
            "TrainingLoss: 57281.199 \n",
            "TestingLoss: 81583.177 )\n",
            "Epoch: 30\n",
            "TrainingLoss: 56651.400 \n",
            "TestingLoss: 81343.399 )\n",
            "Epoch: 31\n",
            "TrainingLoss: 55929.891 \n",
            "TestingLoss: 81610.610 )\n",
            "Epoch: 32\n",
            "TrainingLoss: 55744.975 \n",
            "TestingLoss: 81821.805 )\n",
            "Epoch: 33\n",
            "TrainingLoss: 55137.056 \n",
            "TestingLoss: 80743.418 )\n",
            "Epoch: 34\n",
            "TrainingLoss: 54886.640 \n",
            "TestingLoss: 78488.992 )\n",
            "Epoch: 35\n",
            "TrainingLoss: 54221.565 \n",
            "TestingLoss: 79868.209 )\n",
            "Epoch: 36\n",
            "TrainingLoss: 53609.267 \n",
            "TestingLoss: 76913.673 )\n",
            "Epoch: 37\n",
            "TrainingLoss: 53502.346 \n",
            "TestingLoss: 77132.663 )\n",
            "Epoch: 38\n",
            "TrainingLoss: 52809.537 \n",
            "TestingLoss: 76483.045 )\n",
            "Epoch: 39\n",
            "TrainingLoss: 52233.283 \n",
            "TestingLoss: 76746.280 )\n",
            "Epoch: 40\n",
            "TrainingLoss: 52187.912 \n",
            "TestingLoss: 76230.801 )\n",
            "Epoch: 41\n",
            "TrainingLoss: 51624.853 \n",
            "TestingLoss: 78457.896 )\n",
            "Epoch: 42\n",
            "TrainingLoss: 51141.104 \n",
            "TestingLoss: 77453.233 )\n",
            "Epoch: 43\n",
            "TrainingLoss: 50859.175 \n",
            "TestingLoss: 74816.302 )\n",
            "Epoch: 44\n",
            "TrainingLoss: 50417.320 \n",
            "TestingLoss: 76129.073 )\n",
            "Epoch: 45\n",
            "TrainingLoss: 50337.122 \n",
            "TestingLoss: 81624.371 )\n",
            "Epoch: 46\n",
            "TrainingLoss: 49599.818 \n",
            "TestingLoss: 74563.789 )\n",
            "Epoch: 47\n",
            "TrainingLoss: 49380.605 \n",
            "TestingLoss: 74450.236 )\n",
            "Epoch: 48\n",
            "TrainingLoss: 48599.414 \n",
            "TestingLoss: 76173.357 )\n",
            "Epoch: 49\n",
            "TrainingLoss: 48788.976 \n",
            "TestingLoss: 75102.358 )\n",
            "Epoch: 50\n",
            "TrainingLoss: 48233.415 \n",
            "TestingLoss: 75291.826 )\n",
            "Epoch: 51\n",
            "TrainingLoss: 47859.030 \n",
            "TestingLoss: 73558.507 )\n",
            "Epoch: 52\n",
            "TrainingLoss: 47315.340 \n",
            "TestingLoss: 76497.709 )\n",
            "Epoch: 53\n",
            "TrainingLoss: 46777.495 \n",
            "TestingLoss: 78424.527 )\n",
            "Epoch: 54\n",
            "TrainingLoss: 46807.654 \n",
            "TestingLoss: 73757.285 )\n",
            "Epoch: 55\n",
            "TrainingLoss: 46137.120 \n",
            "TestingLoss: 74426.297 )\n",
            "Epoch: 56\n",
            "TrainingLoss: 45995.068 \n",
            "TestingLoss: 75358.110 )\n",
            "Epoch: 57\n",
            "TrainingLoss: 45609.397 \n",
            "TestingLoss: 77120.272 )\n",
            "Epoch: 58\n",
            "TrainingLoss: 45720.235 \n",
            "TestingLoss: 75694.043 )\n",
            "Epoch: 59\n",
            "TrainingLoss: 44470.983 \n",
            "TestingLoss: 74560.164 )\n",
            "Epoch: 60\n",
            "TrainingLoss: 44100.913 \n",
            "TestingLoss: 74904.038 )\n",
            "Epoch: 61\n",
            "TrainingLoss: 43836.506 \n",
            "TestingLoss: 74735.057 )\n",
            "Epoch: 62\n",
            "TrainingLoss: 43172.093 \n",
            "TestingLoss: 73347.739 )\n",
            "Epoch: 63\n",
            "TrainingLoss: 42711.679 \n",
            "TestingLoss: 72001.702 )\n",
            "Epoch: 64\n",
            "TrainingLoss: 42097.608 \n",
            "TestingLoss: 74447.244 )\n",
            "Epoch: 65\n",
            "TrainingLoss: 41856.105 \n",
            "TestingLoss: 69915.119 )\n",
            "Epoch: 66\n",
            "TrainingLoss: 41387.323 \n",
            "TestingLoss: 74194.388 )\n",
            "Epoch: 67\n",
            "TrainingLoss: 40856.684 \n",
            "TestingLoss: 69933.877 )\n",
            "Epoch: 68\n",
            "TrainingLoss: 40055.254 \n",
            "TestingLoss: 72361.044 )\n",
            "Epoch: 69\n",
            "TrainingLoss: 40082.126 \n",
            "TestingLoss: 73270.854 )\n",
            "Epoch: 70\n",
            "TrainingLoss: 39573.640 \n",
            "TestingLoss: 70494.696 )\n",
            "Epoch: 71\n",
            "TrainingLoss: 39109.801 \n",
            "TestingLoss: 74063.993 )\n",
            "Epoch: 72\n",
            "TrainingLoss: 38220.464 \n",
            "TestingLoss: 68879.613 )\n",
            "Epoch: 73\n",
            "TrainingLoss: 38041.005 \n",
            "TestingLoss: 70754.051 )\n",
            "Epoch: 74\n",
            "TrainingLoss: 37452.064 \n",
            "TestingLoss: 69639.257 )\n",
            "Epoch: 75\n",
            "TrainingLoss: 37335.362 \n",
            "TestingLoss: 71437.633 )\n",
            "Epoch: 76\n",
            "TrainingLoss: 36979.440 \n",
            "TestingLoss: 72171.043 )\n",
            "Epoch: 77\n",
            "TrainingLoss: 36746.729 \n",
            "TestingLoss: 69632.548 )\n",
            "Epoch: 78\n",
            "TrainingLoss: 35703.857 \n",
            "TestingLoss: 69079.646 )\n",
            "Epoch: 79\n",
            "TrainingLoss: 36012.904 \n",
            "TestingLoss: 67424.136 )\n",
            "Epoch: 80\n",
            "TrainingLoss: 35382.382 \n",
            "TestingLoss: 65652.523 )\n",
            "Epoch: 81\n",
            "TrainingLoss: 35274.376 \n",
            "TestingLoss: 69000.192 )\n",
            "Epoch: 82\n",
            "TrainingLoss: 34949.597 \n",
            "TestingLoss: 69331.112 )\n",
            "Epoch: 83\n",
            "TrainingLoss: 34155.085 \n",
            "TestingLoss: 65563.139 )\n",
            "Epoch: 84\n",
            "TrainingLoss: 33944.857 \n",
            "TestingLoss: 67983.400 )\n",
            "Epoch: 85\n",
            "TrainingLoss: 33700.613 \n",
            "TestingLoss: 68783.215 )\n",
            "Epoch: 86\n",
            "TrainingLoss: 33031.806 \n",
            "TestingLoss: 66423.179 )\n",
            "Epoch: 87\n",
            "TrainingLoss: 32986.631 \n",
            "TestingLoss: 66650.887 )\n",
            "Epoch: 88\n",
            "TrainingLoss: 32720.793 \n",
            "TestingLoss: 69420.938 )\n",
            "Epoch: 89\n",
            "TrainingLoss: 32329.182 \n",
            "TestingLoss: 67825.342 )\n",
            "Epoch: 90\n",
            "TrainingLoss: 32107.914 \n",
            "TestingLoss: 66738.386 )\n",
            "Epoch: 91\n",
            "TrainingLoss: 31750.081 \n",
            "TestingLoss: 65367.002 )\n",
            "Epoch: 92\n",
            "TrainingLoss: 31375.393 \n",
            "TestingLoss: 67552.104 )\n",
            "Epoch: 93\n",
            "TrainingLoss: 30917.459 \n",
            "TestingLoss: 66857.853 )\n",
            "Epoch: 94\n",
            "TrainingLoss: 31169.485 \n",
            "TestingLoss: 65232.311 )\n",
            "Epoch: 95\n",
            "TrainingLoss: 30609.910 \n",
            "TestingLoss: 68479.922 )\n",
            "Epoch: 96\n",
            "TrainingLoss: 30305.200 \n",
            "TestingLoss: 69093.645 )\n",
            "Epoch: 97\n",
            "TrainingLoss: 30279.174 \n",
            "TestingLoss: 65268.775 )\n",
            "Epoch: 98\n",
            "TrainingLoss: 29584.517 \n",
            "TestingLoss: 68545.828 )\n",
            "Epoch: 99\n",
            "TrainingLoss: 29700.941 \n",
            "TestingLoss: 69766.400 )\n",
            "Epoch: 100\n",
            "TrainingLoss: 28968.338 \n",
            "TestingLoss: 63763.345 )\n",
            "Epoch: 101\n",
            "TrainingLoss: 29002.318 \n",
            "TestingLoss: 68707.065 )\n",
            "Epoch: 102\n",
            "TrainingLoss: 28704.948 \n",
            "TestingLoss: 64889.333 )\n",
            "Epoch: 103\n",
            "TrainingLoss: 29346.067 \n",
            "TestingLoss: 71491.193 )\n",
            "Epoch: 104\n",
            "TrainingLoss: 28491.327 \n",
            "TestingLoss: 69769.317 )\n",
            "Epoch: 105\n",
            "TrainingLoss: 28264.181 \n",
            "TestingLoss: 66259.996 )\n",
            "Epoch: 106\n",
            "TrainingLoss: 27845.648 \n",
            "TestingLoss: 66450.923 )\n",
            "Epoch: 107\n",
            "TrainingLoss: 27791.883 \n",
            "TestingLoss: 65166.930 )\n",
            "Epoch: 108\n",
            "TrainingLoss: 27391.597 \n",
            "TestingLoss: 74845.580 )\n",
            "Epoch: 109\n",
            "TrainingLoss: 27053.928 \n",
            "TestingLoss: 65033.857 )\n",
            "Epoch: 110\n",
            "TrainingLoss: 27204.013 \n",
            "TestingLoss: 66363.943 )\n",
            "Epoch: 111\n",
            "TrainingLoss: 26715.902 \n",
            "TestingLoss: 66770.446 )\n",
            "Epoch: 112\n",
            "TrainingLoss: 26562.981 \n",
            "TestingLoss: 66681.988 )\n",
            "Epoch: 113\n",
            "TrainingLoss: 26495.675 \n",
            "TestingLoss: 66192.462 )\n",
            "Epoch: 114\n",
            "TrainingLoss: 26167.107 \n",
            "TestingLoss: 65362.912 )\n",
            "Epoch: 115\n",
            "TrainingLoss: 26279.137 \n",
            "TestingLoss: 67147.260 )\n",
            "Epoch: 116\n",
            "TrainingLoss: 25950.769 \n",
            "TestingLoss: 67617.746 )\n",
            "Epoch: 117\n",
            "TrainingLoss: 25631.115 \n",
            "TestingLoss: 67958.791 )\n",
            "Epoch: 118\n",
            "TrainingLoss: 25816.320 \n",
            "TestingLoss: 68275.555 )\n",
            "Epoch: 119\n",
            "TrainingLoss: 25335.691 \n",
            "TestingLoss: 65969.029 )\n",
            "Epoch: 120\n",
            "TrainingLoss: 25103.868 \n",
            "TestingLoss: 67975.858 )\n",
            "Epoch: 121\n",
            "TrainingLoss: 25072.574 \n",
            "TestingLoss: 67573.685 )\n",
            "Epoch: 122\n",
            "TrainingLoss: 25253.754 \n",
            "TestingLoss: 67546.703 )\n",
            "Epoch: 123\n",
            "TrainingLoss: 24619.808 \n",
            "TestingLoss: 66865.332 )\n",
            "Epoch: 124\n",
            "TrainingLoss: 24699.601 \n",
            "TestingLoss: 67094.360 )\n",
            "Epoch: 125\n",
            "TrainingLoss: 24612.001 \n",
            "TestingLoss: 67887.527 )\n",
            "Epoch: 126\n",
            "TrainingLoss: 24394.022 \n",
            "TestingLoss: 67032.688 )\n",
            "Epoch: 127\n",
            "TrainingLoss: 24578.607 \n",
            "TestingLoss: 65409.992 )\n",
            "Epoch: 128\n",
            "TrainingLoss: 24174.281 \n",
            "TestingLoss: 65640.377 )\n",
            "Epoch: 129\n",
            "TrainingLoss: 24055.782 \n",
            "TestingLoss: 65888.234 )\n",
            "Epoch: 130\n",
            "TrainingLoss: 23908.412 \n",
            "TestingLoss: 66972.682 )\n",
            "Epoch: 131\n",
            "TrainingLoss: 23736.334 \n",
            "TestingLoss: 68180.300 )\n",
            "Epoch: 132\n",
            "TrainingLoss: 23939.126 \n",
            "TestingLoss: 66985.996 )\n",
            "Epoch: 133\n",
            "TrainingLoss: 23494.954 \n",
            "TestingLoss: 65694.501 )\n",
            "Epoch: 134\n",
            "TrainingLoss: 23472.582 \n",
            "TestingLoss: 64826.938 )\n",
            "Epoch: 135\n",
            "TrainingLoss: 23158.434 \n",
            "TestingLoss: 64896.057 )\n",
            "Epoch: 136\n",
            "TrainingLoss: 23063.601 \n",
            "TestingLoss: 65262.043 )\n",
            "Epoch: 137\n",
            "TrainingLoss: 22848.291 \n",
            "TestingLoss: 69122.973 )\n",
            "Epoch: 138\n",
            "TrainingLoss: 22891.362 \n",
            "TestingLoss: 65825.983 )\n",
            "Epoch: 139\n",
            "TrainingLoss: 23237.145 \n",
            "TestingLoss: 68064.212 )\n",
            "Epoch: 140\n",
            "TrainingLoss: 23064.649 \n",
            "TestingLoss: 69105.809 )\n",
            "Epoch: 141\n",
            "TrainingLoss: 22692.456 \n",
            "TestingLoss: 65691.224 )\n",
            "Epoch: 142\n",
            "TrainingLoss: 23580.290 \n",
            "TestingLoss: 64402.454 )\n",
            "Epoch: 143\n",
            "TrainingLoss: 23037.341 \n",
            "TestingLoss: 66194.893 )\n",
            "Epoch: 144\n",
            "TrainingLoss: 22854.277 \n",
            "TestingLoss: 67794.308 )\n",
            "Epoch: 145\n",
            "TrainingLoss: 22368.478 \n",
            "TestingLoss: 65767.060 )\n",
            "Epoch: 146\n",
            "TrainingLoss: 22259.067 \n",
            "TestingLoss: 65107.529 )\n",
            "Epoch: 147\n",
            "TrainingLoss: 22159.715 \n",
            "TestingLoss: 67539.460 )\n",
            "Epoch: 148\n",
            "TrainingLoss: 22063.373 \n",
            "TestingLoss: 67345.817 )\n",
            "Epoch: 149\n",
            "TrainingLoss: 21956.778 \n",
            "TestingLoss: 64943.273 )\n",
            "Epoch: 150\n",
            "TrainingLoss: 21531.184 \n",
            "TestingLoss: 67667.023 )\n",
            "Epoch: 151\n",
            "TrainingLoss: 21851.449 \n",
            "TestingLoss: 68316.360 )\n",
            "Epoch: 152\n",
            "TrainingLoss: 21845.640 \n",
            "TestingLoss: 65603.597 )\n",
            "Epoch: 153\n",
            "TrainingLoss: 21379.751 \n",
            "TestingLoss: 67415.022 )\n",
            "Epoch: 154\n",
            "TrainingLoss: 21719.628 \n",
            "TestingLoss: 69004.924 )\n",
            "Epoch: 155\n",
            "TrainingLoss: 21294.097 \n",
            "TestingLoss: 65360.520 )\n",
            "Epoch: 156\n",
            "TrainingLoss: 21240.508 \n",
            "TestingLoss: 64764.470 )\n",
            "Epoch: 157\n",
            "TrainingLoss: 21156.264 \n",
            "TestingLoss: 64723.258 )\n",
            "Epoch: 158\n",
            "TrainingLoss: 21375.514 \n",
            "TestingLoss: 65313.316 )\n",
            "Epoch: 159\n",
            "TrainingLoss: 21047.247 \n",
            "TestingLoss: 68192.363 )\n",
            "Epoch: 160\n",
            "TrainingLoss: 21021.161 \n",
            "TestingLoss: 64630.843 )\n",
            "Epoch: 161\n",
            "TrainingLoss: 21003.319 \n",
            "TestingLoss: 64462.843 )\n",
            "Epoch: 162\n",
            "TrainingLoss: 20751.201 \n",
            "TestingLoss: 66062.959 )\n",
            "Epoch: 163\n",
            "TrainingLoss: 20751.085 \n",
            "TestingLoss: 65794.784 )\n",
            "Epoch: 164\n",
            "TrainingLoss: 20759.253 \n",
            "TestingLoss: 67328.347 )\n",
            "Epoch: 165\n",
            "TrainingLoss: 20338.142 \n",
            "TestingLoss: 66238.294 )\n",
            "Epoch: 166\n",
            "TrainingLoss: 20588.940 \n",
            "TestingLoss: 64321.759 )\n",
            "Epoch: 167\n",
            "TrainingLoss: 20325.318 \n",
            "TestingLoss: 68452.677 )\n",
            "Epoch: 168\n",
            "TrainingLoss: 20059.617 \n",
            "TestingLoss: 66187.186 )\n",
            "Epoch: 169\n",
            "TrainingLoss: 20023.751 \n",
            "TestingLoss: 67342.238 )\n",
            "Epoch: 170\n",
            "TrainingLoss: 20136.392 \n",
            "TestingLoss: 66980.770 )\n",
            "Epoch: 171\n",
            "TrainingLoss: 20461.184 \n",
            "TestingLoss: 65854.473 )\n",
            "Epoch: 172\n",
            "TrainingLoss: 19693.638 \n",
            "TestingLoss: 65438.928 )\n",
            "Epoch: 173\n",
            "TrainingLoss: 20318.781 \n",
            "TestingLoss: 69885.888 )\n",
            "Epoch: 174\n",
            "TrainingLoss: 19633.403 \n",
            "TestingLoss: 68101.033 )\n",
            "Epoch: 175\n",
            "TrainingLoss: 19927.498 \n",
            "TestingLoss: 63443.303 )\n",
            "Epoch: 176\n",
            "TrainingLoss: 20085.889 \n",
            "TestingLoss: 65960.063 )\n",
            "Epoch: 177\n",
            "TrainingLoss: 19670.941 \n",
            "TestingLoss: 69299.783 )\n",
            "Epoch: 178\n",
            "TrainingLoss: 19812.308 \n",
            "TestingLoss: 66133.061 )\n",
            "Epoch: 179\n",
            "TrainingLoss: 19392.307 \n",
            "TestingLoss: 66176.092 )\n",
            "Epoch: 180\n",
            "TrainingLoss: 19526.659 \n",
            "TestingLoss: 63816.213 )\n",
            "Epoch: 181\n",
            "TrainingLoss: 19341.999 \n",
            "TestingLoss: 63705.422 )\n",
            "Epoch: 182\n",
            "TrainingLoss: 19082.885 \n",
            "TestingLoss: 65918.032 )\n",
            "Epoch: 183\n",
            "TrainingLoss: 19392.307 \n",
            "TestingLoss: 66201.870 )\n",
            "Epoch: 184\n",
            "TrainingLoss: 19814.204 \n",
            "TestingLoss: 63803.121 )\n",
            "Epoch: 185\n",
            "TrainingLoss: 19287.664 \n",
            "TestingLoss: 69213.427 )\n",
            "Epoch: 186\n",
            "TrainingLoss: 19228.902 \n",
            "TestingLoss: 62373.153 )\n",
            "Epoch: 187\n",
            "TrainingLoss: 19293.744 \n",
            "TestingLoss: 66402.117 )\n",
            "Epoch: 188\n",
            "TrainingLoss: 19368.498 \n",
            "TestingLoss: 62310.309 )\n",
            "Epoch: 189\n",
            "TrainingLoss: 19113.908 \n",
            "TestingLoss: 64414.140 )\n",
            "Epoch: 190\n",
            "TrainingLoss: 18931.716 \n",
            "TestingLoss: 64064.602 )\n",
            "Epoch: 191\n",
            "TrainingLoss: 18942.528 \n",
            "TestingLoss: 65476.748 )\n",
            "Epoch: 192\n",
            "TrainingLoss: 18862.237 \n",
            "TestingLoss: 62181.818 )\n",
            "Epoch: 193\n",
            "TrainingLoss: 18504.816 \n",
            "TestingLoss: 65423.036 )\n",
            "Epoch: 194\n",
            "TrainingLoss: 18906.818 \n",
            "TestingLoss: 64416.218 )\n",
            "Epoch: 195\n",
            "TrainingLoss: 18435.691 \n",
            "TestingLoss: 64068.031 )\n",
            "Epoch: 196\n",
            "TrainingLoss: 18706.180 \n",
            "TestingLoss: 67850.680 )\n",
            "Epoch: 197\n",
            "TrainingLoss: 18488.022 \n",
            "TestingLoss: 64748.864 )\n",
            "Epoch: 198\n",
            "TrainingLoss: 18381.013 \n",
            "TestingLoss: 64336.037 )\n",
            "Epoch: 199\n",
            "TrainingLoss: 18588.286 \n",
            "TestingLoss: 67154.694 )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAP_nn_module = module(NN(input_size=L), MAP_train_loader, MAP_test_loader, EPOCH=200, LR=LR)\n",
        "MAP_nn_module.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_SMac8tEArf",
        "outputId": "92eb7165-cbee-485d-fcbc-ddb369200b2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "TrainingLoss: 304027.109 \n",
            "TestingLoss: 194153.287 )\n",
            "Epoch: 1\n",
            "TrainingLoss: 138437.649 \n",
            "TestingLoss: 173186.018 )\n",
            "Epoch: 2\n",
            "TrainingLoss: 126334.416 \n",
            "TestingLoss: 159318.272 )\n",
            "Epoch: 3\n",
            "TrainingLoss: 116551.597 \n",
            "TestingLoss: 150069.784 )\n",
            "Epoch: 4\n",
            "TrainingLoss: 108930.389 \n",
            "TestingLoss: 139600.853 )\n",
            "Epoch: 5\n",
            "TrainingLoss: 103824.116 \n",
            "TestingLoss: 138032.898 )\n",
            "Epoch: 6\n",
            "TrainingLoss: 99468.830 \n",
            "TestingLoss: 129005.259 )\n",
            "Epoch: 7\n",
            "TrainingLoss: 96037.662 \n",
            "TestingLoss: 128439.733 )\n",
            "Epoch: 8\n",
            "TrainingLoss: 93390.059 \n",
            "TestingLoss: 122811.718 )\n",
            "Epoch: 9\n",
            "TrainingLoss: 91127.256 \n",
            "TestingLoss: 123457.971 )\n",
            "Epoch: 10\n",
            "TrainingLoss: 88792.944 \n",
            "TestingLoss: 116544.698 )\n",
            "Epoch: 11\n",
            "TrainingLoss: 87277.216 \n",
            "TestingLoss: 112970.677 )\n",
            "Epoch: 12\n",
            "TrainingLoss: 85490.087 \n",
            "TestingLoss: 113351.670 )\n",
            "Epoch: 13\n",
            "TrainingLoss: 83952.451 \n",
            "TestingLoss: 112348.912 )\n",
            "Epoch: 14\n",
            "TrainingLoss: 82231.552 \n",
            "TestingLoss: 107033.595 )\n",
            "Epoch: 15\n",
            "TrainingLoss: 80917.714 \n",
            "TestingLoss: 106020.267 )\n",
            "Epoch: 16\n",
            "TrainingLoss: 80000.148 \n",
            "TestingLoss: 105990.793 )\n",
            "Epoch: 17\n",
            "TrainingLoss: 79083.007 \n",
            "TestingLoss: 103779.383 )\n",
            "Epoch: 18\n",
            "TrainingLoss: 77929.697 \n",
            "TestingLoss: 103749.867 )\n",
            "Epoch: 19\n",
            "TrainingLoss: 76834.192 \n",
            "TestingLoss: 103658.682 )\n",
            "Epoch: 20\n",
            "TrainingLoss: 76012.091 \n",
            "TestingLoss: 100082.738 )\n",
            "Epoch: 21\n",
            "TrainingLoss: 75142.792 \n",
            "TestingLoss: 100300.855 )\n",
            "Epoch: 22\n",
            "TrainingLoss: 74227.310 \n",
            "TestingLoss: 101501.845 )\n",
            "Epoch: 23\n",
            "TrainingLoss: 73398.032 \n",
            "TestingLoss: 97573.321 )\n",
            "Epoch: 24\n",
            "TrainingLoss: 72678.961 \n",
            "TestingLoss: 97170.426 )\n",
            "Epoch: 25\n",
            "TrainingLoss: 71746.326 \n",
            "TestingLoss: 97051.344 )\n",
            "Epoch: 26\n",
            "TrainingLoss: 71509.980 \n",
            "TestingLoss: 98731.333 )\n",
            "Epoch: 27\n",
            "TrainingLoss: 70838.434 \n",
            "TestingLoss: 96623.556 )\n",
            "Epoch: 28\n",
            "TrainingLoss: 69523.901 \n",
            "TestingLoss: 95381.117 )\n",
            "Epoch: 29\n",
            "TrainingLoss: 68861.206 \n",
            "TestingLoss: 95356.629 )\n",
            "Epoch: 30\n",
            "TrainingLoss: 68508.015 \n",
            "TestingLoss: 93232.940 )\n",
            "Epoch: 31\n",
            "TrainingLoss: 68096.687 \n",
            "TestingLoss: 96126.908 )\n",
            "Epoch: 32\n",
            "TrainingLoss: 67570.560 \n",
            "TestingLoss: 92618.900 )\n",
            "Epoch: 33\n",
            "TrainingLoss: 67009.908 \n",
            "TestingLoss: 91423.345 )\n",
            "Epoch: 34\n",
            "TrainingLoss: 66355.208 \n",
            "TestingLoss: 92489.733 )\n",
            "Epoch: 35\n",
            "TrainingLoss: 65677.084 \n",
            "TestingLoss: 92457.092 )\n",
            "Epoch: 36\n",
            "TrainingLoss: 65404.203 \n",
            "TestingLoss: 91063.698 )\n",
            "Epoch: 37\n",
            "TrainingLoss: 64860.941 \n",
            "TestingLoss: 92610.411 )\n",
            "Epoch: 38\n",
            "TrainingLoss: 64134.317 \n",
            "TestingLoss: 90422.953 )\n",
            "Epoch: 39\n",
            "TrainingLoss: 63438.123 \n",
            "TestingLoss: 91308.609 )\n",
            "Epoch: 40\n",
            "TrainingLoss: 63343.547 \n",
            "TestingLoss: 88645.071 )\n",
            "Epoch: 41\n",
            "TrainingLoss: 62729.017 \n",
            "TestingLoss: 88653.809 )\n",
            "Epoch: 42\n",
            "TrainingLoss: 62494.434 \n",
            "TestingLoss: 90459.392 )\n",
            "Epoch: 43\n",
            "TrainingLoss: 61587.662 \n",
            "TestingLoss: 92142.936 )\n",
            "Epoch: 44\n",
            "TrainingLoss: 61422.038 \n",
            "TestingLoss: 89862.368 )\n",
            "Epoch: 45\n",
            "TrainingLoss: 60672.560 \n",
            "TestingLoss: 91088.405 )\n",
            "Epoch: 46\n",
            "TrainingLoss: 60614.263 \n",
            "TestingLoss: 89245.871 )\n",
            "Epoch: 47\n",
            "TrainingLoss: 59959.920 \n",
            "TestingLoss: 88146.524 )\n",
            "Epoch: 48\n",
            "TrainingLoss: 60209.887 \n",
            "TestingLoss: 89267.378 )\n",
            "Epoch: 49\n",
            "TrainingLoss: 59071.779 \n",
            "TestingLoss: 88274.433 )\n",
            "Epoch: 50\n",
            "TrainingLoss: 58617.454 \n",
            "TestingLoss: 87806.570 )\n",
            "Epoch: 51\n",
            "TrainingLoss: 58015.032 \n",
            "TestingLoss: 86674.848 )\n",
            "Epoch: 52\n",
            "TrainingLoss: 57790.535 \n",
            "TestingLoss: 88496.631 )\n",
            "Epoch: 53\n",
            "TrainingLoss: 56984.204 \n",
            "TestingLoss: 89307.734 )\n",
            "Epoch: 54\n",
            "TrainingLoss: 56973.643 \n",
            "TestingLoss: 86805.437 )\n",
            "Epoch: 55\n",
            "TrainingLoss: 55992.513 \n",
            "TestingLoss: 88906.982 )\n",
            "Epoch: 56\n",
            "TrainingLoss: 55764.369 \n",
            "TestingLoss: 87374.591 )\n",
            "Epoch: 57\n",
            "TrainingLoss: 55193.320 \n",
            "TestingLoss: 92809.657 )\n",
            "Epoch: 58\n",
            "TrainingLoss: 54334.358 \n",
            "TestingLoss: 84559.801 )\n",
            "Epoch: 59\n",
            "TrainingLoss: 54040.150 \n",
            "TestingLoss: 82785.314 )\n",
            "Epoch: 60\n",
            "TrainingLoss: 53539.856 \n",
            "TestingLoss: 83714.373 )\n",
            "Epoch: 61\n",
            "TrainingLoss: 52789.250 \n",
            "TestingLoss: 82791.894 )\n",
            "Epoch: 62\n",
            "TrainingLoss: 52881.641 \n",
            "TestingLoss: 82370.249 )\n",
            "Epoch: 63\n",
            "TrainingLoss: 52000.103 \n",
            "TestingLoss: 81505.383 )\n",
            "Epoch: 64\n",
            "TrainingLoss: 51576.364 \n",
            "TestingLoss: 81929.509 )\n",
            "Epoch: 65\n",
            "TrainingLoss: 51179.737 \n",
            "TestingLoss: 82281.803 )\n",
            "Epoch: 66\n",
            "TrainingLoss: 50472.799 \n",
            "TestingLoss: 84336.456 )\n",
            "Epoch: 67\n",
            "TrainingLoss: 50108.222 \n",
            "TestingLoss: 80955.880 )\n",
            "Epoch: 68\n",
            "TrainingLoss: 49514.941 \n",
            "TestingLoss: 83036.285 )\n",
            "Epoch: 69\n",
            "TrainingLoss: 48974.672 \n",
            "TestingLoss: 81467.585 )\n",
            "Epoch: 70\n",
            "TrainingLoss: 48817.692 \n",
            "TestingLoss: 81569.132 )\n",
            "Epoch: 71\n",
            "TrainingLoss: 47860.173 \n",
            "TestingLoss: 77573.309 )\n",
            "Epoch: 72\n",
            "TrainingLoss: 46906.989 \n",
            "TestingLoss: 78310.997 )\n",
            "Epoch: 73\n",
            "TrainingLoss: 46781.948 \n",
            "TestingLoss: 76015.539 )\n",
            "Epoch: 74\n",
            "TrainingLoss: 45536.192 \n",
            "TestingLoss: 76334.100 )\n",
            "Epoch: 75\n",
            "TrainingLoss: 45185.453 \n",
            "TestingLoss: 76336.058 )\n",
            "Epoch: 76\n",
            "TrainingLoss: 44698.059 \n",
            "TestingLoss: 76426.495 )\n",
            "Epoch: 77\n",
            "TrainingLoss: 44136.692 \n",
            "TestingLoss: 77627.427 )\n",
            "Epoch: 78\n",
            "TrainingLoss: 43451.134 \n",
            "TestingLoss: 75299.472 )\n",
            "Epoch: 79\n",
            "TrainingLoss: 42911.170 \n",
            "TestingLoss: 75879.424 )\n",
            "Epoch: 80\n",
            "TrainingLoss: 42217.563 \n",
            "TestingLoss: 75561.793 )\n",
            "Epoch: 81\n",
            "TrainingLoss: 41619.960 \n",
            "TestingLoss: 75511.323 )\n",
            "Epoch: 82\n",
            "TrainingLoss: 41362.848 \n",
            "TestingLoss: 75415.778 )\n",
            "Epoch: 83\n",
            "TrainingLoss: 40750.569 \n",
            "TestingLoss: 74248.610 )\n",
            "Epoch: 84\n",
            "TrainingLoss: 40041.089 \n",
            "TestingLoss: 76473.121 )\n",
            "Epoch: 85\n",
            "TrainingLoss: 39721.055 \n",
            "TestingLoss: 76212.873 )\n",
            "Epoch: 86\n",
            "TrainingLoss: 38986.324 \n",
            "TestingLoss: 73622.050 )\n",
            "Epoch: 87\n",
            "TrainingLoss: 38848.616 \n",
            "TestingLoss: 72912.846 )\n",
            "Epoch: 88\n",
            "TrainingLoss: 37793.469 \n",
            "TestingLoss: 74179.386 )\n",
            "Epoch: 89\n",
            "TrainingLoss: 37791.635 \n",
            "TestingLoss: 75055.476 )\n",
            "Epoch: 90\n",
            "TrainingLoss: 37537.529 \n",
            "TestingLoss: 73564.067 )\n",
            "Epoch: 91\n",
            "TrainingLoss: 37173.233 \n",
            "TestingLoss: 73259.827 )\n",
            "Epoch: 92\n",
            "TrainingLoss: 36442.508 \n",
            "TestingLoss: 73258.436 )\n",
            "Epoch: 93\n",
            "TrainingLoss: 36042.769 \n",
            "TestingLoss: 75016.060 )\n",
            "Epoch: 94\n",
            "TrainingLoss: 35685.236 \n",
            "TestingLoss: 72374.654 )\n",
            "Epoch: 95\n",
            "TrainingLoss: 35308.543 \n",
            "TestingLoss: 73621.280 )\n",
            "Epoch: 96\n",
            "TrainingLoss: 34917.246 \n",
            "TestingLoss: 70481.964 )\n",
            "Epoch: 97\n",
            "TrainingLoss: 34435.059 \n",
            "TestingLoss: 71649.376 )\n",
            "Epoch: 98\n",
            "TrainingLoss: 34032.499 \n",
            "TestingLoss: 71985.749 )\n",
            "Epoch: 99\n",
            "TrainingLoss: 33923.359 \n",
            "TestingLoss: 72498.443 )\n",
            "Epoch: 100\n",
            "TrainingLoss: 33330.554 \n",
            "TestingLoss: 73965.611 )\n",
            "Epoch: 101\n",
            "TrainingLoss: 32943.607 \n",
            "TestingLoss: 73769.165 )\n",
            "Epoch: 102\n",
            "TrainingLoss: 32601.164 \n",
            "TestingLoss: 70725.593 )\n",
            "Epoch: 103\n",
            "TrainingLoss: 32452.657 \n",
            "TestingLoss: 71923.812 )\n",
            "Epoch: 104\n",
            "TrainingLoss: 32537.218 \n",
            "TestingLoss: 68878.195 )\n",
            "Epoch: 105\n",
            "TrainingLoss: 31821.155 \n",
            "TestingLoss: 69324.232 )\n",
            "Epoch: 106\n",
            "TrainingLoss: 31566.091 \n",
            "TestingLoss: 70499.101 )\n",
            "Epoch: 107\n",
            "TrainingLoss: 31040.631 \n",
            "TestingLoss: 71204.912 )\n",
            "Epoch: 108\n",
            "TrainingLoss: 30764.451 \n",
            "TestingLoss: 70086.309 )\n",
            "Epoch: 109\n",
            "TrainingLoss: 30415.144 \n",
            "TestingLoss: 71335.024 )\n",
            "Epoch: 110\n",
            "TrainingLoss: 30186.825 \n",
            "TestingLoss: 70601.737 )\n",
            "Epoch: 111\n",
            "TrainingLoss: 30116.233 \n",
            "TestingLoss: 70224.288 )\n",
            "Epoch: 112\n",
            "TrainingLoss: 29999.049 \n",
            "TestingLoss: 69576.555 )\n",
            "Epoch: 113\n",
            "TrainingLoss: 29522.254 \n",
            "TestingLoss: 69412.399 )\n",
            "Epoch: 114\n",
            "TrainingLoss: 29510.921 \n",
            "TestingLoss: 69618.275 )\n",
            "Epoch: 115\n",
            "TrainingLoss: 29112.118 \n",
            "TestingLoss: 71170.189 )\n",
            "Epoch: 116\n",
            "TrainingLoss: 28490.521 \n",
            "TestingLoss: 71378.289 )\n",
            "Epoch: 117\n",
            "TrainingLoss: 28650.006 \n",
            "TestingLoss: 71319.037 )\n",
            "Epoch: 118\n",
            "TrainingLoss: 28766.748 \n",
            "TestingLoss: 72810.517 )\n",
            "Epoch: 119\n",
            "TrainingLoss: 28253.990 \n",
            "TestingLoss: 72112.464 )\n",
            "Epoch: 120\n",
            "TrainingLoss: 28222.941 \n",
            "TestingLoss: 71578.904 )\n",
            "Epoch: 121\n",
            "TrainingLoss: 27667.343 \n",
            "TestingLoss: 70546.514 )\n",
            "Epoch: 122\n",
            "TrainingLoss: 27584.243 \n",
            "TestingLoss: 71942.731 )\n",
            "Epoch: 123\n",
            "TrainingLoss: 27468.044 \n",
            "TestingLoss: 69454.357 )\n",
            "Epoch: 124\n",
            "TrainingLoss: 27645.793 \n",
            "TestingLoss: 72754.447 )\n",
            "Epoch: 125\n",
            "TrainingLoss: 26890.575 \n",
            "TestingLoss: 70131.440 )\n",
            "Epoch: 126\n",
            "TrainingLoss: 26906.328 \n",
            "TestingLoss: 70413.266 )\n",
            "Epoch: 127\n",
            "TrainingLoss: 26800.120 \n",
            "TestingLoss: 70910.174 )\n",
            "Epoch: 128\n",
            "TrainingLoss: 26719.965 \n",
            "TestingLoss: 71055.677 )\n",
            "Epoch: 129\n",
            "TrainingLoss: 26353.478 \n",
            "TestingLoss: 69204.700 )\n",
            "Epoch: 130\n",
            "TrainingLoss: 26496.031 \n",
            "TestingLoss: 70177.392 )\n",
            "Epoch: 131\n",
            "TrainingLoss: 25809.499 \n",
            "TestingLoss: 71542.399 )\n",
            "Epoch: 132\n",
            "TrainingLoss: 25483.309 \n",
            "TestingLoss: 69044.968 )\n",
            "Epoch: 133\n",
            "TrainingLoss: 25672.098 \n",
            "TestingLoss: 70885.464 )\n",
            "Epoch: 134\n",
            "TrainingLoss: 25591.643 \n",
            "TestingLoss: 68431.737 )\n",
            "Epoch: 135\n",
            "TrainingLoss: 25206.006 \n",
            "TestingLoss: 69613.020 )\n",
            "Epoch: 136\n",
            "TrainingLoss: 25293.596 \n",
            "TestingLoss: 70645.021 )\n",
            "Epoch: 137\n",
            "TrainingLoss: 24993.815 \n",
            "TestingLoss: 70219.403 )\n",
            "Epoch: 138\n",
            "TrainingLoss: 25056.447 \n",
            "TestingLoss: 70956.166 )\n",
            "Epoch: 139\n",
            "TrainingLoss: 24800.638 \n",
            "TestingLoss: 70387.610 )\n",
            "Epoch: 140\n",
            "TrainingLoss: 24585.135 \n",
            "TestingLoss: 69912.573 )\n",
            "Epoch: 141\n",
            "TrainingLoss: 24612.000 \n",
            "TestingLoss: 68817.576 )\n",
            "Epoch: 142\n",
            "TrainingLoss: 24301.918 \n",
            "TestingLoss: 68390.060 )\n",
            "Epoch: 143\n",
            "TrainingLoss: 24249.277 \n",
            "TestingLoss: 68803.620 )\n",
            "Epoch: 144\n",
            "TrainingLoss: 23929.058 \n",
            "TestingLoss: 69488.910 )\n",
            "Epoch: 145\n",
            "TrainingLoss: 24090.804 \n",
            "TestingLoss: 69978.602 )\n",
            "Epoch: 146\n",
            "TrainingLoss: 23988.988 \n",
            "TestingLoss: 70459.607 )\n",
            "Epoch: 147\n",
            "TrainingLoss: 23498.154 \n",
            "TestingLoss: 69126.500 )\n",
            "Epoch: 148\n",
            "TrainingLoss: 23530.365 \n",
            "TestingLoss: 68919.543 )\n",
            "Epoch: 149\n",
            "TrainingLoss: 23372.489 \n",
            "TestingLoss: 69846.486 )\n",
            "Epoch: 150\n",
            "TrainingLoss: 23289.744 \n",
            "TestingLoss: 68944.732 )\n",
            "Epoch: 151\n",
            "TrainingLoss: 23222.803 \n",
            "TestingLoss: 76379.302 )\n",
            "Epoch: 152\n",
            "TrainingLoss: 23002.606 \n",
            "TestingLoss: 69364.062 )\n",
            "Epoch: 153\n",
            "TrainingLoss: 22935.338 \n",
            "TestingLoss: 70019.758 )\n",
            "Epoch: 154\n",
            "TrainingLoss: 22801.619 \n",
            "TestingLoss: 70249.651 )\n",
            "Epoch: 155\n",
            "TrainingLoss: 22737.798 \n",
            "TestingLoss: 68922.681 )\n",
            "Epoch: 156\n",
            "TrainingLoss: 22649.327 \n",
            "TestingLoss: 68705.740 )\n",
            "Epoch: 157\n",
            "TrainingLoss: 22608.752 \n",
            "TestingLoss: 69767.600 )\n",
            "Epoch: 158\n",
            "TrainingLoss: 22462.091 \n",
            "TestingLoss: 68436.594 )\n",
            "Epoch: 159\n",
            "TrainingLoss: 22603.737 \n",
            "TestingLoss: 68760.253 )\n",
            "Epoch: 160\n",
            "TrainingLoss: 21945.575 \n",
            "TestingLoss: 68969.703 )\n",
            "Epoch: 161\n",
            "TrainingLoss: 22348.057 \n",
            "TestingLoss: 69603.715 )\n",
            "Epoch: 162\n",
            "TrainingLoss: 22100.607 \n",
            "TestingLoss: 68697.503 )\n",
            "Epoch: 163\n",
            "TrainingLoss: 21799.180 \n",
            "TestingLoss: 68724.981 )\n",
            "Epoch: 164\n",
            "TrainingLoss: 21983.178 \n",
            "TestingLoss: 68255.117 )\n",
            "Epoch: 165\n",
            "TrainingLoss: 21813.093 \n",
            "TestingLoss: 68745.576 )\n",
            "Epoch: 166\n",
            "TrainingLoss: 22265.102 \n",
            "TestingLoss: 68776.020 )\n",
            "Epoch: 167\n",
            "TrainingLoss: 21518.374 \n",
            "TestingLoss: 70428.854 )\n",
            "Epoch: 168\n",
            "TrainingLoss: 21190.800 \n",
            "TestingLoss: 69247.375 )\n",
            "Epoch: 169\n",
            "TrainingLoss: 21304.529 \n",
            "TestingLoss: 69688.724 )\n",
            "Epoch: 170\n",
            "TrainingLoss: 21304.366 \n",
            "TestingLoss: 68610.760 )\n",
            "Epoch: 171\n",
            "TrainingLoss: 21162.771 \n",
            "TestingLoss: 68160.550 )\n",
            "Epoch: 172\n",
            "TrainingLoss: 21369.296 \n",
            "TestingLoss: 68750.249 )\n",
            "Epoch: 173\n",
            "TrainingLoss: 21074.284 \n",
            "TestingLoss: 69346.397 )\n",
            "Epoch: 174\n",
            "TrainingLoss: 21152.079 \n",
            "TestingLoss: 69838.314 )\n",
            "Epoch: 175\n",
            "TrainingLoss: 20882.305 \n",
            "TestingLoss: 69025.933 )\n",
            "Epoch: 176\n",
            "TrainingLoss: 20764.172 \n",
            "TestingLoss: 68344.122 )\n",
            "Epoch: 177\n",
            "TrainingLoss: 20834.371 \n",
            "TestingLoss: 67961.490 )\n",
            "Epoch: 178\n",
            "TrainingLoss: 20565.625 \n",
            "TestingLoss: 67678.336 )\n",
            "Epoch: 179\n",
            "TrainingLoss: 20629.555 \n",
            "TestingLoss: 69020.567 )\n",
            "Epoch: 180\n",
            "TrainingLoss: 20647.004 \n",
            "TestingLoss: 69545.341 )\n",
            "Epoch: 181\n",
            "TrainingLoss: 20591.254 \n",
            "TestingLoss: 69492.243 )\n",
            "Epoch: 182\n",
            "TrainingLoss: 20162.254 \n",
            "TestingLoss: 70322.139 )\n",
            "Epoch: 183\n",
            "TrainingLoss: 20539.461 \n",
            "TestingLoss: 68860.804 )\n",
            "Epoch: 184\n",
            "TrainingLoss: 20455.525 \n",
            "TestingLoss: 68507.716 )\n",
            "Epoch: 185\n",
            "TrainingLoss: 20149.152 \n",
            "TestingLoss: 68905.059 )\n",
            "Epoch: 186\n",
            "TrainingLoss: 20010.659 \n",
            "TestingLoss: 69713.655 )\n",
            "Epoch: 187\n",
            "TrainingLoss: 19920.225 \n",
            "TestingLoss: 70176.760 )\n",
            "Epoch: 188\n",
            "TrainingLoss: 20004.666 \n",
            "TestingLoss: 68618.651 )\n",
            "Epoch: 189\n",
            "TrainingLoss: 19905.010 \n",
            "TestingLoss: 68033.715 )\n",
            "Epoch: 190\n",
            "TrainingLoss: 19968.105 \n",
            "TestingLoss: 68070.498 )\n",
            "Epoch: 191\n",
            "TrainingLoss: 19735.479 \n",
            "TestingLoss: 67619.739 )\n",
            "Epoch: 192\n",
            "TrainingLoss: 20077.435 \n",
            "TestingLoss: 68689.150 )\n",
            "Epoch: 193\n",
            "TrainingLoss: 19600.066 \n",
            "TestingLoss: 66810.396 )\n",
            "Epoch: 194\n",
            "TrainingLoss: 19733.870 \n",
            "TestingLoss: 68923.989 )\n",
            "Epoch: 195\n",
            "TrainingLoss: 19293.465 \n",
            "TestingLoss: 67570.590 )\n",
            "Epoch: 196\n",
            "TrainingLoss: 19604.204 \n",
            "TestingLoss: 68518.469 )\n",
            "Epoch: 197\n",
            "TrainingLoss: 19511.858 \n",
            "TestingLoss: 74783.872 )\n",
            "Epoch: 198\n",
            "TrainingLoss: 19301.355 \n",
            "TestingLoss: 67867.702 )\n",
            "Epoch: 199\n",
            "TrainingLoss: 19736.799 \n",
            "TestingLoss: 71557.219 )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# temp_predict = []\n",
        "# for predicts in SBP_predict:\n",
        "#     for i in range(len(predicts))\n",
        "#         temp_predict.append(predicts[i])\n",
        "# print(SBP_predict[0:20])\n",
        "\n",
        "# print(SBP_test[0:20])\n",
        "import time\n",
        "\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "SBP_predict = np.array(SBP_nn_module.predict(SBP_test_loader)).reshape(-1)\n",
        "SBP_test = np.array(SBP_test)\n",
        "DBP_predict = np.array(DBP_nn_module.predict(DBP_test_loader)).reshape(-1)\n",
        "DBP_test = np.array(DBP_test)\n",
        "MAP_predict = np.array(MAP_nn_module.predict(MAP_test_loader)).reshape(-1)\n",
        "MAP_test = np.array(MAP_test)\n",
        "\n",
        "print('Fully Connected Nueral Network')\n",
        "\n",
        "total, ME, MAE, SD = AAMI_standard(SBP_predict, SBP_test)\n",
        "print()\n",
        "print()\n",
        "print('----------------SBP-----------------')\n",
        "print()\n",
        "print('-------------AAMI standard----------')\n",
        "print('ME             MAE             SD           total')\n",
        "print('%.3f         %.3f         %.3f          %d' % (ME,MAE,SD,total))\n",
        "#print('ME: ', ME, '   MAE: ', MAE, '   SD: ', SD, '   total: ', total)\n",
        "total, mm5, mm10, mm15 = BHS_standard(SBP_predict, SBP_test)\n",
        "print()\n",
        "print('-------------BHS standard------------')\n",
        "print('<5mmHg        <10mmHg        <15mmHg        total')\n",
        "print('%d            %d           %d          %d' % (mm5,mm10,mm15,total))\n",
        "print('%.3f%s        %.3f%s       %.3f%s         %d' % (mm5/total*100, '%',mm10/total*100, '%',mm15/total*100, '%', total))\n",
        "\n",
        "total, ME, MAE, SD = AAMI_standard(DBP_predict, DBP_test)\n",
        "print()\n",
        "print()\n",
        "print('----------------DBP-----------------')\n",
        "print()\n",
        "print('-------------AAMI standard----------')\n",
        "print('ME             MAE             SD           total')\n",
        "print('%.3f         %.3f         %.3f          %d' % (ME,MAE,SD,total))\n",
        "#print('ME: ', ME, '   MAE: ', MAE, '   SD: ', SD, '   total: ', total)\n",
        "total, mm5, mm10, mm15 = BHS_standard(DBP_predict, DBP_test)\n",
        "print()\n",
        "print('-------------BHS standard------------')\n",
        "print('<5mmHg        <10mmHg        <15mmHg        total')\n",
        "print('%d            %d           %d          %d' % (mm5,mm10,mm15,total))\n",
        "print('%.3f%s        %.3f%s       %.3f%s         %d' % (mm5/total*100, '%',mm10/total*100, '%',mm15/total*100, '%', total))\n",
        "\n",
        "total, ME, MAE, SD = AAMI_standard(MAP_predict, MAP_test)\n",
        "print()\n",
        "print()\n",
        "print('----------------MAP-----------------')\n",
        "print()\n",
        "print('-------------AAMI standard----------')\n",
        "print('ME             MAE             SD           total')\n",
        "print('%.3f         %.3f         %.3f          %d' % (ME,MAE,SD,total))\n",
        "#print('ME: ', ME, '   MAE: ', MAE, '   SD: ', SD, '   total: ', total)\n",
        "total, mm5, mm10, mm15 = BHS_standard(MAP_predict, MAP_test)\n",
        "print()\n",
        "print('-------------BHS standard------------')\n",
        "print('<5mmHg        <10mmHg        <15mmHg        total')\n",
        "print('%d            %d           %d          %d' % (mm5,mm10,mm15,total))\n",
        "print('%.3f%s        %.3f%s       %.3f%s         %d' % (mm5/total*100, '%',mm10/total*100, '%',mm15/total*100, '%', total))\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Print the elapsed time\n",
        "print(\"Time taken:\", elapsed_time, \"seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HP69JAYdEA27",
        "outputId": "d01da51d-8158-4a7a-af9d-011c948b2483"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fully Connected Nueral Network\n",
            "\n",
            "\n",
            "----------------SBP-----------------\n",
            "\n",
            "-------------AAMI standard----------\n",
            "ME             MAE             SD           total\n",
            "1.512         6.997         11.391          19663\n",
            "\n",
            "-------------BHS standard------------\n",
            "<5mmHg        <10mmHg        <15mmHg        total\n",
            "11533            15793           17379          19663\n",
            "58.653%        80.318%       88.384%         19663\n",
            "\n",
            "\n",
            "----------------DBP-----------------\n",
            "\n",
            "-------------AAMI standard----------\n",
            "ME             MAE             SD           total\n",
            "0.498         3.364         5.818          19663\n",
            "\n",
            "-------------BHS standard------------\n",
            "<5mmHg        <10mmHg        <15mmHg        total\n",
            "15973            18349           19052          19663\n",
            "81.234%        93.317%       96.893%         19663\n",
            "\n",
            "\n",
            "----------------MAP-----------------\n",
            "\n",
            "-------------AAMI standard----------\n",
            "ME             MAE             SD           total\n",
            "0.340         3.659         6.016          19663\n",
            "\n",
            "-------------BHS standard------------\n",
            "<5mmHg        <10mmHg        <15mmHg        total\n",
            "15423            18124           18950          19663\n",
            "78.437%        92.173%       96.374%         19663\n",
            "Time taken: 3.150196075439453 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Fully Connected Neural Network')\n",
        "\n",
        "SBP_cc= np.corrcoef(SBP_predict, SBP_test)\n",
        "DBP_cc= np.corrcoef(DBP_predict, DBP_test)\n",
        "MAP_cc= np.corrcoef(MAP_predict, MAP_test)\n",
        "print()\n",
        "print()\n",
        "print('------------Correlation Coefficient-------------')\n",
        "print()\n",
        "print('SBP: %.3f' % (SBP_cc[0, 1]))\n",
        "print()\n",
        "print('DBP: %.3f' % (DBP_cc[0, 1]))\n",
        "print()\n",
        "print('MAP: %.3f' % (MAP_cc[0, 1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ug_aIFLcEBCH",
        "outputId": "7a9e9f59-51a2-4ad9-9b3b-697a2f505ec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fully Connected Neural Network\n",
            "\n",
            "\n",
            "------------Correlation Coefficient-------------\n",
            "\n",
            "SBP: 0.862\n",
            "\n",
            "DBP: 0.829\n",
            "\n",
            "MAP: 0.859\n"
          ]
        }
      ]
    }
  ]
}