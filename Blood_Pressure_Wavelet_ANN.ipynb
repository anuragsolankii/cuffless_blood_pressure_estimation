{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zp8oZIxSEXM-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.ndimage import uniform_filter1d\n",
        "from scipy.signal import detrend\n",
        "\n",
        "\n",
        "def find_peaks_original(x, scale=None, debug=False):\n",
        "    \"\"\"Find peaks in quasi-periodic noisy signals using AMPD algorithm.\n",
        "    Automatic Multi-Scale Peak Detection originally proposed in\n",
        "    \"An Efficient Algorithm for Automatic Peak Detection in\n",
        "    Noisy Periodic and Quasi-Periodic Signals\", Algorithms 2012, 5, 588-603\n",
        "    https://doi.org/10.1109/ICRERA.2016.7884365\n",
        "    Optimized implementation by Igor Gotlibovych, 2018\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : ndarray\n",
        "        1-D array on which to find peaks\n",
        "    scale : int, optional\n",
        "        specify maximum scale window size of (2 * scale + 1)\n",
        "    debug : bool, optional\n",
        "        if set to True, return the Local Scalogram Matrix, `LSM`,\n",
        "        and scale with most local maxima, `l`,\n",
        "        together with peak locations\n",
        "    Returns\n",
        "    -------\n",
        "    pks: ndarray\n",
        "        The ordered array of peak indices found in `x`\n",
        "    \"\"\"\n",
        "    x = detrend(x)\n",
        "    N = len(x)\n",
        "    L = N // 2\n",
        "    if scale:\n",
        "        L = min(scale, L)\n",
        "\n",
        "    # create LSM matix\n",
        "    LSM = np.zeros((L, N), dtype=bool)\n",
        "    for k in np.arange(1, L):\n",
        "        LSM[k - 1, k:N - k] = (\n",
        "            (x[0:N - 2 * k] < x[k:N - k]) & (x[k:N - k] > x[2 * k:N])\n",
        "        )\n",
        "\n",
        "    # Find scale with most maxima\n",
        "    G = LSM.sum(axis=1)\n",
        "    l_scale = np.argmax(G)\n",
        "\n",
        "    # find peaks that persist on all scales up to l\n",
        "    pks_logical = np.min(LSM[0:l_scale, :], axis=0)\n",
        "    pks = np.flatnonzero(pks_logical)\n",
        "    if debug:\n",
        "        return pks, LSM, l_scale\n",
        "    return pks\n",
        "\n",
        "\n",
        "def find_peaks(x, scale=None, debug=False):\n",
        "    \"\"\"Find peaks in quasi-periodic noisy signals using AMPD algorithm.\n",
        "    Extended implementation handles peaks near start/end of the signal.\n",
        "    Optimized implementation by Igor Gotlibovych, 2018\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : ndarray\n",
        "        1-D array on which to find peaks\n",
        "    scale : int, optional\n",
        "        specify maximum scale window size of (2 * scale + 1)\n",
        "    debug : bool, optional\n",
        "        if set to True, return the Local Scalogram Matrix, `LSM`,\n",
        "        weigted number of maxima, 'G',\n",
        "        and scale at which G is maximized, `l`,\n",
        "        together with peak locations\n",
        "    Returns\n",
        "    -------\n",
        "    pks: ndarray\n",
        "        The ordered array of peak indices found in `x`\n",
        "    \"\"\"\n",
        "    x = detrend(x)\n",
        "    N = len(x)\n",
        "    L = N // 2\n",
        "    if scale:\n",
        "        L = min(scale, L)\n",
        "\n",
        "    # create LSM matix\n",
        "    LSM = np.ones((L, N), dtype=bool)\n",
        "    for k in np.arange(1, L + 1):\n",
        "        LSM[k - 1, 0:N - k] &= (x[0:N - k] > x[k:N]\n",
        "                                )  # compare to right neighbours\n",
        "        LSM[k - 1, k:N] &= (x[k:N] > x[0:N - k])  # compare to left neighbours\n",
        "\n",
        "    # Find scale with most maxima\n",
        "    G = LSM.sum(axis=1)\n",
        "    G = G * np.arange(\n",
        "        N // 2, N // 2 - L, -1\n",
        "    )  # normalize to adjust for new edge regions\n",
        "    l_scale = np.argmax(G)\n",
        "\n",
        "    # find peaks that persist on all scales up to l\n",
        "    pks_logical = np.min(LSM[0:l_scale, :], axis=0)\n",
        "    pks = np.flatnonzero(pks_logical)\n",
        "    if debug:\n",
        "        return pks, LSM, G, l_scale\n",
        "    return pks\n",
        "\n",
        "\n",
        "def find_peaks_adaptive(x, window=None, debug=False):\n",
        "    \"\"\"Find peaks in quasi-periodic noisy signals using ASS-AMPD algorithm.\n",
        "    Adaptive Scale Selection Automatic Multi-Scale Peak Detection,\n",
        "    an extension of AMPD -\n",
        "    \"An Efficient Algorithm for Automatic Peak Detection in\n",
        "    Noisy Periodic and Quasi-Periodic Signals\", Algorithms 2012, 5, 588-603\n",
        "    https://doi.org/10.1109/ICRERA.2016.7884365\n",
        "    Optimized implementation by Igor Gotlibovych, 2018\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : ndarray\n",
        "        1-D array on which to find peaks\n",
        "    window : int, optional\n",
        "        sliding window size for adaptive scale selection\n",
        "    debug : bool, optional\n",
        "        if set to True, return the Local Scalogram Matrix, `LSM`,\n",
        "        and `adaptive_scale`,\n",
        "        together with peak locations\n",
        "    Returns\n",
        "    -------\n",
        "    pks: ndarray\n",
        "        The ordered array of peak indices found in `x`\n",
        "    \"\"\"\n",
        "    x = detrend(x)\n",
        "    N = len(x)\n",
        "    if not window:\n",
        "        window = N\n",
        "    if window > N:\n",
        "        window = N\n",
        "    L = window // 2\n",
        "\n",
        "    # create LSM matix\n",
        "    LSM = np.ones((L, N), dtype=bool)\n",
        "    for k in np.arange(1, L + 1):\n",
        "        LSM[k - 1, 0:N - k] &= (x[0:N - k] > x[k:N]\n",
        "                                )  # compare to right neighbours\n",
        "        LSM[k - 1, k:N] &= (x[k:N] > x[0:N - k])  # compare to left neighbours\n",
        "\n",
        "    # Create continuos adaptive LSM\n",
        "    ass_LSM = uniform_filter1d(LSM * window, window, axis=1, mode='nearest')\n",
        "    normalization = np.arange(L, 0, -1)  # scale normalization weight\n",
        "    ass_LSM = ass_LSM * normalization.reshape(-1, 1)\n",
        "\n",
        "    # Find adaptive scale at each point\n",
        "    adaptive_scale = ass_LSM.argmax(axis=0)\n",
        "\n",
        "    # construct reduced LSM\n",
        "    LSM_reduced = LSM[:adaptive_scale.max(), :]\n",
        "    mask = (np.indices(LSM_reduced.shape)[0] > adaptive_scale\n",
        "            )  # these elements are outside scale of interest\n",
        "    LSM_reduced[mask] = 1\n",
        "\n",
        "    # find peaks that persist on all scales up to l\n",
        "    pks_logical = np.min(LSM_reduced, axis=0)\n",
        "    pks = np.flatnonzero(pks_logical)\n",
        "    if debug:\n",
        "        return pks, ass_LSM, adaptive_scale\n",
        "    return pks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.utils.data as Data\n",
        "import torchvision.datasets\n",
        "import torchvision.transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import glob\n",
        "# from pyampd.ampd import find_peaks\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import joblib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sheh3UCrEhm6",
        "outputId": "d92921da-9269-4205-b1b7-93f9656abd5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABouklA7Ehq4",
        "outputId": "98bd56d3-d6bb-41f8-959c-4dc67e8cc2c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip gdrive/My\\ Drive/archive.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8W2SlJ9ZEhuR",
        "outputId": "6f050aef-6e86-45c6-98e6-b2d5bc73254a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  gdrive/My Drive/archive.zip\n",
            "  inflating: Samples/rec_1.csv       \n",
            "  inflating: Samples/rec_10.csv      \n",
            "  inflating: Samples/rec_100.csv     \n",
            "  inflating: Samples/rec_101.csv     \n",
            "  inflating: Samples/rec_102.csv     \n",
            "  inflating: Samples/rec_103.csv     \n",
            "  inflating: Samples/rec_104.csv     \n",
            "  inflating: Samples/rec_105.csv     \n",
            "  inflating: Samples/rec_106.csv     \n",
            "  inflating: Samples/rec_107.csv     \n",
            "  inflating: Samples/rec_108.csv     \n",
            "  inflating: Samples/rec_109.csv     \n",
            "  inflating: Samples/rec_11.csv      \n",
            "  inflating: Samples/rec_110.csv     \n",
            "  inflating: Samples/rec_111.csv     \n",
            "  inflating: Samples/rec_112.csv     \n",
            "  inflating: Samples/rec_113.csv     \n",
            "  inflating: Samples/rec_114.csv     \n",
            "  inflating: Samples/rec_115.csv     \n",
            "  inflating: Samples/rec_116.csv     \n",
            "  inflating: Samples/rec_117.csv     \n",
            "  inflating: Samples/rec_118.csv     \n",
            "  inflating: Samples/rec_119.csv     \n",
            "  inflating: Samples/rec_12.csv      \n",
            "  inflating: Samples/rec_120.csv     \n",
            "  inflating: Samples/rec_121.csv     \n",
            "  inflating: Samples/rec_122.csv     \n",
            "  inflating: Samples/rec_123.csv     \n",
            "  inflating: Samples/rec_124.csv     \n",
            "  inflating: Samples/rec_125.csv     \n",
            "  inflating: Samples/rec_126.csv     \n",
            "  inflating: Samples/rec_127.csv     \n",
            "  inflating: Samples/rec_128.csv     \n",
            "  inflating: Samples/rec_129.csv     \n",
            "  inflating: Samples/rec_13.csv      \n",
            "  inflating: Samples/rec_130.csv     \n",
            "  inflating: Samples/rec_131.csv     \n",
            "  inflating: Samples/rec_132.csv     \n",
            "  inflating: Samples/rec_133.csv     \n",
            "  inflating: Samples/rec_134.csv     \n",
            "  inflating: Samples/rec_135.csv     \n",
            "  inflating: Samples/rec_136.csv     \n",
            "  inflating: Samples/rec_137.csv     \n",
            "  inflating: Samples/rec_138.csv     \n",
            "  inflating: Samples/rec_139.csv     \n",
            "  inflating: Samples/rec_14.csv      \n",
            "  inflating: Samples/rec_140.csv     \n",
            "  inflating: Samples/rec_141.csv     \n",
            "  inflating: Samples/rec_142.csv     \n",
            "  inflating: Samples/rec_143.csv     \n",
            "  inflating: Samples/rec_144.csv     \n",
            "  inflating: Samples/rec_145.csv     \n",
            "  inflating: Samples/rec_146.csv     \n",
            "  inflating: Samples/rec_147.csv     \n",
            "  inflating: Samples/rec_148.csv     \n",
            "  inflating: Samples/rec_149.csv     \n",
            "  inflating: Samples/rec_15.csv      \n",
            "  inflating: Samples/rec_150.csv     \n",
            "  inflating: Samples/rec_151.csv     \n",
            "  inflating: Samples/rec_152.csv     \n",
            "  inflating: Samples/rec_153.csv     \n",
            "  inflating: Samples/rec_154.csv     \n",
            "  inflating: Samples/rec_155.csv     \n",
            "  inflating: Samples/rec_156.csv     \n",
            "  inflating: Samples/rec_157.csv     \n",
            "  inflating: Samples/rec_158.csv     \n",
            "  inflating: Samples/rec_159.csv     \n",
            "  inflating: Samples/rec_16.csv      \n",
            "  inflating: Samples/rec_160.csv     \n",
            "  inflating: Samples/rec_161.csv     \n",
            "  inflating: Samples/rec_162.csv     \n",
            "  inflating: Samples/rec_163.csv     \n",
            "  inflating: Samples/rec_164.csv     \n",
            "  inflating: Samples/rec_165.csv     \n",
            "  inflating: Samples/rec_166.csv     \n",
            "  inflating: Samples/rec_167.csv     \n",
            "  inflating: Samples/rec_168.csv     \n",
            "  inflating: Samples/rec_169.csv     \n",
            "  inflating: Samples/rec_17.csv      \n",
            "  inflating: Samples/rec_170.csv     \n",
            "  inflating: Samples/rec_171.csv     \n",
            "  inflating: Samples/rec_172.csv     \n",
            "  inflating: Samples/rec_173.csv     \n",
            "  inflating: Samples/rec_174.csv     \n",
            "  inflating: Samples/rec_175.csv     \n",
            "  inflating: Samples/rec_176.csv     \n",
            "  inflating: Samples/rec_177.csv     \n",
            "  inflating: Samples/rec_178.csv     \n",
            "  inflating: Samples/rec_179.csv     \n",
            "  inflating: Samples/rec_18.csv      \n",
            "  inflating: Samples/rec_180.csv     \n",
            "  inflating: Samples/rec_181.csv     \n",
            "  inflating: Samples/rec_182.csv     \n",
            "  inflating: Samples/rec_183.csv     \n",
            "  inflating: Samples/rec_184.csv     \n",
            "  inflating: Samples/rec_185.csv     \n",
            "  inflating: Samples/rec_186.csv     \n",
            "  inflating: Samples/rec_187.csv     \n",
            "  inflating: Samples/rec_188.csv     \n",
            "  inflating: Samples/rec_189.csv     \n",
            "  inflating: Samples/rec_19.csv      \n",
            "  inflating: Samples/rec_190.csv     \n",
            "  inflating: Samples/rec_191.csv     \n",
            "  inflating: Samples/rec_192.csv     \n",
            "  inflating: Samples/rec_193.csv     \n",
            "  inflating: Samples/rec_194.csv     \n",
            "  inflating: Samples/rec_195.csv     \n",
            "  inflating: Samples/rec_196.csv     \n",
            "  inflating: Samples/rec_197.csv     \n",
            "  inflating: Samples/rec_198.csv     \n",
            "  inflating: Samples/rec_199.csv     \n",
            "  inflating: Samples/rec_2.csv       \n",
            "  inflating: Samples/rec_20.csv      \n",
            "  inflating: Samples/rec_200.csv     \n",
            "  inflating: Samples/rec_201.csv     \n",
            "  inflating: Samples/rec_202.csv     \n",
            "  inflating: Samples/rec_203.csv     \n",
            "  inflating: Samples/rec_204.csv     \n",
            "  inflating: Samples/rec_205.csv     \n",
            "  inflating: Samples/rec_206.csv     \n",
            "  inflating: Samples/rec_207.csv     \n",
            "  inflating: Samples/rec_208.csv     \n",
            "  inflating: Samples/rec_209.csv     \n",
            "  inflating: Samples/rec_21.csv      \n",
            "  inflating: Samples/rec_210.csv     \n",
            "  inflating: Samples/rec_211.csv     \n",
            "  inflating: Samples/rec_212.csv     \n",
            "  inflating: Samples/rec_213.csv     \n",
            "  inflating: Samples/rec_214.csv     \n",
            "  inflating: Samples/rec_215.csv     \n",
            "  inflating: Samples/rec_216.csv     \n",
            "  inflating: Samples/rec_217.csv     \n",
            "  inflating: Samples/rec_218.csv     \n",
            "  inflating: Samples/rec_219.csv     \n",
            "  inflating: Samples/rec_22.csv      \n",
            "  inflating: Samples/rec_220.csv     \n",
            "  inflating: Samples/rec_221.csv     \n",
            "  inflating: Samples/rec_222.csv     \n",
            "  inflating: Samples/rec_223.csv     \n",
            "  inflating: Samples/rec_224.csv     \n",
            "  inflating: Samples/rec_225.csv     \n",
            "  inflating: Samples/rec_226.csv     \n",
            "  inflating: Samples/rec_227.csv     \n",
            "  inflating: Samples/rec_228.csv     \n",
            "  inflating: Samples/rec_229.csv     \n",
            "  inflating: Samples/rec_23.csv      \n",
            "  inflating: Samples/rec_230.csv     \n",
            "  inflating: Samples/rec_231.csv     \n",
            "  inflating: Samples/rec_232.csv     \n",
            "  inflating: Samples/rec_233.csv     \n",
            "  inflating: Samples/rec_234.csv     \n",
            "  inflating: Samples/rec_235.csv     \n",
            "  inflating: Samples/rec_236.csv     \n",
            "  inflating: Samples/rec_237.csv     \n",
            "  inflating: Samples/rec_238.csv     \n",
            "  inflating: Samples/rec_239.csv     \n",
            "  inflating: Samples/rec_24.csv      \n",
            "  inflating: Samples/rec_240.csv     \n",
            "  inflating: Samples/rec_241.csv     \n",
            "  inflating: Samples/rec_242.csv     \n",
            "  inflating: Samples/rec_243.csv     \n",
            "  inflating: Samples/rec_244.csv     \n",
            "  inflating: Samples/rec_245.csv     \n",
            "  inflating: Samples/rec_246.csv     \n",
            "  inflating: Samples/rec_247.csv     \n",
            "  inflating: Samples/rec_248.csv     \n",
            "  inflating: Samples/rec_249.csv     \n",
            "  inflating: Samples/rec_25.csv      \n",
            "  inflating: Samples/rec_250.csv     \n",
            "  inflating: Samples/rec_251.csv     \n",
            "  inflating: Samples/rec_252.csv     \n",
            "  inflating: Samples/rec_253.csv     \n",
            "  inflating: Samples/rec_254.csv     \n",
            "  inflating: Samples/rec_255.csv     \n",
            "  inflating: Samples/rec_256.csv     \n",
            "  inflating: Samples/rec_257.csv     \n",
            "  inflating: Samples/rec_258.csv     \n",
            "  inflating: Samples/rec_259.csv     \n",
            "  inflating: Samples/rec_26.csv      \n",
            "  inflating: Samples/rec_260.csv     \n",
            "  inflating: Samples/rec_261.csv     \n",
            "  inflating: Samples/rec_262.csv     \n",
            "  inflating: Samples/rec_263.csv     \n",
            "  inflating: Samples/rec_264.csv     \n",
            "  inflating: Samples/rec_265.csv     \n",
            "  inflating: Samples/rec_266.csv     \n",
            "  inflating: Samples/rec_267.csv     \n",
            "  inflating: Samples/rec_268.csv     \n",
            "  inflating: Samples/rec_269.csv     \n",
            "  inflating: Samples/rec_27.csv      \n",
            "  inflating: Samples/rec_270.csv     \n",
            "  inflating: Samples/rec_271.csv     \n",
            "  inflating: Samples/rec_272.csv     \n",
            "  inflating: Samples/rec_273.csv     \n",
            "  inflating: Samples/rec_274.csv     \n",
            "  inflating: Samples/rec_275.csv     \n",
            "  inflating: Samples/rec_276.csv     \n",
            "  inflating: Samples/rec_277.csv     \n",
            "  inflating: Samples/rec_278.csv     \n",
            "  inflating: Samples/rec_279.csv     \n",
            "  inflating: Samples/rec_28.csv      \n",
            "  inflating: Samples/rec_280.csv     \n",
            "  inflating: Samples/rec_281.csv     \n",
            "  inflating: Samples/rec_282.csv     \n",
            "  inflating: Samples/rec_283.csv     \n",
            "  inflating: Samples/rec_284.csv     \n",
            "  inflating: Samples/rec_285.csv     \n",
            "  inflating: Samples/rec_286.csv     \n",
            "  inflating: Samples/rec_287.csv     \n",
            "  inflating: Samples/rec_288.csv     \n",
            "  inflating: Samples/rec_289.csv     \n",
            "  inflating: Samples/rec_29.csv      \n",
            "  inflating: Samples/rec_290.csv     \n",
            "  inflating: Samples/rec_291.csv     \n",
            "  inflating: Samples/rec_292.csv     \n",
            "  inflating: Samples/rec_293.csv     \n",
            "  inflating: Samples/rec_294.csv     \n",
            "  inflating: Samples/rec_295.csv     \n",
            "  inflating: Samples/rec_296.csv     \n",
            "  inflating: Samples/rec_297.csv     \n",
            "  inflating: Samples/rec_298.csv     \n",
            "  inflating: Samples/rec_299.csv     \n",
            "  inflating: Samples/rec_3.csv       \n",
            "  inflating: Samples/rec_30.csv      \n",
            "  inflating: Samples/rec_300.csv     \n",
            "  inflating: Samples/rec_301.csv     \n",
            "  inflating: Samples/rec_302.csv     \n",
            "  inflating: Samples/rec_303.csv     \n",
            "  inflating: Samples/rec_304.csv     \n",
            "  inflating: Samples/rec_305.csv     \n",
            "  inflating: Samples/rec_306.csv     \n",
            "  inflating: Samples/rec_307.csv     \n",
            "  inflating: Samples/rec_308.csv     \n",
            "  inflating: Samples/rec_309.csv     \n",
            "  inflating: Samples/rec_31.csv      \n",
            "  inflating: Samples/rec_310.csv     \n",
            "  inflating: Samples/rec_311.csv     \n",
            "  inflating: Samples/rec_312.csv     \n",
            "  inflating: Samples/rec_313.csv     \n",
            "  inflating: Samples/rec_314.csv     \n",
            "  inflating: Samples/rec_315.csv     \n",
            "  inflating: Samples/rec_316.csv     \n",
            "  inflating: Samples/rec_317.csv     \n",
            "  inflating: Samples/rec_318.csv     \n",
            "  inflating: Samples/rec_319.csv     \n",
            "  inflating: Samples/rec_32.csv      \n",
            "  inflating: Samples/rec_320.csv     \n",
            "  inflating: Samples/rec_321.csv     \n",
            "  inflating: Samples/rec_322.csv     \n",
            "  inflating: Samples/rec_323.csv     \n",
            "  inflating: Samples/rec_324.csv     \n",
            "  inflating: Samples/rec_325.csv     \n",
            "  inflating: Samples/rec_326.csv     \n",
            "  inflating: Samples/rec_327.csv     \n",
            "  inflating: Samples/rec_328.csv     \n",
            "  inflating: Samples/rec_329.csv     \n",
            "  inflating: Samples/rec_33.csv      \n",
            "  inflating: Samples/rec_330.csv     \n",
            "  inflating: Samples/rec_331.csv     \n",
            "  inflating: Samples/rec_332.csv     \n",
            "  inflating: Samples/rec_333.csv     \n",
            "  inflating: Samples/rec_334.csv     \n",
            "  inflating: Samples/rec_335.csv     \n",
            "  inflating: Samples/rec_336.csv     \n",
            "  inflating: Samples/rec_337.csv     \n",
            "  inflating: Samples/rec_338.csv     \n",
            "  inflating: Samples/rec_339.csv     \n",
            "  inflating: Samples/rec_34.csv      \n",
            "  inflating: Samples/rec_340.csv     \n",
            "  inflating: Samples/rec_341.csv     \n",
            "  inflating: Samples/rec_342.csv     \n",
            "  inflating: Samples/rec_343.csv     \n",
            "  inflating: Samples/rec_344.csv     \n",
            "  inflating: Samples/rec_345.csv     \n",
            "  inflating: Samples/rec_346.csv     \n",
            "  inflating: Samples/rec_347.csv     \n",
            "  inflating: Samples/rec_348.csv     \n",
            "  inflating: Samples/rec_349.csv     \n",
            "  inflating: Samples/rec_35.csv      \n",
            "  inflating: Samples/rec_350.csv     \n",
            "  inflating: Samples/rec_351.csv     \n",
            "  inflating: Samples/rec_352.csv     \n",
            "  inflating: Samples/rec_353.csv     \n",
            "  inflating: Samples/rec_354.csv     \n",
            "  inflating: Samples/rec_355.csv     \n",
            "  inflating: Samples/rec_356.csv     \n",
            "  inflating: Samples/rec_357.csv     \n",
            "  inflating: Samples/rec_358.csv     \n",
            "  inflating: Samples/rec_359.csv     \n",
            "  inflating: Samples/rec_36.csv      \n",
            "  inflating: Samples/rec_360.csv     \n",
            "  inflating: Samples/rec_361.csv     \n",
            "  inflating: Samples/rec_362.csv     \n",
            "  inflating: Samples/rec_363.csv     \n",
            "  inflating: Samples/rec_364.csv     \n",
            "  inflating: Samples/rec_365.csv     \n",
            "  inflating: Samples/rec_366.csv     \n",
            "  inflating: Samples/rec_367.csv     \n",
            "  inflating: Samples/rec_368.csv     \n",
            "  inflating: Samples/rec_369.csv     \n",
            "  inflating: Samples/rec_37.csv      \n",
            "  inflating: Samples/rec_370.csv     \n",
            "  inflating: Samples/rec_371.csv     \n",
            "  inflating: Samples/rec_372.csv     \n",
            "  inflating: Samples/rec_373.csv     \n",
            "  inflating: Samples/rec_374.csv     \n",
            "  inflating: Samples/rec_375.csv     \n",
            "  inflating: Samples/rec_376.csv     \n",
            "  inflating: Samples/rec_377.csv     \n",
            "  inflating: Samples/rec_378.csv     \n",
            "  inflating: Samples/rec_379.csv     \n",
            "  inflating: Samples/rec_38.csv      \n",
            "  inflating: Samples/rec_380.csv     \n",
            "  inflating: Samples/rec_381.csv     \n",
            "  inflating: Samples/rec_382.csv     \n",
            "  inflating: Samples/rec_383.csv     \n",
            "  inflating: Samples/rec_384.csv     \n",
            "  inflating: Samples/rec_385.csv     \n",
            "  inflating: Samples/rec_386.csv     \n",
            "  inflating: Samples/rec_387.csv     \n",
            "  inflating: Samples/rec_388.csv     \n",
            "  inflating: Samples/rec_389.csv     \n",
            "  inflating: Samples/rec_39.csv      \n",
            "  inflating: Samples/rec_390.csv     \n",
            "  inflating: Samples/rec_391.csv     \n",
            "  inflating: Samples/rec_392.csv     \n",
            "  inflating: Samples/rec_393.csv     \n",
            "  inflating: Samples/rec_394.csv     \n",
            "  inflating: Samples/rec_395.csv     \n",
            "  inflating: Samples/rec_396.csv     \n",
            "  inflating: Samples/rec_397.csv     \n",
            "  inflating: Samples/rec_398.csv     \n",
            "  inflating: Samples/rec_399.csv     \n",
            "  inflating: Samples/rec_4.csv       \n",
            "  inflating: Samples/rec_40.csv      \n",
            "  inflating: Samples/rec_400.csv     \n",
            "  inflating: Samples/rec_401.csv     \n",
            "  inflating: Samples/rec_402.csv     \n",
            "  inflating: Samples/rec_403.csv     \n",
            "  inflating: Samples/rec_404.csv     \n",
            "  inflating: Samples/rec_405.csv     \n",
            "  inflating: Samples/rec_406.csv     \n",
            "  inflating: Samples/rec_407.csv     \n",
            "  inflating: Samples/rec_408.csv     \n",
            "  inflating: Samples/rec_409.csv     \n",
            "  inflating: Samples/rec_41.csv      \n",
            "  inflating: Samples/rec_410.csv     \n",
            "  inflating: Samples/rec_411.csv     \n",
            "  inflating: Samples/rec_412.csv     \n",
            "  inflating: Samples/rec_413.csv     \n",
            "  inflating: Samples/rec_414.csv     \n",
            "  inflating: Samples/rec_415.csv     \n",
            "  inflating: Samples/rec_416.csv     \n",
            "  inflating: Samples/rec_417.csv     \n",
            "  inflating: Samples/rec_418.csv     \n",
            "  inflating: Samples/rec_419.csv     \n",
            "  inflating: Samples/rec_42.csv      \n",
            "  inflating: Samples/rec_420.csv     \n",
            "  inflating: Samples/rec_421.csv     \n",
            "  inflating: Samples/rec_422.csv     \n",
            "  inflating: Samples/rec_423.csv     \n",
            "  inflating: Samples/rec_424.csv     \n",
            "  inflating: Samples/rec_425.csv     \n",
            "  inflating: Samples/rec_426.csv     \n",
            "  inflating: Samples/rec_427.csv     \n",
            "  inflating: Samples/rec_428.csv     \n",
            "  inflating: Samples/rec_429.csv     \n",
            "  inflating: Samples/rec_43.csv      \n",
            "  inflating: Samples/rec_430.csv     \n",
            "  inflating: Samples/rec_431.csv     \n",
            "  inflating: Samples/rec_432.csv     \n",
            "  inflating: Samples/rec_433.csv     \n",
            "  inflating: Samples/rec_434.csv     \n",
            "  inflating: Samples/rec_435.csv     \n",
            "  inflating: Samples/rec_436.csv     \n",
            "  inflating: Samples/rec_437.csv     \n",
            "  inflating: Samples/rec_438.csv     \n",
            "  inflating: Samples/rec_439.csv     \n",
            "  inflating: Samples/rec_44.csv      \n",
            "  inflating: Samples/rec_440.csv     \n",
            "  inflating: Samples/rec_441.csv     \n",
            "  inflating: Samples/rec_442.csv     \n",
            "  inflating: Samples/rec_443.csv     \n",
            "  inflating: Samples/rec_444.csv     \n",
            "  inflating: Samples/rec_445.csv     \n",
            "  inflating: Samples/rec_446.csv     \n",
            "  inflating: Samples/rec_447.csv     \n",
            "  inflating: Samples/rec_448.csv     \n",
            "  inflating: Samples/rec_449.csv     \n",
            "  inflating: Samples/rec_45.csv      \n",
            "  inflating: Samples/rec_450.csv     \n",
            "  inflating: Samples/rec_451.csv     \n",
            "  inflating: Samples/rec_452.csv     \n",
            "  inflating: Samples/rec_453.csv     \n",
            "  inflating: Samples/rec_454.csv     \n",
            "  inflating: Samples/rec_455.csv     \n",
            "  inflating: Samples/rec_456.csv     \n",
            "  inflating: Samples/rec_457.csv     \n",
            "  inflating: Samples/rec_458.csv     \n",
            "  inflating: Samples/rec_459.csv     \n",
            "  inflating: Samples/rec_46.csv      \n",
            "  inflating: Samples/rec_460.csv     \n",
            "  inflating: Samples/rec_461.csv     \n",
            "  inflating: Samples/rec_462.csv     \n",
            "  inflating: Samples/rec_463.csv     \n",
            "  inflating: Samples/rec_464.csv     \n",
            "  inflating: Samples/rec_465.csv     \n",
            "  inflating: Samples/rec_466.csv     \n",
            "  inflating: Samples/rec_467.csv     \n",
            "  inflating: Samples/rec_468.csv     \n",
            "  inflating: Samples/rec_469.csv     \n",
            "  inflating: Samples/rec_47.csv      \n",
            "  inflating: Samples/rec_470.csv     \n",
            "  inflating: Samples/rec_471.csv     \n",
            "  inflating: Samples/rec_472.csv     \n",
            "  inflating: Samples/rec_473.csv     \n",
            "  inflating: Samples/rec_474.csv     \n",
            "  inflating: Samples/rec_475.csv     \n",
            "  inflating: Samples/rec_476.csv     \n",
            "  inflating: Samples/rec_477.csv     \n",
            "  inflating: Samples/rec_478.csv     \n",
            "  inflating: Samples/rec_479.csv     \n",
            "  inflating: Samples/rec_48.csv      \n",
            "  inflating: Samples/rec_480.csv     \n",
            "  inflating: Samples/rec_481.csv     \n",
            "  inflating: Samples/rec_482.csv     \n",
            "  inflating: Samples/rec_483.csv     \n",
            "  inflating: Samples/rec_484.csv     \n",
            "  inflating: Samples/rec_485.csv     \n",
            "  inflating: Samples/rec_486.csv     \n",
            "  inflating: Samples/rec_487.csv     \n",
            "  inflating: Samples/rec_488.csv     \n",
            "  inflating: Samples/rec_489.csv     \n",
            "  inflating: Samples/rec_49.csv      \n",
            "  inflating: Samples/rec_490.csv     \n",
            "  inflating: Samples/rec_491.csv     \n",
            "  inflating: Samples/rec_492.csv     \n",
            "  inflating: Samples/rec_493.csv     \n",
            "  inflating: Samples/rec_494.csv     \n",
            "  inflating: Samples/rec_495.csv     \n",
            "  inflating: Samples/rec_496.csv     \n",
            "  inflating: Samples/rec_497.csv     \n",
            "  inflating: Samples/rec_498.csv     \n",
            "  inflating: Samples/rec_499.csv     \n",
            "  inflating: Samples/rec_5.csv       \n",
            "  inflating: Samples/rec_50.csv      \n",
            "  inflating: Samples/rec_500.csv     \n",
            "  inflating: Samples/rec_51.csv      \n",
            "  inflating: Samples/rec_52.csv      \n",
            "  inflating: Samples/rec_53.csv      \n",
            "  inflating: Samples/rec_54.csv      \n",
            "  inflating: Samples/rec_55.csv      \n",
            "  inflating: Samples/rec_56.csv      \n",
            "  inflating: Samples/rec_57.csv      \n",
            "  inflating: Samples/rec_58.csv      \n",
            "  inflating: Samples/rec_59.csv      \n",
            "  inflating: Samples/rec_6.csv       \n",
            "  inflating: Samples/rec_60.csv      \n",
            "  inflating: Samples/rec_61.csv      \n",
            "  inflating: Samples/rec_62.csv      \n",
            "  inflating: Samples/rec_63.csv      \n",
            "  inflating: Samples/rec_64.csv      \n",
            "  inflating: Samples/rec_65.csv      \n",
            "  inflating: Samples/rec_66.csv      \n",
            "  inflating: Samples/rec_67.csv      \n",
            "  inflating: Samples/rec_68.csv      \n",
            "  inflating: Samples/rec_69.csv      \n",
            "  inflating: Samples/rec_7.csv       \n",
            "  inflating: Samples/rec_70.csv      \n",
            "  inflating: Samples/rec_71.csv      \n",
            "  inflating: Samples/rec_72.csv      \n",
            "  inflating: Samples/rec_73.csv      \n",
            "  inflating: Samples/rec_74.csv      \n",
            "  inflating: Samples/rec_75.csv      \n",
            "  inflating: Samples/rec_76.csv      \n",
            "  inflating: Samples/rec_77.csv      \n",
            "  inflating: Samples/rec_78.csv      \n",
            "  inflating: Samples/rec_79.csv      \n",
            "  inflating: Samples/rec_8.csv       \n",
            "  inflating: Samples/rec_80.csv      \n",
            "  inflating: Samples/rec_81.csv      \n",
            "  inflating: Samples/rec_82.csv      \n",
            "  inflating: Samples/rec_83.csv      \n",
            "  inflating: Samples/rec_84.csv      \n",
            "  inflating: Samples/rec_85.csv      \n",
            "  inflating: Samples/rec_86.csv      \n",
            "  inflating: Samples/rec_87.csv      \n",
            "  inflating: Samples/rec_88.csv      \n",
            "  inflating: Samples/rec_89.csv      \n",
            "  inflating: Samples/rec_9.csv       \n",
            "  inflating: Samples/rec_90.csv      \n",
            "  inflating: Samples/rec_91.csv      \n",
            "  inflating: Samples/rec_92.csv      \n",
            "  inflating: Samples/rec_93.csv      \n",
            "  inflating: Samples/rec_94.csv      \n",
            "  inflating: Samples/rec_95.csv      \n",
            "  inflating: Samples/rec_96.csv      \n",
            "  inflating: Samples/rec_97.csv      \n",
            "  inflating: Samples/rec_98.csv      \n",
            "  inflating: Samples/rec_99.csv      \n",
            "  inflating: part_1.mat              \n",
            "  inflating: part_10.mat             \n",
            "  inflating: part_11.mat             \n",
            "  inflating: part_12.mat             \n",
            "  inflating: part_2.mat              \n",
            "  inflating: part_3.mat              \n",
            "  inflating: part_4.mat              \n",
            "  inflating: part_5.mat              \n",
            "  inflating: part_6.mat              \n",
            "  inflating: part_7.mat              \n",
            "  inflating: part_8.mat              \n",
            "  inflating: part_9.mat              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PPG_datas = []\n",
        "ABP_datas = []\n",
        "ECG_datas = []\n",
        "i = 0\n",
        "for name in glob.glob('./Samples/*.csv'):\n",
        "    raw_training_data = pd.read_csv(name, header=None)\n",
        "    raw_training_data = np.array(raw_training_data)\n",
        "    #print(raw_training_data.shape)\n",
        "    PPG_data = raw_training_data[0]\n",
        "    ABP_data = raw_training_data[1]\n",
        "    ECG_data = raw_training_data[2]\n",
        "\n",
        "    PPG_datas.append(PPG_data)\n",
        "    ABP_datas.append(ABP_data)\n",
        "    ECG_datas.append(ECG_data)\n",
        "    # i = i + 1\n",
        " #   print(i)\n",
        "#     if (i == 100):\n",
        "#         break\n",
        "\n",
        "PPG_datas = np.array(PPG_datas)\n",
        "ABP_datas = np.array(ABP_datas)\n",
        "ECG_datas = np.array(ECG_datas)\n",
        "# raw_training_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQnlw_RMEhxS",
        "outputId": "e49ec06e-37b2-4304-9a74-b075c9bc8608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-cafb0a523fd7>:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  PPG_datas = np.array(PPG_datas)\n",
            "<ipython-input-6-cafb0a523fd7>:22: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  ABP_datas = np.array(ABP_datas)\n",
            "<ipython-input-6-cafb0a523fd7>:23: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  ECG_datas = np.array(ECG_datas)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pywt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def generate_wavelet_vector(X, wavelet_family, decomposition_level):\n",
        "    coefficients = pywt.wavedec(X, wavelet_family, level=decomposition_level)\n",
        "    vector = np.array([])\n",
        "    for coeffs in coefficients:\n",
        "        vector = np.concatenate((vector, coeffs))\n",
        "    return vector"
      ],
      "metadata": {
        "id": "b4udcgVNEh0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLE_FREQ = 125"
      ],
      "metadata": {
        "id": "DEuXQ4uUEh3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wavelet_family = 'db4'\n",
        "decomposition_level = 5\n",
        "\n",
        "wavelet_vectors = []\n",
        "SBP_data = []\n",
        "DBP_data = []\n",
        "MAP_data = []\n",
        "\n",
        "for j in range(len(PPG_datas)):\n",
        "    sec_15 = 15 * SAMPLE_FREQ\n",
        "    PPG_data = PPG_datas[j]\n",
        "    ABP_data = ABP_datas[j]\n",
        "    PPG_peaks = find_peaks(PPG_data, scale=SAMPLE_FREQ)\n",
        "    for i in range(2, PPG_peaks.shape[0]):\n",
        "        X = PPG_data[PPG_peaks[i - 1]:PPG_peaks[i]]\n",
        "        if len(X) < SAMPLE_FREQ:\n",
        "            wavelet_vector = generate_wavelet_vector(X, wavelet_family, decomposition_level)\n",
        "\n",
        "            SBP = np.max(ABP_data[PPG_peaks[i - 1]:PPG_peaks[i - 1] + sec_15])\n",
        "            DBP = np.min(ABP_data[PPG_peaks[i - 1]:PPG_peaks[i - 1] + sec_15])\n",
        "            MAP = SBP / 3 + 2 * DBP / 3\n",
        "\n",
        "            wavelet_vectors.append(wavelet_vector)\n",
        "            SBP_data.append(SBP)\n",
        "            DBP_data.append(DBP)\n",
        "            MAP_data.append(MAP)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8B6aHrPEh6A",
        "outputId": "c0e42dd2-b03d-4bc6-f4cc-10c6da35ace0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pywt/_multilevel.py:43: UserWarning: Level value of 5 is too high: all coefficients will experience boundary effects.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wavelet_vectors = np.array(wavelet_vectors)\n",
        "SBP_data = np.array(SBP_data)\n",
        "DBP_data = np.array(DBP_data)\n",
        "MAP_data = np.array(MAP_data)\n",
        "\n",
        "print(wavelet_vectors.shape)\n",
        "print(SBP_data.shape)\n",
        "\n",
        "# Find the maximum length of the wavelet vectors\n",
        "max_length = max([len(vector) for vector in wavelet_vectors])\n",
        "\n",
        "# Pad the wavelet vectors with zeros to make them uniform length\n",
        "padded_wavelet_vectors = []\n",
        "for vector in wavelet_vectors:\n",
        "    padded_vector = np.pad(vector, (0, max_length - len(vector)), mode='constant')\n",
        "    padded_wavelet_vectors.append(padded_vector)\n",
        "\n",
        "padded_wavelet_vectors = np.array(padded_wavelet_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsMnvkmfEh83",
        "outputId": "47dd65ff-bb97-4684-815d-0cd7f86135f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-6f3e639507bb>:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  wavelet_vectors = np.array(wavelet_vectors)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(196625,)\n",
            "(196625,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=43)\n",
        "pca_wavelet_vectors = pca.fit_transform(padded_wavelet_vectors)\n",
        "\n",
        "print(pca_wavelet_vectors.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuNArWbyEh_f",
        "outputId": "0ab07dcb-18c9-491c-dc7c-0518b82d1c8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(196625, 43)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, SBP_train, SBP_test, DBP_train, DBP_test, MAP_train, MAP_test = train_test_split(\n",
        "    pca_wavelet_vectors, SBP_data, DBP_data, MAP_data, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "Z8a3yDcVEiCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def AAMI_standard(predict, test):\n",
        "    total = len(predict)\n",
        "    ME = np.mean(predict - test)\n",
        "    MAE = np.mean(np.abs(predict-test))\n",
        "    SD = np.std(predict-test)\n",
        "\n",
        "    return total, ME, MAE, SD\n",
        "\n",
        "def BHS_standard(predict, test):\n",
        "    total = len(predict)\n",
        "    mm5 = np.sum(np.abs(predict-test)<=5)\n",
        "    mm10 = np.sum(np.abs(predict-test)<=10)\n",
        "    mm15 = np.sum(np.abs(predict-test)<=15)\n",
        "    return total, mm5, mm10, mm15"
      ],
      "metadata": {
        "id": "WIhMGA1KEiE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 2\n",
        "LR = 0.001"
      ],
      "metadata": {
        "id": "UTj-YLzuEiHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ANN(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(ANN, self).__init__()\n",
        "        self.input = nn.Linear(input_size, 1024)\n",
        "        self.hidden1 = nn.Linear(1024, 1024)\n",
        "        self.hidden2 = nn.Linear(1024, 1024)\n",
        "        self.output = nn.Linear(1024, 1)\n",
        "        self.batch_norm1 = nn.BatchNorm1d(1024)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(1024)\n",
        "        self.batch_norm3 = nn.BatchNorm1d(1024)\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.input(x)\n",
        "        out = self.batch_norm1(out)\n",
        "        out = self.activation(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        out = self.hidden1(out)\n",
        "        out = self.batch_norm2(out)\n",
        "        out = self.activation(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        out = self.hidden2(out)\n",
        "        out = self.batch_norm3(out)\n",
        "        out = self.activation(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        out = self.output(out)\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "pOV5n-GwEiKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_default_tensor_type(torch.FloatTensor)\n",
        "\n",
        "class ANNModule():\n",
        "    def __init__(self, net, train_loader, test_loader, EPOCH=20, LR=0.0001):\n",
        "        self.net = net\n",
        "        self.optimizer = torch.optim.Adam(net.parameters(), lr = LR)\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.EPOCHS = EPOCH\n",
        "        self.LR = LR\n",
        "\n",
        "        self.net = self.net.to(device)\n",
        "        if device == 'cuda':\n",
        "            self.net = torch.nn.DataParallel(self.net)\n",
        "            torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    def start(self):\n",
        "\n",
        "        self.history_train_loss = []\n",
        "        self.history_test_loss = []\n",
        "#         history_train_accuracy = []\n",
        "#         history_test_accuracy = []\n",
        "        for epoch in range(self.EPOCHS):\n",
        "            print('Epoch:', epoch)\n",
        "            train_loss = self.train()\n",
        "            test_loss = self.test()\n",
        "\n",
        "            self.history_train_loss.append(train_loss)\n",
        "            self.history_test_loss.append(test_loss)\n",
        "#             history_train_accuracy.append(train_accuracy)\n",
        "#             history_test_accuracy.append(test_accuracy)\n",
        "\n",
        "    def train(self):\n",
        "        self.net.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for step, (batch_X, batch_y) in enumerate(self.train_loader):\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.net(batch_X).double()\n",
        "\n",
        "            loss = self.criterion(outputs.reshape(-1), batch_y)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            #_, predicted = outputs.max(1)\n",
        "            #total += batch_y.size(0)\n",
        "            #correct += predicted.eq(batch_y).sum().item()\n",
        "\n",
        "        print('【Training】Loss: %.3f ' % ( train_loss))\n",
        "        return train_loss\n",
        "\n",
        "    def test(self):\n",
        "        self.net.eval()\n",
        "\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for step, (batch_X, batch_y) in enumerate(self.test_loader):\n",
        "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "                outputs = self.net(batch_X).double()\n",
        "                loss = self.criterion(outputs.reshape(-1), batch_y)\n",
        "\n",
        "                test_loss += loss.item()\n",
        "#                 _, predicted = outputs.max(1)\n",
        "#                 #print(predicted, batch_y)\n",
        "#                 total += batch_y.size(0)\n",
        "#                 correct += predicted.eq(batch_y).sum().item()\n",
        "\n",
        "        print('【Testing】Loss: %.3f )' % ( test_loss))\n",
        "        return test_loss\n",
        "\n",
        "    def predict(self, test_loader):\n",
        "        outputs = []\n",
        "        self.net.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for step, (batch_X, batch_y) in enumerate(test_loader):\n",
        "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "                output = self.net(batch_X).double().cpu().numpy().reshape(-1)\n",
        "                for i in range(len(output)):\n",
        "                    outputs.append(output[i])\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def get_model(self):\n",
        "        return self.net.state_dict()"
      ],
      "metadata": {
        "id": "S0WCfC5gEiNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "L = 43\n",
        "\n",
        "x_train, x_test, SBP_train, SBP_test, DBP_train, DBP_test, MAP_train, MAP_test = train_test_split(\n",
        "                                            pca_wavelet_vectors, SBP_data, DBP_data, MAP_data, test_size=0.1, random_state=42)\n",
        "\n",
        "\n",
        "x_train = torch.from_numpy(x_train).type(torch.FloatTensor)\n",
        "x_test = torch.from_numpy(x_test).type(torch.FloatTensor)\n",
        "SBP_train = torch.from_numpy(SBP_train)\n",
        "SBP_test = torch.from_numpy(SBP_test)\n",
        "DBP_train = torch.from_numpy(DBP_train)\n",
        "DBP_test = torch.from_numpy(DBP_test)\n",
        "MAP_train = torch.from_numpy(MAP_train)\n",
        "MAP_test = torch.from_numpy(MAP_test)\n",
        "\n",
        "print(x_train.shape, x_test.shape, SBP_train.shape, SBP_test.shape)\n",
        "\n",
        "SBP_train_dataset = Data.TensorDataset(x_train, SBP_train)\n",
        "SBP_test_dataset = Data.TensorDataset(x_test, SBP_test)\n",
        "\n",
        "SBP_train_loader = Data.DataLoader(\n",
        "    dataset=SBP_train_dataset,\n",
        "    batch_size=128,\n",
        ")\n",
        "\n",
        "SBP_test_loader = Data.DataLoader(\n",
        "    dataset=SBP_test_dataset,\n",
        "    batch_size=10,\n",
        ")\n",
        "\n",
        "DBP_train_dataset = Data.TensorDataset(x_train, DBP_train)\n",
        "DBP_test_dataset = Data.TensorDataset(x_test, DBP_test)\n",
        "\n",
        "DBP_train_loader = Data.DataLoader(\n",
        "    dataset=DBP_train_dataset,\n",
        "    batch_size=128,\n",
        ")\n",
        "\n",
        "DBP_test_loader = Data.DataLoader(\n",
        "    dataset=DBP_test_dataset,\n",
        "    batch_size=10,\n",
        ")\n",
        "\n",
        "MAP_train_dataset = Data.TensorDataset(x_train, MAP_train)\n",
        "MAP_test_dataset = Data.TensorDataset(x_test, MAP_test)\n",
        "\n",
        "MAP_train_loader = Data.DataLoader(\n",
        "    dataset=MAP_train_dataset,\n",
        "    batch_size=128,\n",
        ")\n",
        "\n",
        "MAP_test_loader = Data.DataLoader(\n",
        "    dataset=MAP_test_dataset,\n",
        "    batch_size=10,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ciU2jDZEiQr",
        "outputId": "ecf36690-282f-4d99-cf37-8c2480fb78e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([176962, 43]) torch.Size([19663, 43]) torch.Size([176962]) torch.Size([19663])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SBP_ann_module = ANNModule(ANN(input_size=L), SBP_train_loader, SBP_test_loader, EPOCH=200, LR=LR)\n",
        "SBP_ann_module.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvTk3vNxF_2V",
        "outputId": "38df9f0a-1499-43b8-ede7-085365f3d3d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "【Training】Loss: 2208646.195 \n",
            "【Testing】Loss: 516690.737 )\n",
            "Epoch: 1\n",
            "【Training】Loss: 382960.898 \n",
            "【Testing】Loss: 462076.544 )\n",
            "Epoch: 2\n",
            "【Training】Loss: 352671.321 \n",
            "【Testing】Loss: 425015.012 )\n",
            "Epoch: 3\n",
            "【Training】Loss: 332808.794 \n",
            "【Testing】Loss: 407127.122 )\n",
            "Epoch: 4\n",
            "【Training】Loss: 318978.871 \n",
            "【Testing】Loss: 392323.805 )\n",
            "Epoch: 5\n",
            "【Training】Loss: 307070.080 \n",
            "【Testing】Loss: 379309.210 )\n",
            "Epoch: 6\n",
            "【Training】Loss: 298116.867 \n",
            "【Testing】Loss: 365663.206 )\n",
            "Epoch: 7\n",
            "【Training】Loss: 290489.393 \n",
            "【Testing】Loss: 361374.987 )\n",
            "Epoch: 8\n",
            "【Training】Loss: 283634.575 \n",
            "【Testing】Loss: 352393.496 )\n",
            "Epoch: 9\n",
            "【Training】Loss: 277086.526 \n",
            "【Testing】Loss: 342711.935 )\n",
            "Epoch: 10\n",
            "【Training】Loss: 270443.836 \n",
            "【Testing】Loss: 342080.798 )\n",
            "Epoch: 11\n",
            "【Training】Loss: 264362.281 \n",
            "【Testing】Loss: 334605.242 )\n",
            "Epoch: 12\n",
            "【Training】Loss: 260691.129 \n",
            "【Testing】Loss: 328051.314 )\n",
            "Epoch: 13\n",
            "【Training】Loss: 256343.806 \n",
            "【Testing】Loss: 324540.325 )\n",
            "Epoch: 14\n",
            "【Training】Loss: 251824.192 \n",
            "【Testing】Loss: 321017.363 )\n",
            "Epoch: 15\n",
            "【Training】Loss: 247415.652 \n",
            "【Testing】Loss: 319001.998 )\n",
            "Epoch: 16\n",
            "【Training】Loss: 243978.345 \n",
            "【Testing】Loss: 313268.778 )\n",
            "Epoch: 17\n",
            "【Training】Loss: 239936.770 \n",
            "【Testing】Loss: 310252.191 )\n",
            "Epoch: 18\n",
            "【Training】Loss: 237275.553 \n",
            "【Testing】Loss: 304495.618 )\n",
            "Epoch: 19\n",
            "【Training】Loss: 233790.519 \n",
            "【Testing】Loss: 300936.047 )\n",
            "Epoch: 20\n",
            "【Training】Loss: 229948.556 \n",
            "【Testing】Loss: 305112.016 )\n",
            "Epoch: 21\n",
            "【Training】Loss: 227124.999 \n",
            "【Testing】Loss: 297832.794 )\n",
            "Epoch: 22\n",
            "【Training】Loss: 224353.063 \n",
            "【Testing】Loss: 297890.661 )\n",
            "Epoch: 23\n",
            "【Training】Loss: 220828.430 \n",
            "【Testing】Loss: 294356.852 )\n",
            "Epoch: 24\n",
            "【Training】Loss: 219355.216 \n",
            "【Testing】Loss: 291389.736 )\n",
            "Epoch: 25\n",
            "【Training】Loss: 216184.521 \n",
            "【Testing】Loss: 287477.010 )\n",
            "Epoch: 26\n",
            "【Training】Loss: 213562.830 \n",
            "【Testing】Loss: 285701.072 )\n",
            "Epoch: 27\n",
            "【Training】Loss: 211614.863 \n",
            "【Testing】Loss: 284639.391 )\n",
            "Epoch: 28\n",
            "【Training】Loss: 208921.764 \n",
            "【Testing】Loss: 284734.790 )\n",
            "Epoch: 29\n",
            "【Training】Loss: 207092.562 \n",
            "【Testing】Loss: 277007.385 )\n",
            "Epoch: 30\n",
            "【Training】Loss: 204635.019 \n",
            "【Testing】Loss: 282844.607 )\n",
            "Epoch: 31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DBP_ann_module = ANNModule(ANN(input_size=L), DBP_train_loader, DBP_test_loader, EPOCH=200, LR=LR)\n",
        "DBP_ann_module.start()"
      ],
      "metadata": {
        "id": "M1DBHs4aGADD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAP_ann_module = ANNModule(ANN(input_size=L), MAP_train_loader, MAP_test_loader, EPOCH=200, LR=LR)\n",
        "MAP_ann_module.start()"
      ],
      "metadata": {
        "id": "oy6xrpb3GANO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# temp_predict = []\n",
        "# for predicts in SBP_predict:\n",
        "#     for i in range(len(predicts))\n",
        "#         temp_predict.append(predicts[i])\n",
        "# print(SBP_predict[0:20])\n",
        "\n",
        "# print(SBP_test[0:20])\n",
        "import time\n",
        "\n",
        "# Record the start time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "SBP_predict = np.array(SBP_ann_module.predict(SBP_test_loader)).reshape(-1)\n",
        "SBP_test = np.array(SBP_test)\n",
        "DBP_predict = np.array(DBP_ann_module.predict(DBP_test_loader)).reshape(-1)\n",
        "DBP_test = np.array(DBP_test)\n",
        "MAP_predict = np.array(MAP_ann_module.predict(MAP_test_loader)).reshape(-1)\n",
        "MAP_test = np.array(MAP_test)\n",
        "\n",
        "print('Artificial Nueral Network')\n",
        "\n",
        "total, ME, MAE, SD = AAMI_standard(SBP_predict, SBP_test)\n",
        "print()\n",
        "print()\n",
        "print('----------------SBP-----------------')\n",
        "print()\n",
        "print('-------------AAMI standard----------')\n",
        "print('ME             MAE             SD           total')\n",
        "print('%.3f         %.3f         %.3f          %d' % (ME,MAE,SD,total))\n",
        "#print('ME: ', ME, '   MAE: ', MAE, '   SD: ', SD, '   total: ', total)\n",
        "total, mm5, mm10, mm15 = BHS_standard(SBP_predict, SBP_test)\n",
        "print()\n",
        "print('-------------BHS standard------------')\n",
        "print('<5mmHg        <10mmHg        <15mmHg        total')\n",
        "print('%d            %d           %d          %d' % (mm5,mm10,mm15,total))\n",
        "print('%.3f%s        %.3f%s       %.3f%s         %d' % (mm5/total*100, '%',mm10/total*100, '%',mm15/total*100, '%', total))\n",
        "\n",
        "total, ME, MAE, SD = AAMI_standard(DBP_predict, DBP_test)\n",
        "print()\n",
        "print()\n",
        "print('----------------DBP-----------------')\n",
        "print()\n",
        "print('-------------AAMI standard----------')\n",
        "print('ME             MAE             SD           total')\n",
        "print('%.3f         %.3f         %.3f          %d' % (ME,MAE,SD,total))\n",
        "#print('ME: ', ME, '   MAE: ', MAE, '   SD: ', SD, '   total: ', total)\n",
        "total, mm5, mm10, mm15 = BHS_standard(DBP_predict, DBP_test)\n",
        "print()\n",
        "print('-------------BHS standard------------')\n",
        "print('<5mmHg        <10mmHg        <15mmHg        total')\n",
        "print('%d            %d           %d          %d' % (mm5,mm10,mm15,total))\n",
        "print('%.3f%s        %.3f%s       %.3f%s         %d' % (mm5/total*100, '%',mm10/total*100, '%',mm15/total*100, '%', total))\n",
        "\n",
        "total, ME, MAE, SD = AAMI_standard(MAP_predict, MAP_test)\n",
        "print()\n",
        "print()\n",
        "print('----------------MAP-----------------')\n",
        "print()\n",
        "print('-------------AAMI standard----------')\n",
        "print('ME             MAE             SD           total')\n",
        "print('%.3f         %.3f         %.3f          %d' % (ME,MAE,SD,total))\n",
        "#print('ME: ', ME, '   MAE: ', MAE, '   SD: ', SD, '   total: ', total)\n",
        "total, mm5, mm10, mm15 = BHS_standard(MAP_predict, MAP_test)\n",
        "print()\n",
        "print('-------------BHS standard------------')\n",
        "print('<5mmHg        <10mmHg        <15mmHg        total')\n",
        "print('%d            %d           %d          %d' % (mm5,mm10,mm15,total))\n",
        "print('%.3f%s        %.3f%s       %.3f%s         %d' % (mm5/total*100, '%',mm10/total*100, '%',mm15/total*100, '%', total))\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Print the elapsed time\n",
        "print(\"Time taken:\", elapsed_time, \"seconds\")"
      ],
      "metadata": {
        "id": "cIL_ARF0GAYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Artificial Neural Network')\n",
        "\n",
        "SBP_cc= np.corrcoef(SBP_predict, SBP_test)\n",
        "DBP_cc= np.corrcoef(DBP_predict, DBP_test)\n",
        "MAP_cc= np.corrcoef(MAP_predict, MAP_test)\n",
        "print()\n",
        "print()\n",
        "print('------------Correlation Coefficient-------------')\n",
        "print()\n",
        "print('SBP: %.3f' % (SBP_cc[0, 1]))\n",
        "print()\n",
        "print('DBP: %.3f' % (DBP_cc[0, 1]))\n",
        "print()\n",
        "print('MAP: %.3f' % (MAP_cc[0, 1]))"
      ],
      "metadata": {
        "id": "FGA1MXPxGAkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# confusion matrix\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(SBP_test, SBP_predict)\n",
        "\n",
        "# Display confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "\n",
        "# Set labels, title, and axis ticks\n",
        "plt.xlabel(\"Predicted labels\")\n",
        "plt.ylabel(\"True labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "wk4nADiFj3ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8MrMg_LAkgYO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}